{
  "Task_id": 252,
  "Github_ID": "27790789",
  "Github_Project_Name": "schema-registry",
  "Programming_Language": "Java",
  "suffix": ".java",
  "Interface_class": "IPC",
  "Interface_name": "Kafka client Consumer in java",
  "Instruction": "Task Description: Create a Java class that implements a Kafka consumer thread for reading and processing messages from a Kafka topic, with support for message validation and local store updates.\n\nClass Description: KafkaStoreReaderThread is a thread-based Kafka consumer that reads messages from a specified topic, validates them using a handler, and updates a local store accordingly. It maintains offset tracking and provides shutdown capabilities.\n\nAttributes:\n- topic: String - The Kafka topic to consume messages from\n- topicPartition: TopicPartition - The topic partition being consumed\n- groupId: String - The consumer group ID\n- storeUpdateHandler: StoreUpdateHandler<K, V> - Handler for message validation\n- serializer: Serializer<K, V> - Serializer for message keys and values\n- localStore: Store<K, V> - Local storage for processed messages\n- offsetUpdateLock: ReentrantLock - Lock for thread-safe offset updates\n- offsetReachedThreshold: Condition - Condition for offset tracking\n- consumer: Consumer<byte[], byte[]> - Kafka consumer instance\n- offsetInSchemasTopic: long - Current offset in the topic\n- consumerProps: Properties - Configuration properties for the Kafka consumer\n\nMethods:\n- KafkaStoreReaderThread(String bootstrapBrokers, String topic, String groupId, StoreUpdateHandler<K, V> storeUpdateHandler, Serializer<K, V> serializer, Store<K, V> localStore) -> void - Constructor that initializes the consumer with configuration and dependencies\n- doWork() -> void - Main work method that polls for messages, processes them, and updates the local store\n- shutdown() -> void - Gracefully shuts down the consumer thread",
  "Canonical_solution": "import io.confluent.kafka.schemaregistry.storage.StoreUpdateHandler.ValidationStatus;\nimport io.confluent.kafka.schemaregistry.utils.ShutdownableThread;\nimport org.apache.kafka.clients.consumer.Consumer;\nimport org.apache.kafka.clients.consumer.KafkaConsumer;\nimport org.apache.kafka.clients.consumer.ConsumerRecords;\nimport org.apache.kafka.clients.consumer.ConsumerRecord;\nimport org.apache.kafka.clients.consumer.ConsumerConfig;\nimport org.apache.kafka.common.KafkaException;\nimport org.apache.kafka.common.PartitionInfo;\nimport org.apache.kafka.common.TopicPartition;\nimport org.apache.kafka.common.errors.RecordTooLargeException;\nimport org.apache.kafka.common.errors.WakeupException;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.util.List;\nimport java.util.Properties;\nimport java.util.Arrays;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.locks.Condition;\nimport java.util.concurrent.locks.ReentrantLock;\n\npublic class KafkaStoreReaderThread<K, V> extends ShutdownableThread {\n\n  private static final Logger log = LoggerFactory.getLogger(KafkaStoreReaderThread.class);\n\n  private final String topic;\n  private final TopicPartition topicPartition;\n  private final String groupId;\n  private final StoreUpdateHandler<K, V> storeUpdateHandler;\n  private final Serializer<K, V> serializer;\n  private final Store<K, V> localStore;\n  private final ReentrantLock offsetUpdateLock;\n  private final Condition offsetReachedThreshold;\n  private Consumer<byte[], byte[]> consumer;\n  private long offsetInSchemasTopic = -1L;\n  private Properties consumerProps = new Properties();\n\n  public KafkaStoreReaderThread(String bootstrapBrokers,\n                              String topic,\n                              String groupId,\n                              StoreUpdateHandler<K, V> storeUpdateHandler,\n                              Serializer<K, V> serializer,\n                              Store<K, V> localStore) {\n    super(\"kafka-store-reader-thread-\" + topic, false);\n    offsetUpdateLock = new ReentrantLock();\n    offsetReachedThreshold = offsetUpdateLock.newCondition();\n    this.topic = topic;\n    this.groupId = groupId;\n    this.storeUpdateHandler = storeUpdateHandler;\n    this.serializer = serializer;\n    this.localStore = localStore;\n\n    consumerProps.put(ConsumerConfig.GROUP_ID_CONFIG, this.groupId);\n    consumerProps.put(ConsumerConfig.CLIENT_ID_CONFIG, \"KafkaStore-reader-\" + this.topic);\n    consumerProps.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapBrokers);\n    consumerProps.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\");\n    consumerProps.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, \"false\");\n    consumerProps.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\n                    org.apache.kafka.common.serialization.ByteArrayDeserializer.class);\n    consumerProps.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\n                    org.apache.kafka.common.serialization.ByteArrayDeserializer.class);\n\n    this.consumer = new KafkaConsumer<>(consumerProps);\n    this.topicPartition = new TopicPartition(topic, 0);\n    List<TopicPartition> topicPartitions = Arrays.asList(this.topicPartition);\n    this.consumer.assign(topicPartitions);\n    consumer.seekToBeginning(topicPartitions);\n  }\n\n  @Override\n  public void doWork() {\n    try {\n      ConsumerRecords<byte[], byte[]> records = consumer.poll(Duration.ofMillis(Long.MAX_VALUE));\n      for (ConsumerRecord<byte[], byte[]> record : records) {\n        K messageKey = this.serializer.deserializeKey(record.key());\n        V message = record.value() == null ? null : serializer.deserializeValue(messageKey, record.value());\n        \n        TopicPartition tp = new TopicPartition(record.topic(), record.partition());\n        long offset = record.offset();\n        long timestamp = record.timestamp();\n        ValidationStatus status = this.storeUpdateHandler.validateUpdate(\n                messageKey, message, tp, offset, timestamp);\n        \n        if (status == ValidationStatus.SUCCESS) {\n          if (message == null) {\n            localStore.delete(messageKey);\n          } else {\n            localStore.put(messageKey, message);\n          }\n        }\n\n        try {\n          offsetUpdateLock.lock();\n          offsetInSchemasTopic = record.offset();\n          offsetReachedThreshold.signalAll();\n        } finally {\n          offsetUpdateLock.unlock();\n        }\n      }\n    } catch (WakeupException we) {\n      // Thread is closing\n    } catch (RecordTooLargeException rtle) {\n      throw new IllegalStateException(\"Consumer threw RecordTooLargeException\", rtle);\n    } catch (RuntimeException e) {\n      log.error(\"KafkaStoreReader thread has died\", e);\n      throw e;\n    }\n  }\n\n  @Override\n  public void shutdown() {\n    super.initiateShutdown();\n    if (consumer != null) {\n      consumer.wakeup();\n    }\n    super.awaitShutdown();\n    if (consumer != null) {\n      consumer.close();\n    }\n  }\n}",
  "FSMID_for_test": 32,
  "Code_level": "Class-level"
}