{
  "Task_id": 254,
  "Github_ID": "27790789",
  "Github_Project_Name": "schema-registry",
  "Programming_Language": "Java",
  "suffix": ".java",
  "Interface_class": "IPC",
  "Interface_name": "Kafka client Producer in java",
  "Instruction": "Task Description: Create a Kafka producer class that handles message serialization and sending to a Kafka topic with configurable properties and error handling.\n\nClass Description: The KafkaStoreProducer class is a generic Kafka producer that serializes and sends key-value pairs to a specified Kafka topic. It provides configurable timeout settings and handles various Kafka-related exceptions.\n\nAttributes:\n- producer: KafkaProducer<byte[], byte[]> - The underlying Kafka producer instance\n- topic: String - The Kafka topic to which messages will be sent\n- serializer: Serializer<K, V> - The serializer for key-value pairs\n- timeout: int - The maximum time to wait for message acknowledgment in milliseconds\n\nMethods:\n- Constructor: KafkaStoreProducer(String bootstrapBrokers, String topic, Serializer<K, V> serializer, int timeout, Properties configProps) -> void - Initializes the Kafka producer with configuration properties including bootstrap servers, topic, serializer, timeout, and additional custom properties\n- put: put(K key, V value) -> V - Serializes and sends a key-value pair to the Kafka topic, returning the value if successful. Throws StoreTimeoutException if the operation times out or StoreException for other errors\n- close: close() -> void - Closes the Kafka producer and releases all resources",
  "Canonical_solution": "import io.confluent.kafka.schemaregistry.storage.exceptions.StoreException;\nimport io.confluent.kafka.schemaregistry.storage.exceptions.StoreTimeoutException;\nimport io.confluent.kafka.schemaregistry.storage.exceptions.SerializationException;\nimport io.confluent.kafka.schemaregistry.storage.serialization.Serializer;\nimport org.apache.kafka.clients.producer.KafkaProducer;\nimport org.apache.kafka.clients.producer.ProducerConfig;\nimport org.apache.kafka.clients.producer.ProducerRecord;\nimport org.apache.kafka.clients.producer.RecordMetadata;\nimport org.apache.kafka.common.errors.RecordTooLargeException;\nimport org.apache.kafka.common.serialization.ByteArraySerializer;\nimport java.util.Properties;\nimport java.util.concurrent.ExecutionException;\nimport java.util.concurrent.Future;\nimport java.util.concurrent.TimeUnit;\nimport java.util.concurrent.TimeoutException;\n\npublic class KafkaStoreProducer<K, V> {\n\n    private final KafkaProducer<byte[], byte[]> producer;\n    private final String topic;\n    private final Serializer<K, V> serializer;\n    private final int timeout;\n\n    public KafkaStoreProducer(String bootstrapBrokers, String topic, \n                            Serializer<K, V> serializer, int timeout,\n                            Properties configProps) {\n        Properties props = new Properties();\n        props.putAll(configProps);\n        props.put(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapBrokers);\n        props.put(ProducerConfig.ACKS_CONFIG, \"-1\");\n        props.put(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, ByteArraySerializer.class);\n        props.put(ProducerConfig.ENABLE_IDEMPOTENCE_CONFIG, true);\n\n        this.producer = new KafkaProducer<>(props);\n        this.topic = topic;\n        this.serializer = serializer;\n        this.timeout = timeout;\n    }\n\n    public V put(K key, V value) throws StoreTimeoutException, StoreException {\n        if (key == null) {\n            throw new StoreException(\"Key should not be null\");\n        }\n\n        ProducerRecord<byte[], byte[]> producerRecord;\n        try {\n            producerRecord = new ProducerRecord<>(\n                topic, \n                0, \n                this.serializer.serializeKey(key),\n                value == null ? null : this.serializer.serializeValue(value)\n            );\n        } catch (SerializationException e) {\n            throw new StoreException(\"Error serializing schema while creating the Kafka produce record\", e);\n        }\n\n        try {\n            Future<RecordMetadata> ack = producer.send(producerRecord);\n            RecordMetadata recordMetadata = ack.get(timeout, TimeUnit.MILLISECONDS);\n            return value;\n        } catch (InterruptedException e) {\n            throw new StoreException(\"Put operation interrupted while waiting for an ack from Kafka\", e);\n        } catch (ExecutionException e) {\n            if (e.getCause() instanceof RecordTooLargeException) {\n                throw new StoreException(\"Put operation failed because entry is too large\");\n            } else {\n                throw new StoreException(\"Put operation failed while waiting for an ack from Kafka\", e);\n            }\n        } catch (TimeoutException e) {\n            throw new StoreTimeoutException(\n                \"Put operation timed out while waiting for an ack from Kafka\", e);\n        } catch (KafkaException ke) {\n            throw new StoreException(\"Put operation to Kafka failed\", ke);\n        }\n    }\n\n    public void close() {\n        producer.close();\n    }\n}",
  "FSMID_for_test": 31,
  "Code_level": "Class-level"
}