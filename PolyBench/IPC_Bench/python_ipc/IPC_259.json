{
  "Task_id": 259,
  "Github_ID": "5934517",
  "Github_Project_Name": "kafka-python",
  "Programming_Language": "Python",
  "suffix": ".py",
  "Interface_class": "IPC",
  "Interface_name": "Kafka producer in python",
  "Instruction": "Task Description: Implement a KafkaProducer class in Python that can send messages to a Kafka cluster with configurable settings for serialization, partitioning, compression, and message delivery guarantees.\n\nClass Description: The KafkaProducer class is a high-level producer client that publishes messages to Kafka topics. It handles message serialization, partitioning, batching, and retries for failed requests. The producer is thread-safe and manages background threads for message accumulation and network I/O.\n\nAttributes:\n- config: [dict] - Configuration parameters for the producer including bootstrap servers, serializers, acks, etc.\n- _metrics: [Metrics] - Tracks producer performance metrics\n- _accumulator: [RecordAccumulator] - Batches messages by topic-partition before sending\n- _metadata: [ClusterMetadata] - Maintains cluster metadata including topic partitions\n- _sender: [Sender] - Background thread that sends batched messages to brokers\n- _closed: [bool] - Flag indicating if producer has been closed\n- _cleanup: [function] - Cleanup handler registered with atexit\n\nMethods:\n- __init__(**configs) -> None - Initializes the producer with given configuration overrides\n- send(topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None) -> FutureRecordMetadata - Asynchronously sends a message to the specified topic and returns a future\n- flush(timeout=None) -> None - Ensures all buffered messages are delivered within timeout\n- close(timeout=None) -> None - Closes the producer and releases resources\n- _cleanup_factory() -> function - Creates cleanup function for atexit registration\n- _unregister_cleanup() -> None - Removes cleanup handler from atexit\n- _max_usable_produce_magic() -> int - Determines message format version based on API version\n- _estimate_size_in_bytes(key, value, headers=[]) -> int - Estimates serialized message size\n- _ensure_valid_record_size(size) -> None - Validates message size against configured limits\n- _wait_on_metadata(topic, max_wait) -> set - Waits for metadata update for given topic\n- _serialize(f, topic, data) -> bytes - Serializes message key/value using configured serializer\n- _partition(topic, partition, key, value, serialized_key, serialized_value) -> int - Determines target partition for message",
  "Canonical_solution": "import atexit\nimport copy\nimport logging\nimport socket\nimport threading\nimport time\nimport weakref\n\nfrom kafka.vendor import six\nimport kafka.errors as Errors\nfrom kafka.client_async import KafkaClient, selectors\nfrom kafka.codec import has_gzip, has_snappy, has_lz4, has_zstd\nfrom kafka.metrics import MetricConfig, Metrics\nfrom kafka.partitioner.default import DefaultPartitioner\nfrom kafka.producer.future import FutureRecordMetadata, FutureProduceResult\nfrom kafka.producer.record_accumulator import AtomicInteger, RecordAccumulator\nfrom kafka.producer.sender import Sender\nfrom kafka.record.default_records import DefaultRecordBatchBuilder\nfrom kafka.record.legacy_records import LegacyRecordBatchBuilder\nfrom kafka.serializer import Serializer\nfrom kafka.structs import TopicPartition\n\nlog = logging.getLogger(__name__)\nPRODUCER_CLIENT_ID_SEQUENCE = AtomicInteger()\n\nclass KafkaProducer:\n    DEFAULT_CONFIG = {\n        'bootstrap_servers': 'localhost',\n        'client_id': None,\n        'key_serializer': None,\n        'value_serializer': None,\n        'acks': 1,\n        'bootstrap_topics_filter': set(),\n        'compression_type': None,\n        'retries': 0,\n        'batch_size': 16384,\n        'linger_ms': 0,\n        'partitioner': DefaultPartitioner(),\n        'buffer_memory': 33554432,\n        'connections_max_idle_ms': 9 * 60 * 1000,\n        'max_block_ms': 60000,\n        'max_request_size': 1048576,\n        'metadata_max_age_ms': 300000,\n        'retry_backoff_ms': 100,\n        'request_timeout_ms': 30000,\n        'receive_buffer_bytes': None,\n        'send_buffer_bytes': None,\n        'socket_options': [(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)],\n        'sock_chunk_bytes': 4096,\n        'sock_chunk_buffer_count': 1000,\n        'reconnect_backoff_ms': 50,\n        'reconnect_backoff_max_ms': 1000,\n        'max_in_flight_requests_per_connection': 5,\n        'security_protocol': 'PLAINTEXT',\n        'ssl_context': None,\n        'ssl_check_hostname': True,\n        'ssl_cafile': None,\n        'ssl_certfile': None,\n        'ssl_keyfile': None,\n        'ssl_crlfile': None,\n        'ssl_password': None,\n        'ssl_ciphers': None,\n        'api_version': None,\n        'api_version_auto_timeout_ms': 2000,\n        'metric_reporters': [],\n        'metrics_num_samples': 2,\n        'metrics_sample_window_ms': 30000,\n        'selector': selectors.DefaultSelector,\n        'sasl_mechanism': None,\n        'sasl_plain_username': None,\n        'sasl_plain_password': None,\n        'sasl_kerberos_service_name': 'kafka',\n        'sasl_kerberos_domain_name': None,\n        'sasl_oauth_token_provider': None,\n        'kafka_client': KafkaClient,\n    }\n\n    _COMPRESSORS = {\n        'gzip': (has_gzip, LegacyRecordBatchBuilder.CODEC_GZIP),\n        'snappy': (has_snappy, LegacyRecordBatchBuilder.CODEC_SNAPPY),\n        'lz4': (has_lz4, LegacyRecordBatchBuilder.CODEC_LZ4),\n        'zstd': (has_zstd, DefaultRecordBatchBuilder.CODEC_ZSTD),\n        None: (lambda: True, LegacyRecordBatchBuilder.CODEC_NONE),\n    }\n\n    def __init__(self, **configs):\n        self.config = copy.copy(self.DEFAULT_CONFIG)\n        for key in self.config:\n            if key in configs:\n                self.config[key] = configs.pop(key)\n\n        if self.config['client_id'] is None:\n            self.config['client_id'] = 'kafka-python-producer-%s' % (PRODUCER_CLIENT_ID_SEQUENCE.increment(),)\n\n        if self.config['acks'] == 'all':\n            self.config['acks'] = -1\n\n        metrics_tags = {'client-id': self.config['client_id']}\n        metric_config = MetricConfig(samples=self.config['metrics_num_samples'],\n                                   time_window_ms=self.config['metrics_sample_window_ms'],\n                                   tags=metrics_tags)\n        reporters = [reporter() for reporter in self.config['metric_reporters']]\n        self._metrics = Metrics(metric_config, reporters)\n\n        client = self.config['kafka_client'](\n            metrics=self._metrics, metric_group_prefix='producer',\n            wakeup_timeout_ms=self.config['max_block_ms'],\n            **self.config)\n\n        if self.config['api_version'] is None:\n            self.config['api_version'] = client.config['api_version']\n\n        ct = self.config['compression_type']\n        if ct not in self._COMPRESSORS:\n            raise ValueError(\"Not supported codec: {}\".format(ct))\n        else:\n            checker, compression_attrs = self._COMPRESSORS[ct]\n            assert checker(), \"Libraries for {} compression codec not found\".format(ct)\n            self.config['compression_attrs'] = compression_attrs\n\n        message_version = self._max_usable_produce_magic()\n        self._accumulator = RecordAccumulator(message_version=message_version, metrics=self._metrics, **self.config)\n        self._metadata = client.cluster\n        guarantee_message_order = bool(self.config['max_in_flight_requests_per_connection'] == 1)\n        self._sender = Sender(client, self._metadata,\n                             self._accumulator, self._metrics,\n                             guarantee_message_order=guarantee_message_order,\n                             **self.config)\n        self._sender.daemon = True\n        self._sender.start()\n        self._closed = False\n\n        self._cleanup = self._cleanup_factory()\n        atexit.register(self._cleanup)\n\n    def send(self, topic, value=None, key=None, headers=None, partition=None, timestamp_ms=None):\n        assert value is not None or self.config['api_version'] >= (0, 8, 1), (\n            'Null messages require kafka >= 0.8.1')\n        assert not (value is None and key is None), 'Need at least one: key or value'\n        key_bytes = value_bytes = None\n        try:\n            self._wait_on_metadata(topic, self.config['max_block_ms'] / 1000.0)\n\n            key_bytes = self._serialize(\n                self.config['key_serializer'],\n                topic, key)\n            value_bytes = self._serialize(\n                self.config['value_serializer'],\n                topic, value)\n            assert type(key_bytes) in (bytes, bytearray, memoryview, type(None))\n            assert type(value_bytes) in (bytes, bytearray, memoryview, type(None))\n\n            partition = self._partition(topic, partition, key, value,\n                                      key_bytes, value_bytes)\n\n            if headers is None:\n                headers = []\n            assert type(headers) == list\n            assert all(type(item) == tuple and len(item) == 2 and type(item[0]) == str and type(item[1]) == bytes for item in headers)\n\n            message_size = self._estimate_size_in_bytes(key_bytes, value_bytes, headers)\n            self._ensure_valid_record_size(message_size)\n\n            tp = TopicPartition(topic, partition)\n            result = self._accumulator.append(tp, timestamp_ms,\n                                             key_bytes, value_bytes, headers,\n                                             self.config['max_block_ms'],\n                                             estimated_size=message_size)\n            future, batch_is_full, new_batch_created = result\n            if batch_is_full or new_batch_created:\n                self._sender.wakeup()\n\n            return future\n        except Errors.BrokerResponseError as e:\n            return FutureRecordMetadata(\n                FutureProduceResult(TopicPartition(topic, partition)),\n                -1, None, None,\n                len(key_bytes) if key_bytes is not None else -1,\n                len(value_bytes) if value_bytes is not None else -1,\n                sum(len(h_key.encode(\"utf-8\")) + len(h_value) for h_key, h_value in headers) if headers else -1,\n            ).failure(e)\n\n    def flush(self, timeout=None):\n        self._accumulator.begin_flush()\n        self._sender.wakeup()\n        self._accumulator.await_flush_completion(timeout=timeout)\n\n    def close(self, timeout=None):\n        self._unregister_cleanup()\n\n        if not hasattr(self, '_closed') or self._closed:\n            return\n\n        invoked_from_callback = bool(threading.current_thread() is self._sender)\n        if timeout > 0:\n            if invoked_from_callback:\n                pass\n            else:\n                if self._sender is not None:\n                    self._sender.initiate_close()\n                    self._sender.join(timeout)\n\n        if self._sender is not None and self._sender.is_alive():\n            self._sender.force_close()\n\n        self._metrics.close()\n        try:\n            self.config['key_serializer'].close()\n        except AttributeError:\n            pass\n        try:\n            self.config['value_serializer'].close()\n        except AttributeError:\n            pass\n        self._closed = True\n\n    def _cleanup_factory(self):\n        _self = weakref.proxy(self)\n        def wrapper():\n            try:\n                _self.close(timeout=0)\n            except (ReferenceError, AttributeError):\n                pass\n        return wrapper\n\n    def _unregister_cleanup(self):\n        if getattr(self, '_cleanup', None):\n            if hasattr(atexit, 'unregister'):\n                atexit.unregister(self._cleanup)\n            else:\n                try:\n                    atexit._exithandlers.remove((self._cleanup, (), {}))\n                except ValueError:\n                    pass\n        self._cleanup = None\n\n    def _max_usable_produce_magic(self):\n        if self.config['api_version'] >= (0, 11):\n            return 2\n        elif self.config['api_version'] >= (0, 10):\n            return 1\n        else:\n            return 0\n\n    def _estimate_size_in_bytes(self, key, value, headers=[]):\n        magic = self._max_usable_produce_magic()\n        if magic == 2:\n            return DefaultRecordBatchBuilder.estimate_size_in_bytes(key, value, headers)\n        else:\n            return LegacyRecordBatchBuilder.estimate_size_in_bytes(magic, self.config['compression_type'], key, value)\n\n    def _ensure_valid_record_size(self, size):\n        if size > self.config['max_request_size']:\n            raise Errors.MessageSizeTooLargeError(\n                \"The message is %d bytes when serialized which is larger than\"\n                \" the maximum request size you have configured with the\"\n                \" max_request_size configuration\" % (size,))\n        if size > self.config['buffer_memory']:\n            raise Errors.MessageSizeTooLargeError(\n                \"The message is %d bytes when serialized which is larger than\"\n                \" the total memory buffer you have configured with the\"\n                \" buffer_memory configuration.\" % (size,))\n\n    def _wait_on_metadata(self, topic, max_wait):\n        self._sender.add_topic(topic)\n        begin = time.time()\n        elapsed = 0.0\n        metadata_event = None\n        while True:\n            partitions = self._metadata.partitions_for_topic(topic)\n            if partitions is not None:\n                return partitions\n\n            if not metadata_event:\n                metadata_event = threading.Event()\n\n            metadata_event.clear()\n            future = self._metadata.request_update()\n            future.add_both(lambda e, *args: e.set(), metadata_event)\n            self._sender.wakeup()\n            metadata_event.wait(max_wait - elapsed)\n            elapsed = time.time() - begin\n            if not metadata_event.is_set():\n                raise Errors.KafkaTimeoutError(\n                    \"Failed to update metadata after %.1f secs.\" % (max_wait,))\n            elif topic in self._metadata.unauthorized_topics:\n                raise Errors.TopicAuthorizationFailedError(topic)\n\n    def _serialize(self, f, topic, data):\n        if not f:\n            return data\n        if isinstance(f, Serializer):\n            return f.serialize(topic, data)\n        return f(data)\n\n    def _partition(self, topic, partition, key, value, serialized_key, serialized_value):\n        if partition is not None:\n            assert partition >= 0\n            assert partition in self._metadata.partitions_for_topic(topic), 'Unrecognized partition'\n            return partition\n\n        all_partitions = sorted(self._metadata.partitions_for_topic(topic))\n        available = list(self._metadata.available_partitions_for_topic(topic))\n        return self.config['partitioner'](serialized_key, all_partitions, available)",
  "FSMID_for_test": 78,
  "Code_level": "Class-level"
}