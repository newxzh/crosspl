{
  "Task_id": 492,
  "Github_ID": "473378688",
  "Github_Project_Name": "DeepSpeed-MII",
  "Programming_Language": "Python",
  "suffix": ".py",
  "Interface_class": "IPC",
  "Interface_name": "gRPC Server - side based on grpcio in python",
  "Instruction": "Task Description: Implement a gRPC server in Python that handles various model inference requests concurrently, with thread-safe operations and graceful shutdown capabilities.\n\nClass Description: The GRPCServer class encapsulates a gRPC server implementation that manages multiple model inference services. It provides concurrent request handling, message size configuration, and proper shutdown mechanisms.\n\nAttributes:\n- server: [grpc.Server] - The gRPC server instance that handles incoming requests\n- stop_event: [threading.Event] - Event flag used to signal server termination\n\nMethods:\n- __init__: [Name](service_impl: [ModelResponseServicer], port: [int]) -> [None] - Initializes the gRPC server with thread pool executor, message size limits, and binds it to the specified port\n- start: [Name]() -> [None] - Starts the server and waits for termination signal before shutting down gracefully\n\nClass Description: The ModelResponseServicer class implements the actual gRPC service methods for various model inference tasks, providing thread-safe operation through locking mechanisms.\n\nAttributes:\n- _stop_event: [threading.Event] - Event flag used to signal service termination\n- inference_pipeline: [object] - The underlying inference processing pipeline\n- lock: [threading.Lock] - Lock for thread-safe operations\n\nMethods:\n- __init__: [Name](inference_pipeline: [object]) -> [None] - Initializes the servicer with inference pipeline and synchronization primitives\n- Terminate: [Name](request: [google.protobuf.Empty], context: [grpc.ServicerContext]) -> [google.protobuf.Empty] - Signals the server to terminate\n- get_stop_event: [Name]() -> [threading.Event] - Returns the stop event for server control\n- _run_inference: [Name](method_name: [str], request_proto: [object]) -> [object] - Internal thread-safe method for processing inference requests\n- GeneratorReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles generator model requests\n- Txt2ImgReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles text-to-image model requests\n- ClassificationReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles classification model requests\n- QuestionAndAnswerReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles Q&A model requests\n- FillMaskReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles fill-mask model requests\n- TokenClassificationReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles token classification requests\n- ZeroShotImgClassificationReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles zero-shot image classification\n- InpaintingReply: [Name](request: [object], context: [grpc.ServicerContext]) -> [object] - Handles image inpainting requests",
  "Canonical_solution": "import grpc\nfrom concurrent import futures\nfrom google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2\nfrom .proto import legacymodelresponse_pb2_grpc as modelresponse_pb2_grpc\nimport threading\nfrom mii.legacy.constants import (\n    GRPC_MAX_MSG_SIZE,\n    LB_MAX_WORKER_THREADS,\n    SERVER_SHUTDOWN_TIMEOUT\n)\n\nclass ModelResponseServicer(modelresponse_pb2_grpc.ModelResponseServicer):\n    def __init__(self, inference_pipeline):\n        self._stop_event = threading.Event()\n        self.inference_pipeline = inference_pipeline\n        self.lock = threading.Lock()\n\n    def Terminate(self, request, context):\n        self._stop_event.set()\n        return google_dot_protobuf_dot_empty__pb2.Empty()\n\n    def get_stop_event(self):\n        return self._stop_event\n\n    def _run_inference(self, method_name, request_proto):\n        with self.lock:\n            response = self.inference_pipeline.process_request(method_name, request_proto)\n        return response\n\n    def GeneratorReply(self, request, context):\n        return self._run_inference(\"GeneratorReply\", request)\n\n    def Txt2ImgReply(self, request, context):\n        return self._run_inference(\"Txt2ImgReply\", request)\n\n    def ClassificationReply(self, request, context):\n        return self._run_inference(\"ClassificationReply\", request)\n\n    def QuestionAndAnswerReply(self, request, context):\n        return self._run_inference(\"QuestionAndAnswerReply\", request)\n\n    def FillMaskReply(self, request, context):\n        return self._run_inference(\"FillMaskReply\", request)\n\n    def TokenClassificationReply(self, request, context):\n        return self._run_inference(\"TokenClassificationReply\", request)\n\n    def ZeroShotImgClassificationReply(self, request, context):\n        return self._run_inference(\"ZeroShotImgClassificationReply\", request)\n\n    def InpaintingReply(self, request, context):\n        return self._run_inference(\"InpaintingReply\", request)\n\nclass GRPCServer:\n    def __init__(self, service_impl, port):\n        self.server = grpc.server(\n            futures.ThreadPoolExecutor(max_workers=LB_MAX_WORKER_THREADS),\n            options=[\n                (\"grpc.max_send_message_length\", GRPC_MAX_MSG_SIZE),\n                (\"grpc.max_receive_message_length\", GRPC_MAX_MSG_SIZE),\n            ]\n        )\n        modelresponse_pb2_grpc.add_ModelResponseServicer_to_server(service_impl, self.server)\n        self.server.add_insecure_port(f\"[::]:{port}\")\n        self.stop_event = service_impl.get_stop_event()\n\n    def start(self):\n        self.server.start()\n        self.stop_event.wait()\n        self.server.stop(SERVER_SHUTDOWN_TIMEOUT)",
  "FSMID_for_test": 74,
  "Code_level": "Class-level"
}