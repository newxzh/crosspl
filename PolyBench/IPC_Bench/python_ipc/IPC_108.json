{
  "Task_id": 108,
  "Github_ID": "76795266",
  "Github_Project_Name": "news-please",
  "Programming_Language": "Python",
  "suffix": ".py",
  "Interface_class": "IPC",
  "Interface_name": "HTTP Client-side by using requests in python",
  "Instruction": "Task Description: Implement a web crawler class that can fetch HTML content from single or multiple URLs, with support for threaded operations and error handling.\n\nClass Description: SimpleCrawler is a class designed to fetch HTML content from web pages either individually or in parallel using threading. It handles various HTTP request scenarios and errors while maintaining proper logging.\n\nAttributes:\n_results: dict - Stores fetched HTML content when operating in threaded mode\nMAX_FILE_SIZE: int - Maximum allowed file size for fetched content (20MB)\nMIN_FILE_SIZE: int - Minimum allowed file size for fetched content (10 bytes)\nLOGGER: Logger - Logger instance for error and debug messages\nUSER_AGENT: str - User agent string for HTTP requests\nHEADERS: dict - Default headers for HTTP requests\n\nMethods:\nfetch_url(url: str, request_args: dict = None) -> str - Fetches HTML content from a single URL\nInput:\n  url: str - URL to fetch\n  request_args: dict (optional) - Additional arguments for the request\nOutput:\n  str - HTML content of the URL or None if failed\n\n_fetch_url(url: str, is_threaded: bool, request_args: dict = None) -> str - Internal method that performs the actual URL fetching\nInput:\n  url: str - URL to fetch\n  is_threaded: bool - Flag indicating if operation is threaded\n  request_args: dict (optional) - Additional arguments for the request\nOutput:\n  str - HTML content of the URL or None if failed\n\nfetch_urls(urls: list, request_args: dict = None) -> dict - Fetches HTML content from multiple URLs in parallel using threads\nInput:\n  urls: list - List of URLs to fetch\n  request_args: dict (optional) - Additional arguments for the requests\nOutput:\n  dict - Dictionary mapping URLs to their HTML content",
  "Canonical_solution": "import socket\nimport copy\nimport threading\nimport logging\nimport requests\nimport urllib3\nfrom .response_decoder import decode_response\n\nMAX_FILE_SIZE = 20000000\nMIN_FILE_SIZE = 10\n\nLOGGER = logging.getLogger(__name__)\nUSER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\"\nHEADERS = {\n    \"Connection\": \"close\",\n    \"User-Agent\": USER_AGENT,\n}\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nclass SimpleCrawler:\n    _results = {}\n\n    def __init__(self):\n        pass\n\n    def fetch_url(self, url, request_args=None):\n        \"\"\"\n        Crawls the html content of the parameter url and returns the html\n        :param url: URL to fetch\n        :param request_args: optional arguments that `request` takes\n        :return: HTML content of the URL\n        \"\"\"\n        return self._fetch_url(url, False, request_args=request_args)\n\n    def _fetch_url(self, url, is_threaded, request_args=None):\n        \"\"\"\n        Crawls the html content of the parameter url and saves the html in _results\n        :param url: URL to fetch\n        :param is_threaded: If True, results will be stored for later processing\n        :param request_args: optional arguments that `request` takes\n        :return: html of the url\n        \"\"\"\n        if request_args is None:\n            request_args = {}\n        if \"headers\" not in request_args:\n            request_args[\"headers\"] = HEADERS\n\n        html_str = None\n        try:\n            response = requests.get(\n                url, verify=False, allow_redirects=True, **request_args)\n            \n            if response.status_code != 200:\n                LOGGER.error(\"not a 200 response: %s\", response.status_code)\n            elif response.text is None or len(response.text) < MIN_FILE_SIZE:\n                LOGGER.error(\"too small/incorrect: %s %s\", url, len(response.text))\n            elif len(response.text) > MAX_FILE_SIZE:\n                LOGGER.error(\"too large: %s %s\", url, len(response.text))\n            else:\n                html_str = decode_response(response)\n                \n        except (requests.exceptions.MissingSchema, requests.exceptions.InvalidURL):\n            LOGGER.error(\"malformed URL: %s\", url)\n        except requests.exceptions.TooManyRedirects:\n            LOGGER.error(\"too many redirects: %s\", url)\n        except requests.exceptions.SSLError as err:\n            LOGGER.error(\"SSL: %s %s\", url, err)\n        except (socket.timeout, requests.exceptions.ConnectionError, \n               requests.exceptions.Timeout, socket.error, socket.gaierror) as err:\n            LOGGER.error(\"connection/timeout error: %s %s\", url, err)\n            \n        if is_threaded:\n            self._results[url] = html_str\n        return html_str\n\n    def fetch_urls(self, urls, request_args=None):\n        \"\"\"\n        Crawls the html content of all given urls in parallel.\n        :param urls: List of URLs to fetch\n        :param request_args: optional arguments that `request` takes\n        :return: Dictionary of URL to HTML content mappings\n        \"\"\"\n        threads = [\n            threading.Thread(target=self._fetch_url, args=(url, True, request_args))\n            for url in urls\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        results = copy.deepcopy(self._results)\n        self._results = {}\n        return results",
  "FSMID_for_test": 57,
  "Code_level": "Class-level"
}