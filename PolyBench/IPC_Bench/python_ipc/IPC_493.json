{
  "Task_id": 493,
  "Github_ID": "819554665",
  "Github_Project_Name": "exo",
  "Programming_Language": "Python",
  "suffix": ".py",
  "Interface_class": "IPC",
  "Interface_name": "gRPC Server - side based on grpcio in python",
  "Instruction": "Task Description: Implement a gRPC server class in Python that handles various types of requests (prompts, tensors, examples) for a distributed node service, with health check capability.\n\nClass Description: GRPCServer is a gRPC server implementation that processes different types of requests for a node service, including prompt processing, tensor processing, and example processing with training capabilities. It manages server lifecycle and handles large message sizes.\n\nAttributes:\n- node: [Any] - The node instance that processes the actual requests\n- host: [str] - The host address where the server will run\n- port: [int] - The port number where the server will listen\n- server: [grpc.aio.Server] - The gRPC server instance (initialized as None)\n\nMethods:\n- start() -> None - Initializes and starts the gRPC server with thread pool executor and configured message size limits\n- stop() -> None - Gracefully stops the gRPC server with a 5-second grace period\n- SendPrompt(request: [node_service_pb2.PromptRequest], context: [grpc.aio.ServicerContext]) -> [node_service_pb2.Tensor] - Processes prompt requests and returns tensor results\n- SendTensor(request: [node_service_pb2.TensorRequest], context: [grpc.aio.ServicerContext]) -> [node_service_pb2.Tensor] - Processes tensor requests and returns tensor results\n- SendExample(request: [node_service_pb2.ExampleRequest], context: [grpc.aio.ServicerContext]) -> [node_service_pb2.Loss] - Processes training examples and returns loss/gradients\n- HealthCheck(request: [node_service_pb2.HealthCheckRequest], context: [grpc.aio.ServicerContext]) -> [node_service_pb2.HealthCheckResponse] - Provides health check status of the server",
  "Canonical_solution": "import grpc\nfrom concurrent import futures\nimport numpy as np\nfrom asyncio import CancelledError\nfrom . import node_service_pb2\nfrom . import node_service_pb2_grpc\n\nclass GRPCServer(node_service_pb2_grpc.NodeServiceServicer):\n    def __init__(self, node, host: str, port: int):\n        self.node = node\n        self.host = host\n        self.port = port\n        self.server = None\n\n    async def start(self) -> None:\n        self.server = grpc.aio.server(\n            futures.ThreadPoolExecutor(max_workers=10),\n            options=[\n                (\"grpc.max_metadata_size\", 32*1024*1024),\n                (\"grpc.max_send_message_length\", 128*1024*1024),\n                (\"grpc.max_receive_message_length\", 128*1024*1024),\n            ],\n        )\n        node_service_pb2_grpc.add_NodeServiceServicer_to_server(self, self.server)\n        listen_addr = f\"{self.host}:{self.port}\"\n        self.server.add_insecure_port(listen_addr)\n        await self.server.start()\n\n    async def stop(self) -> None:\n        if self.server:\n            try:\n                await self.server.stop(grace=5)\n                await self.server.wait_for_termination()\n            except CancelledError:\n                pass\n\n    async def SendPrompt(self, request, context):\n        shard = Shard(\n            model_id=request.shard.model_id,\n            start_layer=request.shard.start_layer,\n            end_layer=request.shard.end_layer,\n            n_layers=request.shard.n_layers,\n        )\n        prompt = request.prompt\n        request_id = request.request_id\n        result = await self.node.process_prompt(shard, prompt, request_id)\n        tensor_data = result.tobytes() if result is not None else None\n        return node_service_pb2.Tensor(tensor_data=tensor_data, shape=result.shape, dtype=str(result.dtype)) if result is not None else node_service_pb2.Tensor()\n\n    async def SendTensor(self, request, context):\n        shard = Shard(\n            model_id=request.shard.model_id,\n            start_layer=request.shard.start_layer,\n            end_layer=request.shard.end_layer,\n            n_layers=request.shard.n_layers,\n        )\n        tensor = np.frombuffer(request.tensor.tensor_data, dtype=np.dtype(request.tensor.dtype)).reshape(request.tensor.shape)\n        request_id = request.request_id\n        result = await self.node.process_tensor(shard, tensor, request_id)\n        tensor_data = result.tobytes() if result is not None else None\n        return node_service_pb2.Tensor(tensor_data=tensor_data, shape=result.shape, dtype=str(result.dtype)) if result is not None else node_service_pb2.Tensor()\n\n    async def SendExample(self, request, context):\n        shard = Shard(\n            model_id=request.shard.model_id,\n            start_layer=request.shard.start_layer,\n            end_layer=request.shard.end_layer,\n            n_layers=request.shard.n_layers,\n        )\n        example = np.frombuffer(request.example.tensor_data, dtype=np.dtype(request.example.dtype)).reshape(request.example.shape)\n        target = np.frombuffer(request.target.tensor_data, dtype=np.dtype(request.target.dtype)).reshape(request.target.shape)\n        length = np.frombuffer(request.length.tensor_data, dtype=np.dtype(request.length.dtype)).reshape(request.length.shape)\n        train = request.train\n        request_id = request.request_id\n\n        if train and not shard.is_first_layer():\n            loss, grad = await self.node.process_example(shard, example, target, length, train, request_id)\n            tensor_data = grad.tobytes()\n            grad_tensor = node_service_pb2.Tensor(tensor_data=tensor_data, shape=grad.shape, dtype=str(grad.dtype))\n            return node_service_pb2.Loss(loss=loss, grads=grad_tensor)\n        else:\n            loss = await self.node.process_example(shard, example, target, length, train, request_id)\n            return node_service_pb2.Loss(loss=loss, grads=None)\n\n    async def HealthCheck(self, request, context):\n        return node_service_pb2.HealthCheckResponse(is_healthy=True)",
  "FSMID_for_test": 74,
  "Code_level": "Class-level"
}