[
  {
    "Task_id": 101,
    "Instruction": "Task Description: Create a Python class that implements a ScalarFunction to retrieve geographic location information for a given IP address using an external API.\n\nClass Description: The GetLocation class extends PyFlink's ScalarFunction to provide IP-to-location functionality. It makes HTTP requests to the ip2location.io API and processes the response to extract country, state, and city information.\n\nAttributes: None\n\nMethods:\n- eval(ip_address: str) -> str: \n  Takes an IP address as input, queries the ip2location.io API, and returns a JSON string containing the country code, region name, and city name. If the request fails, returns an empty JSON object. The API key is retrieved from environment variables.\n\n  Input Parameters:\n  - ip_address: str - The IP address to look up\n\n  Return Type:\n  - str - JSON string containing location information in format {'country': str, 'state': str, 'city': str} or empty object if request fails",
    "Canonical_solution": "import os\nimport json\nimport requests\nfrom pyflink.table.udf import ScalarFunction\nfrom pyflink.table import DataTypes\n\nclass GetLocation(ScalarFunction):\n    def eval(self, ip_address):\n        url = \"https://api.ip2location.io\"\n        response = requests.get(url, params={\n            'ip': ip_address,\n            'key': os.environ.get(\"IP_CODING_KEY\")\n        })\n\n        if response.status_code != 200:\n            return json.dumps({})\n\n        data = json.loads(response.text)\n        country = data.get('country_code', '')\n        state = data.get('region_name', '')\n        city = data.get('city_name', '')\n        return json.dumps({'country': country, 'state': state, 'city': city})",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport json\nimport requests\nfrom pyflink.table.udf import ScalarFunction\n\nclass GetLocation(ScalarFunction):\n    def eval(self, ip_address):\n        api_key = os.getenv('IP2LOCATION_API_KEY')\n        if not api_key:\n            return '{}'\n        \n        url = f\"https://api.ip2location.io/?key={api_key}&ip={ip_address}\"\n        try:\n            response = requests.get(url)\n            response.raise_for_status()\n            data = response.json()\n            result = {\n                'country': data.get('country_code', ''),\n                'state': data.get('region_name', ''),\n                'city': data.get('city_name', '')\n            }\n            return json.dumps(result)\n        except:\n            return '{}'"
  },
  {
    "Task_id": 102,
    "Instruction": "Task Description: Create a function that handles GitHub webhook events for pull requests, fetches a random meme from an API, and posts it as a comment on the pull request.\n\nFunction Description: The function `bot()` is a webhook handler that processes GitHub pull request events. When a pull request is opened, it authenticates with the GitHub API, fetches a random meme from an external API, and posts the meme as a comment on the pull request. The function uses the `requests` library for HTTP communication with the meme API and the `PyGithub` library for GitHub interactions.\n\nInput: \n- The function receives a Flask `request` object containing JSON payload from GitHub webhook. The payload includes:\n  - `action`: The event type (e.g., \"opened\")\n  - `pull_request`: Details about the pull request\n  - `repository`: Information about the repository containing owner and name\n\nOutput: \n- Returns \"ok\" string in all cases (success or failure) to acknowledge webhook receipt\n- As a side effect, posts a meme comment on the pull request when conditions are met",
    "Canonical_solution": "import os\nimport requests\nfrom dotenv import load_dotenv\nfrom flask import Flask, request\nfrom github import Github, GithubIntegration\n\nload_dotenv()\n\n__flask_app_name__ = 'github_bot'\napp = Flask(__flask_app_name__)\napp_id = int(os.environ['GITHUB_APP_ID'])\nwith open(\n        os.path.normpath(os.path.expanduser(os.getenv(\"GITHUB_PRIVATE_KEY_FILE\", '~/.certs/github/bot_key.pem'))),\n        'r'\n) as cert_file:\n    app_key = cert_file.read()\n\ngit_integration = GithubIntegration(\n    app_id,\n    app_key,\n)\n\ndef bot():\n    payload = request.json\n    if not all(k in payload.keys() for k in ['action', 'pull_request']) and \\\n            payload['action'] == 'opened':\n        return \"ok\"\n    owner = payload['repository']['owner']['login']\n    repo_name = payload['repository']['name']\n    git_connection = Github(\n        login_or_token=git_integration.get_access_token(\n            git_integration.get_installation(owner, repo_name).id\n        ).token\n    )\n    repo = git_connection.get_repo(f\"{owner}/{repo_name}\")\n    issue = repo.get_issue(number=payload['pull_request']['number'])\n    response = requests.get(url='https://meme-api.herokuapp.com/gimme')\n    if response.status_code != 200:\n        return 'ok'\n    meme_url = response.json()['preview'][-1]\n    issue.create_comment(f\"![Alt Text]({meme_url})\")\n    return \"ok\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom github import Github\nfrom flask import request\n\ndef bot():\n    if request.json.get('action') != 'opened':\n        return \"ok\"\n\n    pr_data = request.json.get('pull_request')\n    repo_data = request.json.get('repository')\n\n    if not pr_data or not repo_data:\n        return \"ok\"\n\n    try:\n        meme_response = requests.get('https://meme-api.com/gimme')\n        meme_response.raise_for_status()\n        meme_url = meme_response.json().get('url')\n\n        if not meme_url:\n            return \"ok\"\n\n        github_token = 'your_github_token_here'\n        g = Github(github_token)\n        repo = g.get_repo(f\"{repo_data['owner']['login']}/{repo_data['name']}\")\n        pr = repo.get_pull(pr_data['number'])\n        pr.create_issue_comment(f\"Here's a random meme for you!\\n\\n![Meme]({meme_url})\")\n\n    except Exception:\n        pass\n\n    return \"ok\""
  },
  {
    "Task_id": 103,
    "Instruction": "Task Description: Create a function that sends audio and transcript files to a server for processing using HTTP POST request and handles the response.\n\nFunction Description: The function `gentle_request` sends a WAV audio file and its corresponding transcript text file to a specified server using an HTTP POST request. It checks if the filenames match, reads the transcript content, and sends both files to the server. The function handles the server response, checks for errors, and optionally prints debug information.\n\nInput:\n- `wav_path` [str] - Path to the WAV audio file to be sent.\n- `txt_path` [str] - Path to the transcript text file corresponding to the audio.\n- `server_addr` [str] - IP address or hostname of the server.\n- `port` [int] - Port number on which the server is listening.\n- `debug` [bool, optional] - If True, prints the server response in JSON format. Default is False.\n\nOutput:\n- `response` [requests.Response or None] - The server's HTTP response object if the request is successful, otherwise None. The response can be parsed for status code, headers, and content.",
    "Canonical_solution": "from docopt import docopt\nfrom glob import glob\nfrom tqdm import tqdm\nimport os.path\nimport requests\nimport numpy as np\n\ndef gentle_request(wav_path,txt_path, server_addr, port, debug=False):\n    print('\\n')\n    response = None\n    wav_name = os.path.basename(wav_path)\n    txt_name = os.path.basename(txt_path)\n    if os.path.splitext(wav_name)[0] != os.path.splitext(txt_name)[0]:\n        print(' [!] wav name and transcript name does not match - exiting...')\n        return response\n    with open(txt_path, 'r', encoding='utf-8-sig') as txt_file:\n        print('Transcript - '+''.join(txt_file.readlines()))\n    with open(wav_path,'rb') as wav_file, open(txt_path, 'rb') as txt_file:\n        params = (('async','false'),)\n        files={'audio':(wav_name,wav_file),\n               'transcript':(txt_name,txt_file),\n               }\n        server_path = 'http://'+server_addr+':'+str(port)+'/transcriptions'\n        response = requests.post(server_path, params=params,files=files)\n        if response.status_code != 200:\n            print(' [!] External server({}) returned bad response({})'.format(server_path, response.status_code))\n    if debug:\n        print('Response')\n        print(response.json())\n    return response",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "```python\nimport os\nimport requests\n\ndef gentle_request(wav_path, txt_path, server_addr, port, debug=False):\n    if not os.path.exists(wav_path) or not os.path.exists(txt_path):\n        return None\n    \n    wav_basename = os.path.splitext(os.path.basename(wav_path))[0]\n    txt_basename = os.path.splitext(os.path.basename(txt_path))[0]\n    \n    if wav_basename != txt_basename:\n        return None\n    \n    try:\n        with open(txt_path, 'r') as f:\n            transcript = f.read()\n    except IOError:\n        return None\n    \n    url = f\"http://{server_addr}:{port}/transcriptions\"\n    files = {\n        'audio': open(wav_path, 'rb'),\n        'transcript': ('transcript.txt', transcript)\n    }\n    \n    try:\n        response = requests.post(url, files=files)\n        if debug:\n            print(response.json())\n        return response\n    except requests.exceptions.RequestException:\n        return None\n    finally:\n        files['audio'].close()\n```"
  },
  {
    "Task_id": 106,
    "Instruction": "Task Description: Create a Python class that implements an HTTP client for interacting with a remote API, featuring automatic retries for failed requests, background polling for data refresh, and graceful shutdown handling.\n\nClass Description: The Fetcher class provides a robust HTTP client implementation for API interactions with features like exponential backoff retries, periodic background data refreshing, and proper thread cleanup on program exit.\n\nAttributes:\n- _base_url: str - The base URL for API endpoints\n- _api_key: str - API key for authentication\n- _stop_polling_event: Event - Threading event to control polling thread lifecycle\n- _exit_monitor: Thread - Thread that monitors main thread for program exit\n- _poller_thread: Thread - Thread that handles periodic data refreshing\n\nMethods:\n- __init__(base_url: str, api_key: str) -> None - Initializes the fetcher with API configuration\n- run() -> None - Starts the background polling and exit monitoring threads\n- post(api: str, body: Dict[str, str]) -> None - Sends a POST request to specified API endpoint\n- _fetch_url(url: str, api_key: str) -> Dict - Performs a GET request with retry logic (private)\n- _post_url(url: str, api_key: str, body: Dict[str, str]) -> None - Performs a POST request (private)\n- _thread_func(base_url: str, api_key: str, stop_polling_event: Event, seconds_interval: Optional[int]) -> None - Background polling thread function (private)\n- _refresh_data(base_url: str, api_key: str) -> None - Refreshes data from multiple endpoints (private)\n- _monitor_exit(exit_event: Event) -> None - Monitors main thread for program exit (private)",
    "Canonical_solution": "import logging\nimport os\nimport threading\nimport time\nimport typing\nimport requests\n\nfrom threading import Thread, Event\nfrom typing import Dict, Optional\nfrom tenacity import (\n    RetryError,\n    retry,\n    stop_after_attempt,\n    wait_exponential,\n    retry_if_exception,\n)\n\nclass Fetcher:\n    def __init__(self, base_url: str, api_key: str):\n        self._base_url = base_url\n        self._api_key = api_key\n        self._stop_polling_event = Event()\n        self._exit_monitor = Thread(\n            target=self._monitor_exit, args=(self._stop_polling_event,), daemon=True\n        )\n        self._poller_thread = Thread(\n            target=self._thread_func,\n            args=(\n                self._base_url,\n                self._api_key,\n                self._stop_polling_event,\n                int(os.getenv(\"TRACELOOP_PROMPT_MANAGER_POLLING_INTERVAL\") or 5),\n            ),\n        )\n\n    def run(self):\n        self._refresh_data(self._base_url, self._api_key)\n        self._exit_monitor.start()\n        self._poller_thread.start()\n\n    def post(self, api: str, body: Dict[str, str]):\n        self._post_url(f\"{self._base_url}/v1/traceloop/{api}\", self._api_key, body)\n\n    @retry(\n        wait=wait_exponential(multiplier=1, min=4),\n        stop=stop_after_attempt(int(os.getenv(\"TRACELOOP_PROMPT_MANAGER_MAX_RETRIES\") or 3)),\n        retry=retry_if_exception(lambda e: isinstance(e, requests.exceptions.HTTPError) and (500 <= e.response.status_code < 600),\n    )\n    def _fetch_url(self, url: str, api_key: str):\n        response = requests.get(\n            url,\n            headers={\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"X-Traceloop-SDK-Version\": \"1.0.0\",\n            },\n        )\n\n        if response.status_code != 200:\n            if response.status_code == 401 or response.status_code == 403:\n                logging.error(\"Authorization error: Invalid Traceloop API key.\")\n                raise requests.exceptions.HTTPError(response=response)\n            else:\n                logging.error(\"Request failed: %s\", response.status_code)\n                raise requests.exceptions.HTTPError(response=response)\n        else:\n            return response.json()\n\n    def _post_url(self, url: str, api_key: str, body: Dict[str, str]):\n        response = requests.post(\n            url,\n            headers={\n                \"Authorization\": f\"Bearer {api_key}\",\n                \"X-Traceloop-SDK-Version\": \"1.0.0\",\n            },\n            json=body,\n        )\n\n        if response.status_code != 200:\n            raise requests.exceptions.HTTPError(response=response)\n\n    def _thread_func(self, base_url: str, api_key: str, stop_polling_event: Event, seconds_interval: Optional[int] = 5):\n        while not stop_polling_event.is_set():\n            try:\n                self._refresh_data(base_url, api_key)\n            except RetryError:\n                logging.error(\"Request failed after retries : stopped polling\")\n                break\n\n            time.sleep(seconds_interval)\n\n    def _refresh_data(self, base_url: str, api_key: str):\n        self._fetch_url(f\"{base_url}/v1/traceloop/prompts\", api_key)\n        self._fetch_url(f\"{base_url}/v1/traceloop/pii/tracing-allow-list\", api_key)\n\n    def _monitor_exit(self, exit_event: Event):\n        main_thread = threading.main_thread()\n        main_thread.join()\n        exit_event.set()",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport time\nimport threading\nfrom typing import Dict, Optional\n\nclass Fetcher:\n    def __init__(self, base_url: str, api_key: str) -> None:\n        self._base_url = base_url\n        self._api_key = api_key\n        self._stop_polling_event = threading.Event()\n        self._exit_monitor = threading.Thread(target=self._monitor_exit, args=(self._stop_polling_event,), daemon=True)\n        self._poller_thread = None\n\n    def run(self) -> None:\n        self._poller_thread = threading.Thread(target=self._thread_func, args=(self._base_url, self._api_key, self._stop_polling_event, 60), daemon=True)\n        self._poller_thread.start()\n        self._exit_monitor.start()\n\n    def post(self, api: str, body: Dict[str, str]) -> None:\n        url = f\"{self._base_url}/{api}\"\n        self._post_url(url, self._api_key, body)\n\n    def _fetch_url(self, url: str, api_key: str) -> Dict:\n        max_retries = 3\n        retry_delay = 1\n        headers = {'Authorization': f'Bearer {api_key}'}\n        for attempt in range(max_retries):\n            try:\n                response = requests.get(url, headers=headers)\n                response.raise_for_status()\n                return response.json()\n            except requests.exceptions.RequestException as e:\n                if attempt == max_retries - 1:\n                    raise\n                time.sleep(retry_delay)\n                retry_delay *= 2\n\n    def _post_url(self, url: str, api_key: str, body: Dict[str, str]) -> None:\n        headers = {'Authorization': f'Bearer {api_key}', 'Content-Type': 'application/json'}\n        response = requests.post(url, json=body, headers=headers)\n        response.raise_for_status()\n\n    def _thread_func(self, base_url: str, api_key: str, stop_polling_event: Event, seconds_interval: Optional[int]) -> None:\n        while not stop_polling_event.is_set():\n            self._refresh_data(base_url, api_key)\n            time.sleep(seconds_interval)\n\n    def _refresh_data(self, base_url: str, api_key: str) -> None:\n        endpoints = ['status', 'metrics', 'config']\n        for endpoint in endpoints:\n            url = f\"{base_url}/{endpoint}\"\n            try:\n                self._fetch_url(url, api_key)\n            except requests.exceptions.RequestException:\n                pass\n\n    def _monitor_exit(self, exit_event: Event) -> None:\n        main_thread = threading.main_thread()\n        main_thread.join()\n        exit_event.set()"
  },
  {
    "Task_id": 108,
    "Instruction": "Task Description: Implement a web crawler class that can fetch HTML content from single or multiple URLs, with support for threaded operations and error handling.\n\nClass Description: SimpleCrawler is a class designed to fetch HTML content from web pages either individually or in parallel using threading. It handles various HTTP request scenarios and errors while maintaining proper logging.\n\nAttributes:\n_results: dict - Stores fetched HTML content when operating in threaded mode\nMAX_FILE_SIZE: int - Maximum allowed file size for fetched content (20MB)\nMIN_FILE_SIZE: int - Minimum allowed file size for fetched content (10 bytes)\nLOGGER: Logger - Logger instance for error and debug messages\nUSER_AGENT: str - User agent string for HTTP requests\nHEADERS: dict - Default headers for HTTP requests\n\nMethods:\nfetch_url(url: str, request_args: dict = None) -> str - Fetches HTML content from a single URL\nInput:\n  url: str - URL to fetch\n  request_args: dict (optional) - Additional arguments for the request\nOutput:\n  str - HTML content of the URL or None if failed\n\n_fetch_url(url: str, is_threaded: bool, request_args: dict = None) -> str - Internal method that performs the actual URL fetching\nInput:\n  url: str - URL to fetch\n  is_threaded: bool - Flag indicating if operation is threaded\n  request_args: dict (optional) - Additional arguments for the request\nOutput:\n  str - HTML content of the URL or None if failed\n\nfetch_urls(urls: list, request_args: dict = None) -> dict - Fetches HTML content from multiple URLs in parallel using threads\nInput:\n  urls: list - List of URLs to fetch\n  request_args: dict (optional) - Additional arguments for the requests\nOutput:\n  dict - Dictionary mapping URLs to their HTML content",
    "Canonical_solution": "import socket\nimport copy\nimport threading\nimport logging\nimport requests\nimport urllib3\nfrom .response_decoder import decode_response\n\nMAX_FILE_SIZE = 20000000\nMIN_FILE_SIZE = 10\n\nLOGGER = logging.getLogger(__name__)\nUSER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\"\nHEADERS = {\n    \"Connection\": \"close\",\n    \"User-Agent\": USER_AGENT,\n}\nurllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n\nclass SimpleCrawler:\n    _results = {}\n\n    def __init__(self):\n        pass\n\n    def fetch_url(self, url, request_args=None):\n        \"\"\"\n        Crawls the html content of the parameter url and returns the html\n        :param url: URL to fetch\n        :param request_args: optional arguments that `request` takes\n        :return: HTML content of the URL\n        \"\"\"\n        return self._fetch_url(url, False, request_args=request_args)\n\n    def _fetch_url(self, url, is_threaded, request_args=None):\n        \"\"\"\n        Crawls the html content of the parameter url and saves the html in _results\n        :param url: URL to fetch\n        :param is_threaded: If True, results will be stored for later processing\n        :param request_args: optional arguments that `request` takes\n        :return: html of the url\n        \"\"\"\n        if request_args is None:\n            request_args = {}\n        if \"headers\" not in request_args:\n            request_args[\"headers\"] = HEADERS\n\n        html_str = None\n        try:\n            response = requests.get(\n                url, verify=False, allow_redirects=True, **request_args)\n            \n            if response.status_code != 200:\n                LOGGER.error(\"not a 200 response: %s\", response.status_code)\n            elif response.text is None or len(response.text) < MIN_FILE_SIZE:\n                LOGGER.error(\"too small/incorrect: %s %s\", url, len(response.text))\n            elif len(response.text) > MAX_FILE_SIZE:\n                LOGGER.error(\"too large: %s %s\", url, len(response.text))\n            else:\n                html_str = decode_response(response)\n                \n        except (requests.exceptions.MissingSchema, requests.exceptions.InvalidURL):\n            LOGGER.error(\"malformed URL: %s\", url)\n        except requests.exceptions.TooManyRedirects:\n            LOGGER.error(\"too many redirects: %s\", url)\n        except requests.exceptions.SSLError as err:\n            LOGGER.error(\"SSL: %s %s\", url, err)\n        except (socket.timeout, requests.exceptions.ConnectionError, \n               requests.exceptions.Timeout, socket.error, socket.gaierror) as err:\n            LOGGER.error(\"connection/timeout error: %s %s\", url, err)\n            \n        if is_threaded:\n            self._results[url] = html_str\n        return html_str\n\n    def fetch_urls(self, urls, request_args=None):\n        \"\"\"\n        Crawls the html content of all given urls in parallel.\n        :param urls: List of URLs to fetch\n        :param request_args: optional arguments that `request` takes\n        :return: Dictionary of URL to HTML content mappings\n        \"\"\"\n        threads = [\n            threading.Thread(target=self._fetch_url, args=(url, True, request_args))\n            for url in urls\n        ]\n        for thread in threads:\n            thread.start()\n        for thread in threads:\n            thread.join()\n\n        results = copy.deepcopy(self._results)\n        self._results = {}\n        return results",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport threading\nimport logging\nfrom typing import Dict, List, Optional\n\nclass SimpleCrawler:\n    def __init__(self):\n        self._results = {}\n        self.MAX_FILE_SIZE = 20 * 1024 * 1024\n        self.MIN_FILE_SIZE = 10\n        self.LOGGER = logging.getLogger(__name__)\n        self.USER_AGENT = 'SimpleCrawler/1.0'\n        self.HEADERS = {\n            'User-Agent': self.USER_AGENT,\n            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n            'Accept-Language': 'en-US,en;q=0.5',\n            'Accept-Encoding': 'gzip, deflate',\n            'Connection': 'keep-alive'\n        }\n\n    def fetch_url(self, url: str, request_args: Optional[Dict] = None) -> Optional[str]:\n        return self._fetch_url(url, False, request_args)\n\n    def _fetch_url(self, url: str, is_threaded: bool, request_args: Optional[Dict] = None) -> Optional[str]:\n        try:\n            headers = self.HEADERS.copy()\n            if request_args and 'headers' in request_args:\n                headers.update(request_args['headers'])\n\n            kwargs = {\n                'headers': headers,\n                'timeout': 30,\n                'stream': True\n            }\n            if request_args:\n                kwargs.update({k: v for k, v in request_args.items() if k != 'headers'})\n\n            response = requests.get(url, **kwargs)\n            response.raise_for_status()\n\n            content_length = int(response.headers.get('Content-Length', 0))\n            if content_length > self.MAX_FILE_SIZE or content_length < self.MIN_FILE_SIZE:\n                raise ValueError(f\"Content size {content_length} is out of allowed range\")\n\n            content = response.text\n            if not content:\n                raise ValueError(\"Empty content received\")\n\n            if is_threaded:\n                self._results[url] = content\n            return content\n\n        except Exception as e:\n            self.LOGGER.error(f\"Error fetching {url}: {str(e)}\")\n            if is_threaded:\n                self._results[url] = None\n            return None\n\n    def fetch_urls(self, urls: List[str], request_args: Optional[Dict] = None) -> Dict[str, Optional[str]]:\n        self._results = {}\n        threads = []\n\n        for url in urls:\n            thread = threading.Thread(\n                target=self._fetch_url,\n                args=(url, True, request_args)\n            threads.append(thread)\n            thread.start()\n\n        for thread in threads:\n            thread.join()\n\n        return self._results.copy()"
  },
  {
    "Task_id": 11,
    "Instruction": "Task Description: Implement a Python class to interact with Hurricane Electric's dynamic DNS API for updating DNS records programmatically.\n\nClass Description: HurricaneElectricAPI is a client class that handles communication with Hurricane Electric's DNS service to update DNS records. It supports both direct connections and proxy connections.\n\nAttributes:\n- SITE: [str] - The target domain for the Hurricane Electric API (dyn.dns.he.net)\n- METHOD: [str] - The HTTP method to use for requests (POST)\n- ACTION: [str] - The API endpoint path (nic/update)\n- TOKEN_PARAM: [str] - The parameter name for the authentication token (password)\n- TOKEN: [str] - The authentication token/password for API access\n- PROXY: [str/None] - Optional proxy server address\n\nMethods:\n- __init__(token=\"password\", proxy=None) -> None - Initializes the API client with optional token and proxy settings\n- _request(param=None, **params) -> str - Internal method to make HTTP requests to the API\n- update_record(domain, value, record_type=\"A\") -> str - Updates a DNS record with the given value\n\nMethod Details:\n1. _request(param=None, **params) -> str\n   - Makes an HTTPS request to the Hurricane Electric API\n   - Input:\n     - param: [dict/None] - Optional base parameters\n     - **params: Additional request parameters\n   - Output: [str] - API response string\n   - Raises Exception on HTTP errors or API failures\n\n2. update_record(domain, value, record_type=\"A\") -> str\n   - Updates a DNS record with the specified value\n   - Input:\n     - domain: [str] - The domain name to update\n     - value: [str] - The new IP address or value\n     - record_type: [str] - The DNS record type (default: \"A\")\n   - Output: [str] - Human-readable status message about the update\n   - Raises Exception if the update fails",
    "Canonical_solution": "from http.client import HTTPSConnection\nfrom urllib.parse import urlencode\nfrom logging import info, debug, warning\n\nclass HurricaneElectricAPI:\n    def __init__(self, token=\"password\", proxy=None):\n        self.SITE = \"dyn.dns.he.net\"\n        self.METHOD = \"POST\"\n        self.ACTION = \"nic/update\"\n        self.TOKEN_PARAM = \"password\"\n        self.TOKEN = token\n        self.PROXY = proxy\n\n    def _request(self, param=None, **params):\n        if param:\n            params.update(param)\n\n        params.update({self.TOKEN_PARAM: '***'})\n        info(\"%s/%s : %s\", self.SITE, self.ACTION, params)\n        params[self.TOKEN_PARAM] = self.TOKEN\n\n        if self.PROXY:\n            conn = HTTPSConnection(self.PROXY)\n            conn.set_tunnel(self.SITE, 443)\n        else:\n            conn = HTTPSConnection(self.SITE)\n\n        conn.request(self.METHOD, '/' + self.ACTION, urlencode(params), {\n            \"Content-type\": \"application/x-www-form-urlencoded\"\n        })\n        response = conn.getresponse()\n        res = response.read().decode('utf8')\n        conn.close()\n\n        if response.status < 200 or response.status >= 300:\n            warning('%s : error[%d]:%s', self.ACTION, response.status, res)\n            raise Exception(res)\n        else:\n            debug('%s : result:%s', self.ACTION, res)\n            if not res:\n                raise Exception(\"empty response\")\n            elif res[:5] == \"nochg\" or res[:4] == \"good\":\n                return res\n            else:\n                raise Exception(res)\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        info(\">>>>>%s(%s)\", domain, record_type)\n        res = self._request(hostname=domain, myip=value)\n        if res[:4] == \"good\":\n            result = \"Record updated. New IP is: \" + res[5:-1]\n        elif res[:5] == \"nochg\":\n            result = \"IP not changed. IP is: \" + res[6:-1]\n        else:\n            result = \"Record update failed.\"\n        return result",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nimport urllib.parse\n\nclass HurricaneElectricAPI:\n    SITE = \"https://dyn.dns.he.net\"\n    METHOD = \"POST\"\n    ACTION = \"nic/update\"\n    TOKEN_PARAM = \"password\"\n\n    def __init__(self, token=\"password\", proxy=None):\n        self.TOKEN = token\n        self.PROXY = proxy\n\n    def _request(self, param=None, **params):\n        if param is None:\n            param = {}\n        param.update(params)\n        param[self.TOKEN_PARAM] = self.TOKEN\n\n        proxies = None\n        if self.PROXY:\n            proxies = {\n                'http': self.PROXY,\n                'https': self.PROXY\n            }\n\n        url = f\"{self.SITE}/{self.ACTION}\"\n        headers = {\n            'User-Agent': 'HurricaneElectricAPI/1.0',\n            'Content-Type': 'application/x-www-form-urlencoded'\n        }\n\n        response = requests.request(\n            self.METHOD,\n            url,\n            data=urllib.parse.urlencode(param),\n            headers=headers,\n            proxies=proxies\n        )\n\n        if response.status_code != 200:\n            raise Exception(f\"HTTP Error: {response.status_code}\")\n\n        return response.text\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        params = {\n            'hostname': domain,\n            record_type: value\n        }\n\n        response = self._request(**params)\n        if response.startswith('good') or response.startswith('nochg'):\n            return f\"Successfully updated {domain} to {value}\"\n        else:\n            raise Exception(f\"Failed to update record: {response}\")"
  },
  {
    "Task_id": 110,
    "Instruction": "Task Description: Create a function that retrieves word embeddings from either a local API or OpenAI's API based on the specified model.\n\nFunction Description: The function `get_embedding_from_api` fetches a word embedding vector by making an HTTP POST request to a local API endpoint or using OpenAI's API, depending on the model specified. It handles the response, checks for errors, and returns the embedding as a NumPy array if successful.\n\nInput:\n- `word` (str): The input word or text for which to generate an embedding.\n- `model` (str, optional): The name of the model to use for generating embeddings. Defaults to \"vicuna-7b-v1.1\". If the model name contains \"ada\", it uses OpenAI's API.\n\nOutput:\n- Returns a NumPy array containing the word embedding if the request is successful.\n- Returns None if an error occurs (prints the error details to the console).",
    "Canonical_solution": "import json\nimport requests\nimport numpy as np\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import numpy as np\nimport requests\nimport openai\nimport os\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.1\"):\n    try:\n        if \"ada\" in model.lower():\n            response = openai.Embedding.create(\n                input=word,\n                model=model\n            )\n            embedding = np.array(response['data'][0]['embedding'])\n        else:\n            local_api_url = \"http://localhost:8000/embed\"\n            payload = {\n                \"input\": word,\n                \"model\": model\n            }\n            response = requests.post(local_api_url, json=payload)\n            response.raise_for_status()\n            data = response.json()\n            embedding = np.array(data['embedding'])\n        return embedding\n    except Exception as e:\n        print(f\"Error fetching embedding: {e}\")\n        return None"
  },
  {
    "Task_id": 112,
    "Instruction": "Task Description: Create a Python class that implements an HTTP client for interacting with the Pulsstory ExternalSyncAPI, handling authentication, data retrieval, and activity synchronization.\n\nClass Description: The PulsstoryHTTPClient class provides methods to interact with the Pulsstory API, including user authentication, activity download/upload, and token management. It handles HTTP requests, error responses, and data serialization.\n\nAttributes:\n- URLBase: str - The base URL for all API endpoints\n- PULSSTORY_CLIENT_ID: str - Client ID for API authentication\n- PULSSTORY_CLIENT_SECRET: str - Client secret for API authentication\n\nMethods:\n- _api_request(endpoint: str, service_record: object = None, method: str = \"POST\", data: dict = None, headers: dict = None, files: dict = None) -> dict - Internal method to make API requests and handle responses\n- _apiData(service_record: object) -> dict - Helper method to prepare authentication data\n- get_user_id(service_record: object) -> str - Retrieves user ID from the API\n- get_api_uris(service_record: object) -> dict - Gets API endpoint URIs from the service\n- retrieve_authorization_token(code: str) -> str - Exchanges an authorization code for an access token\n- revoke_authorization(service_record: object) -> None - Revokes an existing authorization\n- download_activity(service_record: object, activity_id: str) -> dict - Downloads activity data\n- upload_activity(service_record: object, upload_data: dict, uris: dict) -> str - Uploads activity data as a compressed file",
    "Canonical_solution": "from tapiriik.settings import PULSSTORY_CLIENT_ID, PULSSTORY_CLIENT_SECRET\nimport requests\nimport urllib.parse\nimport json\n\nclass PulsstoryHTTPClient:\n    def __init__(self, base_url, client_id=None, client_secret=None):\n        self.URLBase = base_url\n        self.PULSSTORY_CLIENT_ID = client_id or PULSSTORY_CLIENT_ID\n        self.PULSSTORY_CLIENT_SECRET = client_secret or PULSSTORY_CLIENT_SECRET\n\n    def _api_request(self, endpoint, service_record=None, method=\"POST\", data=None, headers=None, files=None):\n        url = self.URLBase + endpoint\n        request_data = self._apiData(service_record) if service_record else {}\n        if data:\n            request_data.update(data)\n        \n        if method == \"POST\":\n            response = requests.post(url, data=request_data, headers=headers, files=files)\n        else:\n            raise ValueError(\"Unsupported HTTP method\")\n\n        if response.status_code != 200:\n            if response.status_code in [401, 403]:\n                raise APIException(\"No authorization for request\", block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))\n            raise APIException(f\"Request failed with status {response.status_code}: {response.text}\")\n\n        return response.json()\n\n    def _apiData(self, service_record):\n        return {\"access_token\": service_record.Authorization[\"Token\"]}\n\n    def get_user_id(self, service_record):\n        return self._api_request(\"/ExternalSyncAPI/GetUserId\", service_record)[\"userID\"]\n\n    def get_api_uris(self, service_record):\n        uris = self._api_request(\"/ExternalSyncAPI/Uris\", service_record)\n        for k in uris.keys():\n            if isinstance(uris[k], str):\n                uris[k] = self.URLBase + uris[k]\n        return uris\n\n    def retrieve_authorization_token(self, code):\n        params = {\n            \"code\": code,\n            \"client_id\": self.PULSSTORY_CLIENT_ID,\n            \"client_secret\": self.PULSSTORY_CLIENT_SECRET\n        }\n        response = requests.post(\n            self.URLBase + \"/ExternalSyncAPI/GenerateToken\",\n            data=urllib.parse.urlencode(params),\n            headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}\n        )\n        if response.status_code != 200:\n            raise APIException(\"Invalid code\")\n        return response.json()[\"access_token\"]\n\n    def revoke_authorization(self, service_record):\n        response = requests.post(self.URLBase + \"/ExternalSyncAPI/Deauthorize\", data=self._apiData(service_record))\n        if response.status_code not in [200, 204]:\n            raise APIException(f\"Unable to deauthorize token, status {response.status_code}\")\n\n    def download_activity(self, service_record, activity_id):\n        return self._api_request(activity_id, service_record)\n\n    def upload_activity(self, service_record, upload_data, uris):\n        json_data = json.dumps(upload_data)\n        buffer = io.BytesIO()\n        with zipfile.ZipFile(buffer, 'w') as myzip:\n            myzip.writestr('activity.txt', json_data, compress_type=zipfile.ZIP_DEFLATED)\n        files = {\"data\": buffer.getvalue()}\n        return self._api_request(uris[\"upload_activity_zip\"], service_record, files=files)[\"Id\"]",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport json\n\nclass PulsstoryHTTPClient:\n    URLBase = \"https://api.pulsstory.com\"\n    PULSSTORY_CLIENT_ID = \"pulsstory_client_id\"\n    PULSSTORY_CLIENT_SECRET = \"pulsstory_client_secret\"\n\n    def _api_request(self, endpoint, service_record=None, method=\"POST\", data=None, headers=None, files=None):\n        url = f\"{self.URLBase}{endpoint}\"\n        if headers is None:\n            headers = {}\n        if data is None:\n            data = {}\n        if service_record:\n            headers.update({\"Authorization\": f\"Bearer {service_record.access_token}\"})\n        try:\n            response = requests.request(method, url, headers=headers, json=data, files=files)\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.HTTPError as e:\n            error_msg = f\"HTTP Error: {e.response.status_code} - {e.response.text}\"\n            raise Exception(error_msg)\n        except requests.exceptions.RequestException as e:\n            raise Exception(f\"Request failed: {str(e)}\")\n\n    def _apiData(self, service_record):\n        return {\n            \"client_id\": self.PULSSTORY_CLIENT_ID,\n            \"client_secret\": self.PULSSTORY_CLIENT_SECRET,\n            \"refresh_token\": service_record.refresh_token\n        }\n\n    def get_user_id(self, service_record):\n        endpoint = \"/v1/user\"\n        response = self._api_request(endpoint, service_record, \"GET\")\n        return response.get(\"user_id\")\n\n    def get_api_uris(self, service_record):\n        endpoint = \"/v1/uris\"\n        response = self._api_request(endpoint, service_record, \"GET\")\n        return response\n\n    def retrieve_authorization_token(self, code):\n        endpoint = \"/oauth/token\"\n        data = {\n            \"client_id\": self.PULSSTORY_CLIENT_ID,\n            \"client_secret\": self.PULSSTORY_CLIENT_SECRET,\n            \"code\": code,\n            \"grant_type\": \"authorization_code\"\n        }\n        response = self._api_request(endpoint, data=data)\n        return response.get(\"access_token\")\n\n    def revoke_authorization(self, service_record):\n        endpoint = \"/oauth/revoke\"\n        data = self._apiData(service_record)\n        self._api_request(endpoint, data=data)\n\n    def download_activity(self, service_record, activity_id):\n        endpoint = f\"/v1/activities/{activity_id}\"\n        response = self._api_request(endpoint, service_record, \"GET\")\n        return response\n\n    def upload_activity(self, service_record, upload_data, uris):\n        endpoint = uris.get(\"upload_uri\")\n        files = {\n            'file': (upload_data['filename'], upload_data['file_data'], 'application/zip')\n        }\n        response = self._api_request(endpoint, service_record, files=files)\n        return response.get(\"activity_id\")"
  },
  {
    "Task_id": 113,
    "Instruction": "Task Description: Implement a Python class that interacts with OpenAI's GPT-4 Vision API to process image and text prompts, handling image encoding and API communication with retry logic.\n\nClass Description: OpenAIGPT4Vision is a client for OpenAI's GPT-4 Vision API that processes both text and image inputs. It handles image conversion to base64 format, API request construction, and response processing with built-in retry mechanisms for failed requests.\n\nAttributes:\napi_key: str - The OpenAI API key for authentication\nheaders: dict - HTTP headers including Content-Type and Authorization\nmax_new_tokens: int - Maximum number of tokens to generate in the response (default: 256)\n\nMethods:\n__init__(api_key: str, max_new_tokens: int = 256) -> None - Initializes the API client with authentication and configuration\nencode_image_to_base64(raw_image_data: PIL.Image.Image) -> str - Static method that converts PIL Image to base64 encoded string\ngenerate(text_prompt: str, raw_image_data: Union[PIL.Image.Image, dict, str]) -> str - Processes text and image input, calls the API, and returns the generated content\n_get_pil_image(raw_image_data: Union[PIL.Image.Image, dict, str]) -> PIL.Image.Image - Static helper method to convert various image formats to PIL Image object",
    "Canonical_solution": "import requests\nimport base64\nfrom PIL import Image\nimport io\nimport time\n\nclass OpenAIGPT4Vision:\n    def __init__(self, api_key: str, max_new_tokens: int = 256):\n        self.api_key = api_key\n        self.headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n        self.max_new_tokens = max_new_tokens\n\n    @staticmethod\n    def encode_image_to_base64(raw_image_data) -> str:\n        if isinstance(raw_image_data, Image.Image):\n            buffered = io.BytesIO()\n            raw_image_data.save(buffered, format=\"JPEG\")\n            return base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n        raise ValueError(\"The input image data must be a PIL.Image.Image\")\n\n    def generate(self, text_prompt: str, raw_image_data):\n        raw_image_data = self._get_pil_image(raw_image_data).convert(\"RGB\")\n        base64_image = self.encode_image_to_base64(raw_image_data)\n\n        payload = {\n            \"model\": \"gpt-4-vision-preview\",\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": text_prompt},\n                        {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{base64_image}\"}},\n                    ],\n                }\n            ],\n            \"max_tokens\": self.max_new_tokens,\n        }\n\n        retry = True\n        retry_times = 0\n        while retry and retry_times < 5:\n            response = requests.post(\"https://api.openai.com/v1/chat/completions\", headers=self.headers, json=payload)\n            if response.status_code == 200:\n                response_data = response.json()\n                return response_data[\"choices\"][0][\"message\"][\"content\"]\n            else:\n                print(f\"Failed to connect to OpenAI API: {response.status_code} - {response.text}. Retrying...\")\n                time.sleep(10)\n                retry_times += 1\n        return \"Failed to connect to OpenAI GPT4V API\"\n\n    @staticmethod\n    def _get_pil_image(raw_image_data) -> Image.Image:\n        if isinstance(raw_image_data, Image.Image):\n            return raw_image_data\n        elif isinstance(raw_image_data, dict) and \"bytes\" in raw_image_data:\n            return Image.open(io.BytesIO(raw_image_data[\"bytes\"]))\n        elif isinstance(raw_image_data, str):\n            image_bytes = base64.b64decode(raw_image_data)\n            return Image.open(io.BytesIO(image_bytes))\n        else:\n            raise ValueError(\"Unsupported image data format\")",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport base64\nimport requests\nimport time\nfrom io import BytesIO\nfrom PIL import Image\nfrom typing import Union\n\nclass OpenAIGPT4Vision:\n    def __init__(self, api_key: str, max_new_tokens: int = 256):\n        self.api_key = api_key\n        self.headers = {\n            \"Content-Type\": \"application/json\",\n            \"Authorization\": f\"Bearer {self.api_key}\"\n        }\n        self.max_new_tokens = max_new_tokens\n\n    @staticmethod\n    def encode_image_to_base64(raw_image_data: Image.Image) -> str:\n        buffered = BytesIO()\n        raw_image_data.save(buffered, format=\"JPEG\")\n        return base64.b64encode(buffered.getvalue()).decode('utf-8')\n\n    @staticmethod\n    def _get_pil_image(raw_image_data: Union[Image.Image, dict, str]) -> Image.Image:\n        if isinstance(raw_image_data, Image.Image):\n            return raw_image_data\n        elif isinstance(raw_image_data, dict):\n            return Image.open(BytesIO(raw_image_data['bytes']))\n        elif isinstance(raw_image_data, str):\n            if os.path.isfile(raw_image_data):\n                return Image.open(raw_image_data)\n            else:\n                return Image.open(BytesIO(base64.b64decode(raw_image_data)))\n        else:\n            raise ValueError(\"Unsupported image data format\")\n\n    def generate(self, text_prompt: str, raw_image_data: Union[Image.Image, dict, str]) -> str:\n        image = self._get_pil_image(raw_image_data)\n        base64_image = self.encode_image_to_base64(image)\n        \n        payload = {\n            \"model\": \"gpt-4-vision-preview\",\n            \"messages\": [\n                {\n                    \"role\": \"user\",\n                    \"content\": [\n                        {\"type\": \"text\", \"text\": text_prompt},\n                        {\n                            \"type\": \"image_url\",\n                            \"image_url\": {\n                                \"url\": f\"data:image/jpeg;base64,{base64_image}\"\n                            }\n                        }\n                    ]\n                }\n            ],\n            \"max_tokens\": self.max_new_tokens\n        }\n\n        max_retries = 3\n        retry_delay = 1\n        for attempt in range(max_retries):\n            try:\n                response = requests.post(\n                    \"https://api.openai.com/v1/chat/completions\",\n                    headers=self.headers,\n                    json=payload\n                )\n                response.raise_for_status()\n                return response.json()['choices'][0]['message']['content']\n            except requests.exceptions.RequestException as e:\n                if attempt == max_retries - 1:\n                    raise\n                time.sleep(retry_delay)\n                retry_delay *= 2"
  },
  {
    "Task_id": 114,
    "Instruction": "Task Description: Write a Python function that downloads a schema file from a given URL and returns its local path. The function should handle cases where the file already exists locally and properly manage HTTP request failures.\n\nFunction Description: The function `download_schema_file` downloads a JSON schema file from a specified URL to a local path. It checks if the file already exists locally before attempting to download. If the download fails, it issues a warning and exits the script. The function uses the `requests` library for HTTP operations and handles file paths using `pathlib.Path`.\n\nInput: \n- `envs` (Dict[str, Any]): A dictionary containing environment variables with the following keys:\n  - `SIGMA_SCHEMA_FILE`: Optional local path for the schema file. If not provided, defaults to \"sigma-schema.json\" in the workspace.\n  - `SIGMA_SCHEMA_URL`: URL from which to download the schema file.\n  - `GITHUB_WORKSPACE`: Base directory path for resolving relative file paths.\n\nOutput: \n- `Path | NoReturn`: Returns the absolute path to the downloaded schema file if successful. If the download fails, the function exits the script with status code -1.",
    "Canonical_solution": "import os\nimport requests\nimport warnings\nfrom pathlib import Path\nfrom typing import Any, Dict, NoReturn\n\ndef download_schema_file(envs: Dict[str, Any]) -> Path | NoReturn:\n    \"\"\"Download the schema file from the given URL and return its path.\n\n    Args:\n        envs (Dict[str, Any]): A dictionary containing the environment variables\n            used by the action.\n\n    Returns:\n        Path | NoReturn: Path to the schema file or exit the script if\n            the file could not be downloaded.\n    \"\"\"\n\n    schema_file = envs[\"SIGMA_SCHEMA_FILE\"]\n    schema_url = envs[\"SIGMA_SCHEMA_URL\"]\n    if not schema_file:\n        schema_file = envs[\"GITHUB_WORKSPACE\"] / \"sigma-schema.json\"\n    else:\n        schema_file = Path(schema_file)\n        if not schema_file.exists():\n            schema_file = envs[\"GITHUB_WORKSPACE\"] / schema_file\n\n    if not schema_file.exists():\n        response = requests.get(schema_url)\n        if response.status_code == 200:\n            with open(schema_file, \"wb\") as f:\n                f.write(response.content)\n        else:\n            warnings.warn(\n                f\"Failed to download schema file {schema_file}, skipping validation\"\n            )\n            os._exit(-1)\n    return (envs[\"GITHUB_WORKSPACE\"] / schema_file).absolute()",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport sys\nfrom pathlib import Path\nfrom typing import Dict, Any, NoReturn\nimport warnings\n\ndef download_schema_file(envs: Dict[str, Any]) -> Path:\n    schema_url = envs.get('SIGMA_SCHEMA_URL')\n    if not schema_url:\n        raise ValueError(\"SIGMA_SCHEMA_URL environment variable is required\")\n\n    workspace = Path(envs.get('GITHUB_WORKSPACE', '.'))\n    schema_file = envs.get('SIGMA_SCHEMA_FILE', 'sigma-schema.json')\n    local_path = workspace / schema_file\n\n    if local_path.exists():\n        return local_path\n\n    try:\n        response = requests.get(schema_url)\n        response.raise_for_status()\n        local_path.parent.mkdir(parents=True, exist_ok=True)\n        local_path.write_text(response.text)\n        return local_path\n    except requests.exceptions.RequestException as e:\n        warnings.warn(f\"Failed to download schema file from {schema_url}: {e}\")\n        sys.exit(-1)"
  },
  {
    "Task_id": 115,
    "Instruction": "Task Description: Create a Python function that fetches stock data from Yahoo Finance API for a given stock symbol using HTTP requests.\n\nFunction Description: The function should make an HTTP GET request to Yahoo Finance's quote API endpoint, handle potential errors, and return the stock data in a string format. It should validate the response status and structure before returning the data.\n\nInput: \n- symbol (str): The stock symbol to fetch data for (e.g., 'AAPL' for Apple Inc.)\n\nOutput: \n- str: A string representation of the stock data dictionary containing price, volume, and other relevant information from the API response\n\nRaises:\n- ValueError: If the HTTP request fails (non-200 status code) or if no data is found for the given symbol",
    "Canonical_solution": "import requests\n\ndef get_stock_data(symbol: str) -> str:\n    \"\"\"\n    Fetches stock data from Yahoo Finance for a given stock symbol.\n\n    Args:\n        symbol (str): The stock symbol to fetch data for (e.g., 'AAPL' for Apple Inc.).\n\n    Returns:\n        Dict[str, Any]: A dictionary containing stock data, including price, volume, and other relevant information.\n\n    Raises:\n        ValueError: If the stock symbol is invalid or data cannot be retrieved.\n    \"\"\"\n    url = f\"https://query1.finance.yahoo.com/v7/finance/quote?symbols={symbol}\"\n    response = requests.get(url)\n\n    if response.status_code != 200:\n        raise ValueError(f\"Error fetching data for symbol: {symbol}\")\n\n    data = response.json()\n    if (\n        \"quoteResponse\" not in data\n        or not data[\"quoteResponse\"][\"result\"]\n    ):\n        raise ValueError(f\"No data found for symbol: {symbol}\")\n\n    return str(data[\"quoteResponse\"][\"result\"][0])",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport json\n\ndef fetch_stock_data(symbol):\n    url = f\"https://query1.finance.yahoo.com/v8/finance/chart/{symbol}\"\n    params = {\n        'interval': '1d',\n        'range': '1d'\n    }\n    headers = {\n        'User-Agent': 'Mozilla/5.0'\n    }\n    \n    try:\n        response = requests.get(url, params=params, headers=headers)\n        response.raise_for_status()\n        \n        data = response.json()\n        if not data or 'chart' not in data or not data['chart']['result']:\n            raise ValueError(f\"No data found for symbol: {symbol}\")\n        \n        result = data['chart']['result'][0]\n        meta = result.get('meta', {})\n        indicators = result.get('indicators', {}).get('quote', [{}])[0]\n        \n        stock_data = {\n            'symbol': symbol,\n            'currency': meta.get('currency', 'N/A'),\n            'price': meta.get('regularMarketPrice', 'N/A'),\n            'previous_close': meta.get('chartPreviousClose', 'N/A'),\n            'open': indicators.get('open', ['N/A'])[0],\n            'high': indicators.get('high', ['N/A'])[0],\n            'low': indicators.get('low', ['N/A'])[0],\n            'volume': indicators.get('volume', ['N/A'])[0]\n        }\n        \n        return str(stock_data)\n    \n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Failed to fetch data for {symbol}: {str(e)}\")"
  },
  {
    "Task_id": 116,
    "Instruction": "Task Description: Create a Python class that interacts with a web API to submit crawling tasks and retrieve their results, supporting both asynchronous and synchronous operations.\n\nClass Description: The Crawl4AiTester class provides functionality to interact with a web API for submitting URL crawling tasks. It handles authentication, task submission, and result retrieval with both asynchronous (polling) and synchronous approaches.\n\nAttributes:\n- base_url: str - The base URL of the API server (default: \"http://localhost:11235\")\n- api_token: str - The authentication token for API access (can be set via environment variable)\n- headers: dict - HTTP headers including the authorization token if provided\n\nMethods:\n- __init__(base_url: str = \"http://localhost:11235\", api_token: str = None) -> None - Initializes the API client with optional base URL and API token\n- submit_and_wait(request_data: Dict[str, Any], timeout: int = 300) -> Dict[str, Any] - Submits an asynchronous crawling task and polls for results until completion or timeout\n- submit_sync(request_data: Dict[str, Any]) -> Dict[str, Any] - Submits a synchronous crawling task and returns the immediate response",
    "Canonical_solution": "import requests\nimport os\nfrom typing import Dict, Any\n\nclass Crawl4AiTester:\n    def __init__(self, base_url: str = \"http://localhost:11235\", api_token: str = None):\n        self.base_url = base_url\n        self.api_token = api_token or os.getenv('CRAWL4AI_API_TOKEN')\n        self.headers = {'Authorization': f'Bearer {self.api_token}'} if self.api_token else {}\n        \n    def submit_and_wait(self, request_data: Dict[str, Any], timeout: int = 300) -> Dict[str, Any]:\n        response = requests.post(f\"{self.base_url}/crawl\", json=request_data, headers=self.headers)\n        if response.status_code == 403:\n            raise Exception(\"API token is invalid or missing\")\n        task_id = response.json()[\"task_id\"]\n        \n        start_time = time.time()\n        while True:\n            if time.time() - start_time > timeout:\n                raise TimeoutError(f\"Task {task_id} did not complete within {timeout} seconds\")\n                \n            result = requests.get(f\"{self.base_url}/task/{task_id}\", headers=self.headers)\n            status = result.json()\n            \n            if status[\"status\"] == \"failed\":\n                raise Exception(f\"Task failed: {status.get('error')}\")\n                \n            if status[\"status\"] == \"completed\":\n                return status\n                \n            time.sleep(2)\n            \n    def submit_sync(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n        response = requests.post(f\"{self.base_url}/crawl_sync\", json=request_data, headers=self.headers, timeout=60)\n        if response.status_code == 408:\n            raise TimeoutError(\"Task did not complete within server timeout\")\n        response.raise_for_status()\n        return response.json()",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport time\nimport requests\nfrom typing import Dict, Any\n\nclass Crawl4AiTester:\n    def __init__(self, base_url: str = \"http://localhost:11235\", api_token: str = None) -> None:\n        self.base_url = base_url\n        self.api_token = api_token or os.getenv('CRAWL4AI_API_TOKEN')\n        self.headers = {'Authorization': f'Bearer {self.api_token}'} if self.api_token else {}\n\n    def submit_and_wait(self, request_data: Dict[str, Any], timeout: int = 300) -> Dict[str, Any]:\n        response = requests.post(f\"{self.base_url}/api/submit\", json=request_data, headers=self.headers)\n        response.raise_for_status()\n        task_id = response.json().get('task_id')\n        \n        start_time = time.time()\n        while time.time() - start_time < timeout:\n            result_response = requests.get(f\"{self.base_url}/api/result/{task_id}\", headers=self.headers)\n            result_response.raise_for_status()\n            result_data = result_response.json()\n            \n            if result_data.get('status') == 'completed':\n                return result_data\n            elif result_data.get('status') == 'failed':\n                raise Exception(f\"Task failed: {result_data.get('error', 'Unknown error')}\")\n            \n            time.sleep(1)\n        \n        raise TimeoutError(\"Task did not complete within the specified timeout\")\n\n    def submit_sync(self, request_data: Dict[str, Any]) -> Dict[str, Any]:\n        response = requests.post(f\"{self.base_url}/api/submit_sync\", json=request_data, headers=self.headers)\n        response.raise_for_status()\n        return response.json()"
  },
  {
    "Task_id": 117,
    "Instruction": "Task Description: Implement a Python class that serves as a client for interacting with the Telegram Bot API, enabling message and file sending functionality.\n\nClass Description: The TelegramClient class provides methods to send messages and files to a specified Telegram chat using the Telegram Bot API. It handles HTTP requests and responses, including error logging.\n\nAttributes:\n- chat_id: [Union[int, str]] - The unique identifier for the target chat or username of the target channel (in format @channelusername)\n- thread_id: [int] - The unique identifier for the target message thread (topic) in a forum-style chat\n- bot_token: [str] - The authentication token for the Telegram bot\n\nMethods:\n- send_message(message: str, disable_links_preview: bool = True) -> None - Sends a text message to the specified chat. Uses Markdown formatting by default and can disable web page previews.\n  Input:\n    - message: [str] - The text message to be sent\n    - disable_links_preview: [bool] - Whether to disable web page previews for links in the message (default: True)\n  Output: None (logs errors if request fails)\n\n- send_file(file_name: str, contents: bytes) -> None - Sends a file to the specified chat, automatically handling different file types (photos vs documents) and converting SVG to PNG if needed.\n  Input:\n    - file_name: [str] - The name of the file to be sent\n    - contents: [bytes] - The binary content of the file\n  Output: None (logs errors if request fails)",
    "Canonical_solution": "import logging\nimport os\nfrom typing import Union\nimport requests\n\nTELEGRAM_BASE_URL = os.environ.get(\"TELEGRAM_BASE_URL\", \"https://api.telegram.org\")\n\nclass TelegramClient:\n    def __init__(self, chat_id: Union[int, str], thread_id: int, bot_token: str):\n        self.chat_id = int(chat_id)\n        self.thread_id = thread_id\n        self.bot_token = bot_token\n\n    def send_message(self, message: str, disable_links_preview: bool = True):\n        url = f\"{TELEGRAM_BASE_URL}/bot{self.bot_token}/sendMessage\"\n        message_json = {\n            \"chat_id\": self.chat_id,\n            \"message_thread_id\": self.thread_id,\n            \"disable_web_page_preview\": disable_links_preview,\n            \"parse_mode\": \"Markdown\",\n            \"text\": message,\n        }\n        response = requests.post(url, json=message_json)\n\n        if response.status_code != 200:\n            logging.error(\n                f\"Failed to send telegram message: chat_id - {self.chat_id} reason - {response.reason} {response.text}\"\n            )\n\n    def send_file(self, file_name: str, contents: bytes):\n        file_type = \"Photo\" if is_image(file_name) else \"Document\"\n        url = f\"{TELEGRAM_BASE_URL}/bot{self.bot_token}/send{file_type}?chat_id={self.chat_id}\"\n        if file_name.endswith(SVG_SUFFIX):\n            contents = convert_svg_to_png(contents)\n            file_name = file_name.replace(SVG_SUFFIX, PNG_SUFFIX)\n\n        files = {file_type.lower(): (file_name, contents)}\n        response = requests.post(url, files=files)\n\n        if response.status_code != 200:\n            logging.error(\n                f\"Failed to send telegram file: chat_id - {self.chat_id} reason - {response.reason} {response.text}\"\n            )",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport logging\nfrom typing import Union\n\nclass TelegramClient:\n    def __init__(self, chat_id: Union[int, str], thread_id: int, bot_token: str):\n        self.chat_id = chat_id\n        self.thread_id = thread_id\n        self.bot_token = bot_token\n        self.base_url = f\"https://api.telegram.org/bot{self.bot_token}/\"\n        logging.basicConfig(level=logging.ERROR)\n\n    def send_message(self, message: str, disable_links_preview: bool = True) -> None:\n        url = f\"{self.base_url}sendMessage\"\n        params = {\n            'chat_id': self.chat_id,\n            'message_thread_id': self.thread_id,\n            'text': message,\n            'parse_mode': 'Markdown',\n            'disable_web_page_preview': disable_links_preview\n        }\n        try:\n            response = requests.post(url, params=params)\n            response.raise_for_status()\n        except Exception as e:\n            logging.error(f\"Failed to send message: {e}\")\n\n    def send_file(self, file_name: str, contents: bytes) -> None:\n        url = f\"{self.base_url}sendDocument\"\n        files = {'document': (file_name, contents)}\n        params = {\n            'chat_id': self.chat_id,\n            'message_thread_id': self.thread_id\n        }\n        try:\n            response = requests.post(url, params=params, files=files)\n            response.raise_for_status()\n        except Exception as e:\n            logging.error(f\"Failed to send file: {e}\")"
  },
  {
    "Task_id": 118,
    "Instruction": "Task Description: Create a Python class that implements HTTP client functionality with proxy support and retry mechanism for handling web requests.\n\nClass Description: The HTTPClient class provides methods for making HTTP GET and POST requests with configurable headers, proxy settings, and automatic retry on failure. It includes features like user-agent rotation and basic response validation.\n\nAttributes:\n- ua: [UserAgent] - Instance for generating random user-agent strings\n- proxyHost: [str] - Proxy server host address\n- proxyPort: [str] - Proxy server port number\n- proxyUser: [str] - Proxy authentication username\n- proxyPass: [str] - Proxy authentication password\n- proxies: [dict] - Configured proxy settings dictionary for HTTP/HTTPS\n\nMethods:\n- get_request(url, headers=None, params=None, timeout=4) -> [requests.Response or None] - Sends HTTP GET request to specified URL with optional headers and query parameters. Returns response if successful (status code 200 and no validation text), otherwise None after retries.\n- post_request(url, data=None, headers=None, timeout=4) -> [requests.Response or None] - Sends HTTP POST request to specified URL with optional data and headers. Returns response if successful (status code 200 and no validation text), otherwise None after retries.",
    "Canonical_solution": "import csv\nimport os\nimport re\nimport warnings\nimport requests\nfrom fake_useragent import UserAgent\nfrom scrapy import Selector\n\nclass HTTPClient:\n    def __init__(self):\n        self.ua = UserAgent()\n        self.proxyHost = \"http-dyn.abuyun.com\"\n        self.proxyPort = \"9020\"\n        self.proxyUser = \"\"\n        self.proxyPass = \"\"\n        \n        proxyMeta = \"http://%(user)s:%(pass)s@%(host)s:%(port)s\" % {\n            \"host\": self.proxyHost,\n            \"port\": self.proxyPort,\n            \"user\": self.proxyUser,\n            \"pass\": self.proxyPass,\n        }\n\n        self.proxies = {\n            \"http\": proxyMeta,\n            \"https\": proxyMeta,\n        }\n\n    def get_request(self, url, headers=None, params=None, timeout=4):\n        if headers is None:\n            headers = {\n                'sec-fetch-dest': 'document',\n                'sec-fetch-mode': 'navigate',\n                'sec-fetch-site': 'none',\n                'sec-fetch-user': '?1',\n                'upgrade-insecure-requests': '1',\n                'user-agent': self.ua.chrome\n            }\n            \n        request_count = 0\n        while request_count < 15:\n            try:\n                response = requests.get(url, headers=headers, proxies=self.proxies, params=params, timeout=timeout)\n                if response.status_code == 200 and ('\u8bbf\u95ee\u9a8c\u8bc1' not in response.text):\n                    return response\n                request_count += 1\n            except:\n                request_count += 1\n        return None\n\n    def post_request(self, url, data=None, headers=None, timeout=4):\n        if headers is None:\n            headers = {\n                'sec-fetch-dest': 'document',\n                'sec-fetch-mode': 'navigate',\n                'sec-fetch-site': 'none',\n                'sec-fetch-user': '?1',\n                'upgrade-insecure-requests': '1',\n                'user-agent': self.ua.chrome\n            }\n            \n        request_count = 0\n        while request_count < 15:\n            try:\n                response = requests.post(url, data=data, headers=headers, proxies=self.proxies, timeout=timeout)\n                if response.status_code == 200 and ('\u8bbf\u95ee\u9a8c\u8bc1' not in response.text):\n                    return response\n                request_count += 1\n            except:\n                request_count += 1\n        return None",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import random\nimport time\nimport requests\nfrom fake_useragent import UserAgent\n\nclass HTTPClient:\n    def __init__(self, proxy_host=None, proxy_port=None, proxy_user=None, proxy_pass=None):\n        self.ua = UserAgent()\n        self.proxyHost = proxy_host\n        self.proxyPort = proxy_port\n        self.proxyUser = proxy_user\n        self.proxyPass = proxy_pass\n        self.proxies = self._configure_proxies()\n\n    def _configure_proxies(self):\n        if not self.proxyHost or not self.proxyPort:\n            return None\n        proxy_url = f\"http://{self.proxyHost}:{self.proxyPort}\"\n        if self.proxyUser and self.proxyPass:\n            proxy_url = f\"http://{self.proxyUser}:{self.proxyPass}@{self.proxyHost}:{self.proxyPort}\"\n        return {\n            'http': proxy_url,\n            'https': proxy_url\n        }\n\n    def _make_request(self, method, url, headers=None, params=None, data=None, timeout=4, max_retries=3):\n        headers = headers or {}\n        if 'User-Agent' not in headers:\n            headers['User-Agent'] = self.ua.random\n\n        for attempt in range(max_retries):\n            try:\n                if method == 'GET':\n                    response = requests.get(\n                        url,\n                        headers=headers,\n                        params=params,\n                        proxies=self.proxies,\n                        timeout=timeout\n                    )\n                elif method == 'POST':\n                    response = requests.post(\n                        url,\n                        headers=headers,\n                        data=data,\n                        proxies=self.proxies,\n                        timeout=timeout\n                    )\n                else:\n                    raise ValueError(\"Unsupported HTTP method\")\n\n                if response.status_code == 200:\n                    return response\n            except (requests.exceptions.RequestException, requests.exceptions.Timeout) as e:\n                if attempt == max_retries - 1:\n                    return None\n                time.sleep(random.uniform(1, 3))\n        return None\n\n    def get_request(self, url, headers=None, params=None, timeout=4):\n        return self._make_request('GET', url, headers=headers, params=params, timeout=timeout)\n\n    def post_request(self, url, data=None, headers=None, timeout=4):\n        return self._make_request('POST', url, headers=headers, data=data, timeout=timeout)"
  },
  {
    "Task_id": 119,
    "Instruction": "Task Description: Create a Python class that serves as a base HTTP client for interacting with a RESTful API, handling various types of API calls (GET, POST, PUT, DELETE) with proper error handling and logging.\n\nClass Description: SXTBaseAPI is a base class that provides HTTP client functionality for making API calls to a specified endpoint. It handles authentication, request construction, response processing, and error handling. The class supports configurable API versions, network call simulation, and comprehensive logging.\n\nAttributes:\n- api_url: str - Base URL for the API endpoint (default: 'https://api.spaceandtime.app')\n- access_token: str - Authentication token for API access\n- logger: logging.Logger - Logger instance for recording API call information\n- network_calls_enabled: bool - Flag to enable/disable actual network calls (default: True)\n- standard_headers: dict - Default headers for API requests\n- versions: dict - Dictionary mapping API endpoints to their versions\n- APICALLTYPE: Enum - Reference to SXTApiCallTypes enum for request types\n\nMethods:\n- __init__(access_token: str = '', logger: logging.Logger = None) -> None - Initializes the API client with optional access token and logger\n- call_api(endpoint: str, auth_header: bool = True, request_type: str = SXTApiCallTypes.POST, header_parms: dict = {}, data_parms: dict = {}, query_parms: dict = {}, path_parms: dict = {}) -> tuple[bool, dict] - Makes an API call with specified parameters and returns success status with response data\n- __fakedata__(endpoint: str) -> Union[list, dict] - Generates mock data when network calls are disabled (for testing purposes)",
    "Canonical_solution": "import requests\nimport json\nimport logging\nfrom pathlib import Path\nfrom .sxtenums import SXTApiCallTypes\nfrom .sxtexceptions import SxTArgumentError, SxTAPINotDefinedError\nfrom .sxtbiscuits import SXTBiscuit\n\nclass SXTBaseAPI:\n    api_url = 'https://api.spaceandtime.app'\n    access_token = ''\n    logger: logging.Logger\n    network_calls_enabled: bool = True\n    standard_headers = {\n        \"accept\": \"application/json\",\n        \"content-type\": \"application/json\"\n    }\n    versions = {}\n    APICALLTYPE = SXTApiCallTypes\n\n    def __init__(self, access_token: str = '', logger: logging.Logger = None) -> None:\n        if logger:\n            self.logger = logger\n        else:\n            self.logger = logging.getLogger()\n            self.logger.setLevel(logging.INFO)\n            if len(self.logger.handlers) == 0:\n                self.logger.addHandler(logging.StreamHandler())\n\n        apiversionfile = Path(Path(__file__).resolve().parent / 'apiversions.json')\n        self.access_token = access_token\n        with open(apiversionfile, 'r') as fh:\n            content = fh.read()\n        self.versions = json.loads(content)\n\n    def call_api(self, endpoint: str,\n                 auth_header: bool = True,\n                 request_type: str = SXTApiCallTypes.POST,\n                 header_parms: dict = {},\n                 data_parms: dict = {},\n                 query_parms: dict = {},\n                 path_parms: dict = {}):\n        txt = 'response.text not available - are you sure you have the correct API Endpoint?'\n        statuscode = 555\n        response = {}\n\n        if not self.network_calls_enabled: return True, self.__fakedata__(endpoint)\n\n        def __handle_errors__(txt, ex, statuscode, responseobject, loggerobject):\n            loggerobject.error(txt)\n            rtn = {'text': txt}\n            rtn['error'] = str(ex)\n            rtn['status_code'] = statuscode\n            rtn['response_object'] = responseobject\n            return False, rtn\n\n        try:\n            if endpoint not in self.versions.keys():\n                raise SxTAPINotDefinedError(\"Endpoint not defined in API Lookup (apiversions.json). Please reach out to Space and Time for assistance. \\nAs a work-around, you can try manually adding the endpoint to the SXTBaseAPI.versions dictionary.\")\n            version = self.versions[endpoint]\n            self.logger.debug(f'API Call started for endpoint: {version}/{endpoint}')\n\n            if request_type not in SXTApiCallTypes:\n                msg = f'request_type must be of type SXTApiCallTypes, not { type(request_type) }'\n                raise SxTArgumentError(msg, logger=self.logger)\n\n            for name, value in path_parms.items():\n                endpoint = endpoint.replace(f'{{{name}}}', value)\n\n            if query_parms != {}:\n                endpoint = f'{endpoint}?' + '&'.join([f'{n}={v}' for n, v in query_parms.items()])\n\n            headers = {k: v for k, v in self.standard_headers.items()}\n            if auth_header: headers['authorization'] = f'Bearer {self.access_token}'\n            headers.update(header_parms)\n\n            url = f'{self.api_url}/{version}/{endpoint}'\n\n            match request_type:\n                case SXTApiCallTypes.POST: callfunc = requests.post\n                case SXTApiCallTypes.GET: callfunc = requests.get\n                case SXTApiCallTypes.PUT: callfunc = requests.put\n                case SXTApiCallTypes.DELETE: callfunc = requests.delete\n                case _: raise SxTArgumentError('Call type must be SXTApiCallTypes enum.', logger=self.logger)\n\n            response = callfunc(url=url, data=json.dumps(data_parms), headers=headers)\n            txt = response.text\n            statuscode = response.status_code\n            response.raise_for_status()\n\n            try:\n                self.logger.debug('API return content type: ' + response.headers.get('content-type', ''))\n                rtn = response.json()\n            except json.decoder.JSONDecodeError as ex:\n                rtn = {'text': txt, 'status_code': statuscode}\n\n            self.logger.debug(f'API call completed for endpoint: \"{endpoint}\" with result: {txt}')\n            return True, rtn\n\n        except requests.exceptions.RequestException as ex:\n            return __handle_errors__(txt, ex, statuscode, response, self.logger)\n        except SxTAPINotDefinedError as ex:\n            return __handle_errors__(txt, ex, statuscode, response, self.logger)\n        except Exception as ex:\n            return __handle_errors__(txt, ex, statuscode, response, self.logger)\n\n    def __fakedata__(self, endpoint: str):\n        if endpoint in ['sql', 'sql/dql']:\n            rtn = [{'id': '1', 'str': 'a', 'this_record': 'is a test'}]\n            rtn.append({'id': '2', 'str': 'b', 'this_record': 'is a test'})\n            rtn.append({'id': '3', 'str': 'c', 'this_record': 'is a test'})\n            return rtn\n        else:\n            return {'authCode': '469867d9660b67f8aa12b2',\n                    'accessToken': 'eyJ0eXBlIjoiYWNjZXNzIiwia2lkIjUxNDVkYmQtZGNmYi00ZjI4LTg3NzItZjVmNjNlMzcwM2JlIiwiYWxnIjoiRVMyNTYifQ.eyJpYXQiOjE2OTczOTM1MDIsIm5iZiI6MTY5NzM5MzUwMiwiZXhwIjoxNjk3Mzk1MDAyLCJ0eXBlIjoiYWNjZXNzIiwidXNlciI6InN0ZXBoZW4iLCJzdWJzY3JpcHRpb24iOiIzMWNiMGI0Yi0xMjZlLTRlM2MtYTdhMS1lNWRmNDc4YTBjMDUiLCJzZXNzaW9uIjoiMzNiNGRhMzYxZjZiNTM3MjZlYmYyNzU4Iiwic3NuX2V4cCI6MTY5NzQ3OTkwMjMxNSwiaXRlcmF0aW9uIjoiNDEwY2YyZTgyYWZlODdmNDRiMzE4NDFiIn0.kpvrG-ro13P1YeMF6sjLh8wn1rO3jpCVeTrzhDe16ZmJu4ik1amcYz9uQff_XQcwBDrpnCeD5ZZ9mHqb_basew',\n                    'refreshToken': 'eyJ0eXBlIjoicmVmcmVzaCIsImtpZCITQ1ZGJkLWRjZmItNGYyOC04NzcyLWY1ZjYzZTM3MDNiZSIsImFsZyI6IkVTMjU2In0.eyJpYXQiOjE2OTczOTM1MDIsIm5iZiI6MTY5NzM5MzUwMiwiZXhwIjoxNjk3Mzk1MzAyLCJ0eXBlIjoicmVmcmVzaCIsInVzZXIiOiJzdGVwaGVuIiwic3Vic2NyaXB0aW9uIjoiMzFjYjBiNGItMTI2ZS00ZTNjLWE3YTEtZTVkZjQ3OGEwYzA1Iiwic2Vzc2lvbiI6IjMzYjRkYTM2MWY2YjUzNzI2ZWJmMjc1OCIsInNzbl9leHAiOjE2OTc0Nzk5MDIzMTUsIml0ZXJhdGlvbiI6IjQxMGNmMmU4MmFmZTg3ZjQ0YjMxODQxYiJ9.3vVYpTGBjXIejlaacaZOh_59O9ETfbvTCWvldoi0ojyXTRkTmENVpQRbw7av7yMM2jA7SRdEPQGGjYmThCfk9w',\n                    'accessTokenExpires': 1973950023160,\n                    'refreshTokenExpires': 1973953023160}",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import logging\nimport requests\nfrom enum import Enum\nfrom typing import Union, Optional, Dict, Any, Tuple\n\nclass SXTApiCallTypes(Enum):\n    GET = 'GET'\n    POST = 'POST'\n    PUT = 'PUT'\n    DELETE = 'DELETE'\n\nclass SXTBaseAPI:\n    def __init__(self, access_token: str = '', logger: logging.Logger = None) -> None:\n        self.api_url = 'https://api.spaceandtime.app'\n        self.access_token = access_token\n        self.logger = logger if logger else logging.getLogger(__name__)\n        self.network_calls_enabled = True\n        self.standard_headers = {\n            'Content-Type': 'application/json',\n            'Accept': 'application/json'\n        }\n        self.versions = {}\n        self.APICALLTYPE = SXTApiCallTypes\n\n    def call_api(self, endpoint: str, auth_header: bool = True, request_type: SXTApiCallTypes = SXTApiCallTypes.POST, \n                 header_parms: Dict[str, Any] = {}, data_parms: Dict[str, Any] = {}, \n                 query_parms: Dict[str, Any] = {}, path_parms: Dict[str, Any] = {}) -> Tuple[bool, Dict[str, Any]]:\n        if not self.network_calls_enabled:\n            fake_data = self.__fakedata__(endpoint)\n            return (True, fake_data) if fake_data else (False, {'error': 'No fake data available'})\n\n        headers = {**self.standard_headers, **header_parms}\n        if auth_header and self.access_token:\n            headers['Authorization'] = f'Bearer {self.access_token}'\n\n        url = f\"{self.api_url}{endpoint}\"\n        if path_parms:\n            url = url.format(**path_parms)\n\n        try:\n            response = None\n            if request_type == SXTApiCallTypes.GET:\n                response = requests.get(url, headers=headers, params=query_parms)\n            elif request_type == SXTApiCallTypes.POST:\n                response = requests.post(url, headers=headers, params=query_parms, json=data_parms)\n            elif request_type == SXTApiCallTypes.PUT:\n                response = requests.put(url, headers=headers, params=query_parms, json=data_parms)\n            elif request_type == SXTApiCallTypes.DELETE:\n                response = requests.delete(url, headers=headers, params=query_parms)\n\n            if response is None:\n                raise ValueError(\"Invalid request type\")\n\n            response.raise_for_status()\n            return (True, response.json())\n\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"API call failed: {str(e)}\")\n            return (False, {'error': str(e)})\n        except Exception as e:\n            self.logger.error(f\"Unexpected error: {str(e)}\")\n            return (False, {'error': str(e)})\n\n    def __fakedata__(self, endpoint: str) -> Union[list, dict]:\n        return {}"
  },
  {
    "Task_id": 123,
    "Instruction": "Task Description: Create a FastAPI-based HTTP server for handling image segmentation and processing requests using SAM (Segment Anything Model) and related models.\n\nClass Description: SamAPI is a class that encapsulates all the route handlers for a FastAPI application dealing with image segmentation tasks. It provides endpoints for model prediction, mask dilation, and various segmentation techniques.\n\nAttributes:\n- app: FastAPI - The FastAPI application instance that will handle HTTP requests\n\nMethods:\n- __init__(app: FastAPI) -> None - Initializes the SamAPI class with a FastAPI instance and sets up routes\n- _setup_routes() -> None - Private method that defines all API endpoints and their handlers\n\nRoute Handlers (methods defined within _setup_routes):\n- heartbeat() -> Dict - Simple health check endpoint\n- api_sam_model() -> List[str] - Returns available SAM models\n- api_sam_predict(payload: SamPredictRequest) -> Any - Handles SAM model prediction requests\n- api_dino_predict(payload: DINOPredictRequest) -> Any - Handles DINO model prediction requests\n- api_dilate_mask(payload: DilateMaskRequest) -> Any - Handles mask dilation requests\n- api_controlnet_seg(payload: ControlNetSegRequest, autosam_conf: AutoSAMConfig) -> Any - Handles ControlNet segmentation requests\n- api_category_mask(payload: CategoryMaskRequest, autosam_conf: AutoSAMConfig) -> Any - Handles category-specific mask generation requests\n\nSupporting Models (Pydantic BaseModel classes):\n- SamPredictRequest - Request model for SAM predictions\n- DINOPredictRequest - Request model for DINO predictions\n- DilateMaskRequest - Request model for mask dilation\n- AutoSAMConfig - Configuration model for AutoSAM parameters\n- ControlNetSegRequest - Request model for ControlNet segmentation\n- CategoryMaskRequest - Request model for category-specific masking",
    "Canonical_solution": "from fastapi import FastAPI, Body\nfrom pydantic import BaseModel\nfrom typing import Any, Optional, List\nfrom PIL import Image\nimport numpy as np\nimport os\n\nclass SamAPI:\n    def __init__(self, app: FastAPI):\n        self.app = app\n        self._setup_routes()\n\n    class SamPredictRequest(BaseModel):\n        sam_model_name: str = \"sam_vit_h_4b8939.pth\"\n        input_image: str\n        sam_positive_points: List[List[float]] = []\n        sam_negative_points: List[List[float]] = []\n        dino_enabled: bool = False\n        dino_model_name: Optional[str] = \"GroundingDINO_SwinT_OGC (694MB)\"\n        dino_text_prompt: Optional[str] = None\n        dino_box_threshold: Optional[float] = 0.3\n        dino_preview_checkbox: bool = False\n        dino_preview_boxes_selection: Optional[List[int]] = None\n\n    class DINOPredictRequest(BaseModel):\n        input_image: str\n        dino_model_name: str = \"GroundingDINO_SwinT_OGC (694MB)\"\n        text_prompt: str\n        box_threshold: float = 0.3\n\n    class DilateMaskRequest(BaseModel):\n        input_image: str\n        mask: str\n        dilate_amount: int = 10\n\n    class AutoSAMConfig(BaseModel):\n        points_per_side: Optional[int] = 32\n        points_per_batch: int = 64\n        pred_iou_thresh: float = 0.88\n        stability_score_thresh: float = 0.95\n        stability_score_offset: float = 1.0\n        box_nms_thresh: float = 0.7\n        crop_n_layers: int = 0\n        crop_nms_thresh: float = 0.7\n        crop_overlap_ratio: float = 512 / 1500\n        crop_n_points_downscale_factor: int = 1\n        min_mask_region_area: int = 0\n\n    class ControlNetSegRequest(BaseModel):\n        sam_model_name: str = \"sam_vit_h_4b8939.pth\"\n        input_image: str\n        processor: str = \"seg_ofade20k\"\n        processor_res: int = 512\n        pixel_perfect: bool = False\n        resize_mode: Optional[int] = 1\n        target_W: Optional[int] = None\n        target_H: Optional[int] = None\n\n    class CategoryMaskRequest(BaseModel):\n        sam_model_name: str = \"sam_vit_h_4b8939.pth\"\n        processor: str = \"seg_ofade20k\"\n        processor_res: int = 512\n        pixel_perfect: bool = False\n        resize_mode: Optional[int] = 1\n        target_W: Optional[int] = None\n        target_H: Optional[int] = None\n        category: str\n        input_image: str\n\n    def _setup_routes(self):\n        @self.app.get(\"/sam/heartbeat\")\n        async def heartbeat():\n            return {\"msg\": \"Success!\"}\n\n        @self.app.get(\"/sam/sam-model\")\n        async def api_sam_model() -> List[str]:\n            return sam_model_list\n\n        @self.app.post(\"/sam/sam-predict\")\n        async def api_sam_predict(payload: SamPredictRequest = Body(...)) -> Any:\n            payload.input_image = decode_to_pil(payload.input_image).convert('RGBA')\n            sam_output_mask_gallery, sam_message = sam_predict(\n                payload.sam_model_name,\n                payload.input_image,\n                payload.sam_positive_points,\n                payload.sam_negative_points,\n                payload.dino_enabled,\n                payload.dino_model_name,\n                payload.dino_text_prompt,\n                payload.dino_box_threshold,\n                payload.dino_preview_checkbox,\n                payload.dino_preview_boxes_selection)\n            result = {\"msg\": sam_message}\n            if len(sam_output_mask_gallery) == 9:\n                result[\"blended_images\"] = list(map(encode_to_base64, sam_output_mask_gallery[:3]))\n                result[\"masks\"] = list(map(encode_to_base64, sam_output_mask_gallery[3:6]))\n                result[\"masked_images\"] = list(map(encode_to_base64, sam_output_mask_gallery[6:]))\n            return result\n\n        @self.app.post(\"/sam/dino-predict\")\n        async def api_dino_predict(payload: DINOPredictRequest = Body(...)) -> Any:\n            payload.input_image = decode_to_pil(payload.input_image)\n            dino_output_img, _, dino_msg = dino_predict(\n                payload.input_image,\n                payload.dino_model_name,\n                payload.text_prompt,\n                payload.box_threshold)\n            return {\n                \"msg\": dino_msg[\"value\"] if \"value\" in dino_msg else \"Done\",\n                \"image_with_box\": encode_to_base64(dino_output_img) if dino_output_img is not None else None,\n            }\n\n        @self.app.post(\"/sam/dilate-mask\")\n        async def api_dilate_mask(payload: DilateMaskRequest = Body(...)) -> Any:\n            payload.input_image = decode_to_pil(payload.input_image).convert(\"RGBA\")\n            payload.mask = decode_to_pil(payload.mask)\n            dilate_result = list(map(encode_to_base64, update_mask(payload.mask, 0, payload.dilate_amount, payload.input_image)))\n            return {\"blended_image\": dilate_result[0], \"mask\": dilate_result[1], \"masked_image\": dilate_result[2]}\n\n        @self.app.post(\"/sam/controlnet-seg\")\n        async def api_controlnet_seg(payload: ControlNetSegRequest = Body(...),\n                                    autosam_conf: AutoSAMConfig = Body(...)) -> Any:\n            payload.input_image = decode_to_pil(payload.input_image)\n            cnet_seg_img, cnet_seg_msg = cnet_seg(\n                payload.sam_model_name,\n                payload.input_image,\n                payload.processor,\n                payload.processor_res,\n                payload.pixel_perfect,\n                payload.resize_mode,\n                payload.target_W,\n                payload.target_H,\n                autosam_conf.points_per_side,\n                autosam_conf.points_per_batch,\n                autosam_conf.pred_iou_thresh,\n                autosam_conf.stability_score_thresh,\n                autosam_conf.stability_score_offset,\n                autosam_conf.box_nms_thresh,\n                autosam_conf.crop_n_layers,\n                autosam_conf.crop_nms_thresh,\n                autosam_conf.crop_overlap_ratio,\n                autosam_conf.crop_n_points_downscale_factor,\n                autosam_conf.min_mask_region_area)\n            cnet_seg_img = list(map(encode_to_base64, cnet_seg_img))\n            result = {\"msg\": cnet_seg_msg}\n            if len(cnet_seg_img) == 3:\n                result[\"blended_images\"] = cnet_seg_img[0]\n                result[\"random_seg\"] = cnet_seg_img[1]\n                result[\"edit_anything_control\"] = cnet_seg_img[2]\n            elif len(cnet_seg_img) == 4:\n                result[\"sem_presam\"] = cnet_seg_img[0]\n                result[\"sem_postsam\"] = cnet_seg_img[1]\n                result[\"blended_presam\"] = cnet_seg_img[2]\n                result[\"blended_postsam\"] = cnet_seg_img[3]\n            return result\n\n        @self.app.post(\"/sam/category-mask\")\n        async def api_category_mask(payload: CategoryMaskRequest = Body(...),\n                                  autosam_conf: AutoSAMConfig = Body(...)) -> Any:\n            payload.input_image = decode_to_pil(payload.input_image)\n            category_mask_img, category_mask_msg, resized_input_img = categorical_mask(\n                payload.sam_model_name,\n                payload.processor,\n                payload.processor_res,\n                payload.pixel_perfect,\n                payload.resize_mode,\n                payload.target_W,\n                payload.target_H,\n                payload.category,\n                payload.input_image,\n                autosam_conf.points_per_side,\n                autosam_conf.points_per_batch,\n                autosam_conf.pred_iou_thresh,\n                autosam_conf.stability_score_thresh,\n                autosam_conf.stability_score_offset,\n                autosam_conf.box_nms_thresh,\n                autosam_conf.crop_n_layers,\n                autosam_conf.crop_nms_thresh,\n                autosam_conf.crop_overlap_ratio,\n                autosam_conf.crop_n_points_downscale_factor,\n                autosam_conf.min_mask_region_area)\n            category_mask_img = list(map(encode_to_base64, category_mask_img))\n            result = {\"msg\": category_mask_msg}\n            if len(category_mask_img) == 3:\n                result[\"blended_image\"] = category_mask_img[0]\n                result[\"mask\"] = category_mask_img[1]\n                result[\"masked_image\"] = category_mask_img[2]\n            if resized_input_img is not None:\n                result[\"resized_input\"] = encode_to_base64(resized_input_img)\n            return result",
    "Code_level": "Class-level",
    "FSMID_for_test": 61,
    "wrong_result": "from fastapi import FastAPI\nfrom pydantic import BaseModel\nfrom typing import List, Dict, Any, Optional\nimport numpy as np\n\nclass SamPredictRequest(BaseModel):\n    image: str\n    points: Optional[List[List[float]]] = None\n    boxes: Optional[List[List[float]]] = None\n    multimask: bool = False\n    model_type: str = \"vit_h\"\n\nclass DINOPredictRequest(BaseModel):\n    image: str\n    text_prompt: str\n    box_threshold: float = 0.3\n    text_threshold: float = 0.25\n\nclass DilateMaskRequest(BaseModel):\n    mask: str\n    dilation_factor: int = 10\n\nclass AutoSAMConfig(BaseModel):\n    points_per_side: int = 32\n    pred_iou_thresh: float = 0.88\n    stability_score_thresh: float = 0.95\n    crop_n_layers: int = 0\n    crop_n_points_downscale_factor: int = 1\n    min_mask_region_area: int = 0\n\nclass ControlNetSegRequest(BaseModel):\n    image: str\n    processor: str = \"seg_ofade20k\"\n    resolution: int = 512\n\nclass CategoryMaskRequest(BaseModel):\n    image: str\n    category: str\n    autosam_config: AutoSAMConfig\n\nclass SamAPI:\n    def __init__(self, app: FastAPI):\n        self.app = app\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.app.get(\"/heartbeat\")\n        async def heartbeat() -> Dict:\n            return {\"status\": \"alive\"}\n\n        @self.app.get(\"/models/sam\")\n        async def api_sam_model() -> List[str]:\n            return [\"vit_h\", \"vit_l\", \"vit_b\"]\n\n        @self.app.post(\"/predict/sam\")\n        async def api_sam_predict(payload: SamPredictRequest) -> Any:\n            return {\"result\": \"sam_prediction_result\"}\n\n        @self.app.post(\"/predict/dino\")\n        async def api_dino_predict(payload: DINOPredictRequest) -> Any:\n            return {\"result\": \"dino_prediction_result\"}\n\n        @self.app.post(\"/process/dilate_mask\")\n        async def api_dilate_mask(payload: DilateMaskRequest) -> Any:\n            return {\"result\": \"dilated_mask_result\"}\n\n        @self.app.post(\"/segment/controlnet\")\n        async def api_controlnet_seg(payload: ControlNetSegRequest, autosam_conf: AutoSAMConfig) -> Any:\n            return {\"result\": \"controlnet_seg_result\"}\n\n        @self.app.post(\"/segment/category\")\n        async def api_category_mask(payload: CategoryMaskRequest, autosam_conf: AutoSAMConfig) -> Any:\n            return {\"result\": \"category_mask_result\"}"
  },
  {
    "Task_id": 124,
    "Instruction": "Task Description: Create a FastAPI-based service for extracting and citing facts from a given context in response to a user query, using OpenAI's language model for processing and providing streaming responses.\n\nClass Description: ExtractionService is a FastAPI application that handles question-answering with citation extraction. It processes user queries against provided context, extracts facts with proper citations, and streams the results back to the client.\n\nAttributes:\napp: FastAPI - The FastAPI application instance that handles HTTP requests and routes\nclient: OpenAI - The OpenAI client configured for structured output using the instructor library\n\nMethods:\n__init__: () -> None - Initializes the FastAPI application and sets up routes\n_setup_routes: () -> None - Configures the API endpoints and their handlers\nextract: (question: Question, openai_key: str) -> StreamingResponse - The endpoint handler that processes extraction requests and returns streaming responses\nstream_extract: (question: Question) -> Iterable[Fact] - Performs the actual extraction using OpenAI's API and yields facts\nget_api_key: (request: Request) -> str - Retrieves the OpenAI API key from either environment variables or request headers",
    "Canonical_solution": "from fastapi import FastAPI, Request, HTTPException, Depends\nfrom fastapi.params import Depends\nfrom pydantic import BaseModel, Field\nfrom starlette.responses import StreamingResponse\nfrom collections.abc import Iterable\nimport json\nimport os\nimport logging\nfrom instructor import OpenAISchema\nfrom instructor.dsl.multitask import MultiTaskBase\nfrom openai import OpenAI\nimport instructor\n\nclient = instructor.from_openai(OpenAI())\nlogger = logging.getLogger(__name__)\n\nclass Fact(BaseModel):\n    fact: str = Field(\n        ...,\n        description=\"Body of the sentences, as part of a response, it should read like a sentence that answers the question\",\n    )\n    substring_quotes: list[str] = Field(\n        ...,\n        description=\"Each source should be a direct quote from the context, as a substring of the original content\",\n    )\n\n    def _get_span(self, quote, context):\n        import regex\n        minor = quote\n        major = context\n        errs_ = 0\n        s = regex.search(f\"({minor}){{e<={errs_}}}\", major)\n        while s is None and errs_ <= len(context) * 0.05:\n            errs_ += 1\n            s = regex.search(f\"({minor}){{e<={errs_}}}\", major)\n        if s is not None:\n            yield from s.spans()\n\n    def get_spans(self, context):\n        if self.substring_quotes:\n            for quote in self.substring_quotes:\n                yield from self._get_span(quote, context)\n\nclass QuestionAnswer(OpenAISchema, MultiTaskBase):\n    question: str = Field(..., description=\"Question that was asked\")\n    tasks: list[Fact] = Field(\n        ...,\n        description=\"Body of the answer, each fact should be its separate object with a body and a list of sources\",\n    )\n\nQuestionAnswer.task_type = Fact\n\nclass Question(BaseModel):\n    context: str = Field(..., description=\"Context to extract answers from\")\n    query: str = Field(..., description=\"Question to answer\")\n\nclass ExtractionService:\n    def __init__(self):\n        self.app = FastAPI(title=\"Citation with Extraction\")\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.app.post(\"/extract\", response_class=StreamingResponse)\n        async def extract(question: Question, openai_key: str = Depends(self.get_api_key)):\n            facts = self.stream_extract(question)\n            async def generate():\n                for fact in facts:\n                    logger.info(f\"Fact: {fact}\")\n                    spans = list(fact.get_spans(question.context))\n                    resp = {\n                        \"body\": fact.fact,\n                        \"spans\": spans,\n                        \"citation\": [question.context[a:b] for (a, b) in spans],\n                    }\n                    resp_json = json.dumps(resp)\n                    yield f\"data: {resp_json}\"\n                yield \"data: [DONE]\"\n            return StreamingResponse(generate(), media_type=\"text/event-stream\")\n\n    def stream_extract(self, question: Question) -> Iterable[Fact]:\n        completion = client.chat.completions.create(\n            model=\"gpt-3.5-turbo-0613\",\n            temperature=0,\n            stream=True,\n            functions=[QuestionAnswer.openai_schema],\n            function_call={\"name\": QuestionAnswer.openai_schema[\"name\"]},\n            messages=[\n                {\n                    \"role\": \"system\",\n                    \"content\": \"You are a world class algorithm to answer questions with correct and exact citations.\",\n                },\n                {\"role\": \"user\", \"content\": \"Answer question using the following context\"},\n                {\"role\": \"user\", \"content\": f\"{question.context}\"},\n                {\"role\": \"user\", \"content\": f\"Question: {question.query}\"},\n                {\n                    \"role\": \"user\",\n                    \"content\": \"Tips: Make sure to cite your sources, and use the exact words from the context.\",\n                },\n            ],\n            max_tokens=2000,\n        )\n        return QuestionAnswer.from_streaming_response(completion)\n\n    def get_api_key(self, request: Request):\n        if \"OPENAI_API_KEY\" in os.environ:\n            return os.environ[\"OPENAI_API_KEY\"]\n        auth = request.headers.get(\"Authorization\")\n        if auth is None:\n            raise HTTPException(status_code=401, detail=\"Missing Authorization header\")\n        if auth.startswith(\"Bearer \"):\n            return auth.replace(\"Bearer \", \"\")\n        return None",
    "Code_level": "Class-level",
    "FSMID_for_test": 61,
    "wrong_result": "from fastapi import FastAPI, Request, HTTPException\nfrom fastapi.responses import StreamingResponse\nfrom pydantic import BaseModel\nimport instructor\nfrom openai import OpenAI\nfrom typing import Iterable\nimport os\n\nclass Question(BaseModel):\n    query: str\n    context: str\n\nclass Fact(BaseModel):\n    answer: str\n    citation: str\n\nclass ExtractionService:\n    def __init__(self):\n        self.app = FastAPI()\n        self.client = None\n        self._setup_routes()\n\n    def _setup_routes(self):\n        self.app.post(\"/extract\")(self.extract)\n\n    async def extract(self, question: Question, request: Request):\n        openai_key = self.get_api_key(request)\n        return StreamingResponse(\n            self.stream_extract(question, openai_key),\n            media_type=\"text/event-stream\"\n        )\n\n    async def stream_extract(self, question: Question, openai_key: str) -> Iterable[Fact]:\n        if not self.client or self.client.api_key != openai_key:\n            self.client = instructor.patch(OpenAI(api_key=openai_key))\n        \n        completion = await self.client.chat.completions.create(\n            model=\"gpt-4\",\n            messages=[\n                {\"role\": \"system\", \"content\": \"Extract facts from the provided context in response to the user query. For each fact, include a verbatim citation from the context.\"},\n                {\"role\": \"user\", \"content\": f\"Question: {question.query}\\nContext: {question.context}\"}\n            ],\n            stream=True,\n            response_model=Fact\n        )\n\n        async for chunk in completion:\n            if chunk.choices[0].delta.content:\n                yield chunk.choices[0].delta.content\n\n    def get_api_key(self, request: Request) -> str:\n        openai_key = request.headers.get(\"OpenAI-Key\") or os.getenv(\"OPENAI_API_KEY\")\n        if not openai_key:\n            raise HTTPException(status_code=401, detail=\"OpenAI API key not provided\")\n        return openai_key"
  },
  {
    "Task_id": 125,
    "Instruction": "Task Description: Create a FastAPI-based HTTP server implementation that handles dynamic route creation, exception translation, and streaming responses, with built-in tracing middleware for request monitoring.\n\nClass Description: LlamaStackServer is a configurable HTTP server implementation using FastAPI that provides dynamic route creation, exception handling, and support for streaming responses (SSE). It includes middleware for request tracing and manages the lifecycle of implementation components.\n\nAttributes:\n\nconfig: [dict] - Configuration dictionary for server setup\napp: [FastAPI] - Main FastAPI application instance\nimpls: [Optional[dict]] - Dictionary of implementation components managed by the server\n\nMethods:\n\n__init__: [Name](config: dict) -> [None] - Initializes the server with given configuration and sets up FastAPI app with tracing middleware\nlifespan: [Name](app: FastAPI) -> [AsyncContextManager] - Async context manager for server lifecycle (startup/shutdown)\nglobal_exception_handler: [Name](request: Request, exc: Exception) -> [JSONResponse] - Handles all uncaught exceptions and translates them to HTTP responses\ntranslate_exception: [Name](exc: Exception) -> [Union[HTTPException, RequestValidationError]] - Converts Python exceptions to appropriate HTTP exceptions\ncreate_dynamic_typed_route: [Name](func: Any, method: str) -> [Callable] - Creates a FastAPI route handler with proper typing and parameter handling\nis_streaming_request: [Name](func_name: str, request: Request, **kwargs) -> [bool] - Determines if a request should be handled as a streaming response\nmaybe_await: [Name](value: Any) -> [Coroutine] - Helper method to await coroutines or return values directly\nsse_generator: [Name](event_gen: AsyncGenerator) -> [AsyncGenerator] - Converts an async generator to Server-Sent Events (SSE) format\ncreate_sse_event: [Name](data: Any) -> [str] - Formats data as an SSE event string",
    "Canonical_solution": "from fastapi import Body, FastAPI, HTTPException, Request\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom pydantic import BaseModel\nfrom typing import Any, Union\nimport asyncio\nimport inspect\nimport json\nfrom typing_extensions import Annotated\n\nclass TracingMiddleware:\n    def __init__(self, app):\n        self.app = app\n\n    async def __call__(self, scope, receive, send):\n        path = scope[\"path\"]\n        await start_trace(path, {\"__location__\": \"server\"})\n        try:\n            return await self.app(scope, receive, send)\n        finally:\n            await end_trace()\n\nclass LlamaStackServer:\n    def __init__(self, config):\n        self.config = config\n        self.app = FastAPI(lifespan=self.lifespan)\n        self.app.add_middleware(TracingMiddleware)\n        self.impls = None\n\n    @asynccontextmanager\n    async def lifespan(self, app: FastAPI):\n        print(\"Starting up\")\n        yield\n        print(\"Shutting down\")\n        for impl in app.__llama_stack_impls__.values():\n            await impl.shutdown()\n\n    async def global_exception_handler(self, request: Request, exc: Exception):\n        traceback.print_exception(exc)\n        http_exc = self.translate_exception(exc)\n        return JSONResponse(\n            status_code=http_exc.status_code, \n            content={\"error\": {\"detail\": http_exc.detail}}\n        )\n\n    def translate_exception(self, exc: Exception) -> Union[HTTPException, RequestValidationError]:\n        if isinstance(exc, ValidationError):\n            exc = RequestValidationError(exc.raw_errors)\n\n        if isinstance(exc, RequestValidationError):\n            return HTTPException(\n                status_code=400,\n                detail={\n                    \"errors\": [\n                        {\n                            \"loc\": list(error[\"loc\"]),\n                            \"msg\": error[\"msg\"],\n                            \"type\": error[\"type\"],\n                        }\n                        for error in exc.errors()\n                    ]\n                },\n            )\n        elif isinstance(exc, ValueError):\n            return HTTPException(status_code=400, detail=f\"Invalid value: {str(exc)}\")\n        elif isinstance(exc, PermissionError):\n            return HTTPException(status_code=403, detail=f\"Permission denied: {str(exc)}\")\n        elif isinstance(exc, TimeoutError):\n            return HTTPException(status_code=504, detail=f\"Operation timed out: {str(exc)}\")\n        elif isinstance(exc, NotImplementedError):\n            return HTTPException(status_code=501, detail=f\"Not implemented: {str(exc)}\")\n        else:\n            return HTTPException(\n                status_code=500,\n                detail=\"Internal server error: An unexpected error occurred.\",\n            )\n\n    def create_dynamic_typed_route(self, func: Any, method: str):\n        async def endpoint(request: Request, **kwargs):\n            set_request_provider_data(request.headers)\n\n            is_streaming = self.is_streaming_request(func.__name__, request, **kwargs)\n            try:\n                if is_streaming:\n                    return StreamingResponse(\n                        self.sse_generator(func(**kwargs)), media_type=\"text/event-stream\"\n                    )\n                else:\n                    value = func(**kwargs)\n                    return await self.maybe_await(value)\n            except Exception as e:\n                traceback.print_exception(e)\n                raise self.translate_exception(e) from e\n\n        sig = inspect.signature(func)\n        new_params = [\n            inspect.Parameter(\n                \"request\", inspect.Parameter.POSITIONAL_OR_KEYWORD, annotation=Request\n            )\n        ]\n        new_params.extend(sig.parameters.values())\n\n        if method == \"post\":\n            new_params = [new_params[0]] + [\n                param.replace(annotation=Annotated[param.annotation, Body(..., embed=True)])\n                for param in new_params[1:]\n            ]\n\n        endpoint.__signature__ = sig.replace(parameters=new_params)\n        return endpoint\n\n    def is_streaming_request(self, func_name: str, request: Request, **kwargs):\n        return kwargs.get(\"stream\", False)\n\n    async def maybe_await(self, value):\n        if inspect.iscoroutine(value):\n            return await value\n        return value\n\n    async def sse_generator(self, event_gen):\n        try:\n            event_gen = await event_gen\n            async for item in event_gen:\n                yield self.create_sse_event(item)\n                await asyncio.sleep(0.01)\n        except asyncio.CancelledError:\n            print(\"Generator cancelled\")\n            await event_gen.aclose()\n        except Exception as e:\n            traceback.print_exception(e)\n            yield self.create_sse_event(\n                {\n                    \"error\": {\n                        \"message\": str(self.translate_exception(e)),\n                    },\n                }\n            )\n\n    def create_sse_event(self, data: Any) -> str:\n        if isinstance(data, BaseModel):\n            data = data.model_dump_json()\n        else:\n            data = json.dumps(data)\n        return f\"data: {data}\\n\\n\"",
    "Code_level": "Class-level",
    "FSMID_for_test": 61,
    "wrong_result": "from fastapi import FastAPI, Request, HTTPException\nfrom fastapi.exceptions import RequestValidationError\nfrom fastapi.responses import JSONResponse, StreamingResponse\nfrom fastapi.routing import APIRoute\nfrom contextlib import asynccontextmanager\nfrom typing import Optional, Dict, Any, Union, Callable, Coroutine, AsyncGenerator\nimport inspect\nimport json\nimport uuid\n\nclass LlamaStackServer:\n    def __init__(self, config: dict) -> None:\n        self.config = config\n        self.app = FastAPI(lifespan=self.lifespan)\n        self.impls = None\n\n        @self.app.middleware(\"http\")\n        async def tracing_middleware(request: Request, call_next):\n            request_id = str(uuid.uuid4())\n            request.state.request_id = request_id\n            response = await call_next(request)\n            response.headers[\"X-Request-ID\"] = request_id\n            return response\n\n        self.app.exception_handler(Exception)(self.global_exception_handler)\n\n    @asynccontextmanager\n    async def lifespan(self, app: FastAPI):\n        self.impls = {}\n        yield\n        self.impls = None\n\n    async def global_exception_handler(self, request: Request, exc: Exception) -> JSONResponse:\n        http_exc = self.translate_exception(exc)\n        return JSONResponse(\n            status_code=http_exc.status_code,\n            content={\"detail\": http_exc.detail},\n        )\n\n    def translate_exception(self, exc: Exception) -> Union[HTTPException, RequestValidationError]:\n        if isinstance(exc, HTTPException):\n            return exc\n        if isinstance(exc, RequestValidationError):\n            return exc\n        return HTTPException(status_code=500, detail=str(exc))\n\n    def create_dynamic_typed_route(self, func: Any, method: str) -> Callable:\n        async def route_handler(request: Request, **kwargs):\n            if self.is_streaming_request(func.__name__, request, **kwargs):\n                event_gen = await self.maybe_await(func(request, **kwargs))\n                return StreamingResponse(\n                    self.sse_generator(event_gen),\n                    media_type=\"text/event-stream\",\n                )\n            result = await self.maybe_await(func(request, **kwargs))\n            return result\n\n        route_handler.__name__ = func.__name__\n        route_handler.__annotations__ = func.__annotations__\n        return route_handler\n\n    def is_streaming_request(self, func_name: str, request: Request, **kwargs) -> bool:\n        return request.headers.get(\"accept\") == \"text/event-stream\"\n\n    async def maybe_await(self, value: Any) -> Coroutine:\n        if inspect.isawaitable(value):\n            return await value\n        return value\n\n    async def sse_generator(self, event_gen: AsyncGenerator) -> AsyncGenerator:\n        async for event in event_gen:\n            yield self.create_sse_event(event)\n\n    def create_sse_event(self, data: Any) -> str:\n        return f\"data: {json.dumps(data)}\\n\\n\""
  },
  {
    "Task_id": 129,
    "Instruction": "Task Description: Create a Flask-based web application class that serves as an interface to retrieve and display data from a Redis database, including application settings and time series metrics.\n\nClass Description: FlaskWebApp is a class that encapsulates a Flask web server with Redis connectivity. It provides routes for serving a frontend interface, application settings, and metric data from Redis. The class handles request processing, data retrieval from Redis, and response generation.\n\nAttributes:\n- app: [Flask] - The Flask application instance\n- redis_conn: [redis.Redis] - Connection to the Redis database\n- PROPAGATE_EXCEPTIONS: [bool] - Flask configuration to propagate exceptions\n\nMethods:\n- __init__(redis_conn) -> [None] - Initializes the Flask application and sets up Redis connection\n- _setup_routes() -> [None] - Defines all route handlers for the web application\n- run(host, port) -> [None] - Starts the Flask development server with specified host and port\n\nRoute Handlers (internal methods):\n- index() -> [tuple(str, int)] - Serves the main index.html template (200 OK)\n- app_settings() -> [tuple(str, int)] - Returns application settings as JSON (200 OK)\n- data() -> [tuple(str, int)] - Retrieves and returns metric data from Redis (200 OK if found, 404 if not found, 500 on error)",
    "Canonical_solution": "import redis\nimport logging\nimport simplejson as json\nfrom msgpack import Unpacker\nfrom flask import Flask, request, render_template\n\nclass FlaskWebApp:\n    def __init__(self, redis_conn):\n        self.app = Flask(__name__)\n        self.app.config['PROPAGATE_EXCEPTIONS'] = True\n        self.redis_conn = redis_conn\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.app.route(\"/\")\n        def index():\n            return render_template('index.html'), 200\n\n        @self.app.route(\"/app_settings\")\n        def app_settings():\n            app_settings = {\n                'GRAPH_URL': settings.GRAPH_URL,\n                'OCULUS_HOST': settings.OCULUS_HOST,\n                'FULL_NAMESPACE': settings.FULL_NAMESPACE,\n            }\n            resp = json.dumps(app_settings)\n            return resp, 200\n\n        @self.app.route(\"/api\", methods=['GET'])\n        def data():\n            metric = request.args.get('metric', None)\n            try:\n                raw_series = self.redis_conn.get(metric)\n                if not raw_series:\n                    resp = json.dumps({'results': 'Error: No metric by that name'})\n                    return resp, 404\n                else:\n                    unpacker = Unpacker(use_list=False)\n                    unpacker.feed(raw_series)\n                    timeseries = [item[:2] for item in unpacker]\n                    resp = json.dumps({'results': timeseries})\n                    return resp, 200\n            except Exception as e:\n                error = \"Error: \" + str(e)\n                resp = json.dumps({'results': error})\n                return resp, 500\n\n    def run(self, host, port):\n        self.app.run(host, port)",
    "Code_level": "Class-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, jsonify\nimport redis\nimport json\n\nclass FlaskWebApp:\n    def __init__(self, redis_conn):\n        self.app = Flask(__name__)\n        self.redis_conn = redis_conn\n        self.app.config['PROPAGATE_EXCEPTIONS'] = True\n        self._setup_routes()\n\n    def _setup_routes(self):\n        self.app.add_url_rule('/', 'index', self.index)\n        self.app.add_url_rule('/settings', 'app_settings', self.app_settings)\n        self.app.add_url_rule('/data', 'data', self.data)\n\n    def run(self, host='0.0.0.0', port=5000):\n        self.app.run(host=host, port=port)\n\n    def index(self):\n        return ('<html><body><h1>Flask Redis Interface</h1></body></html>', 200)\n\n    def app_settings(self):\n        try:\n            settings = self.redis_conn.get('app_settings')\n            if settings:\n                return (jsonify(json.loads(settings)), 200)\n            return ('Settings not found', 404)\n        except Exception as e:\n            return (f'Error retrieving settings: {str(e)}', 500)\n\n    def data(self):\n        try:\n            data = self.redis_conn.get('metrics')\n            if data:\n                return (jsonify(json.loads(data)), 200)\n            return ('Data not found', 404)\n        except Exception as e:\n            return (f'Error retrieving data: {str(e)}', 500)"
  },
  {
    "Task_id": 13,
    "Instruction": "Task Description: Implement an HTTP client function that sends authenticated requests to a Huawei Cloud API endpoint using HMAC-SHA256 signature for authentication.\n\nFunction Description: The function `request` sends HTTP requests to a specified API endpoint with proper authentication headers. It handles request signing, connection management, and response processing. The authentication follows Huawei Cloud's SDK-HMAC-SHA256 algorithm.\n\nInput:\n- `method`: [str] - HTTP method (e.g., 'GET', 'POST')\n- `path`: [str] - API endpoint path\n- `param`: [dict, optional] - Query parameters to be appended to the URL\n- `body`: [str/dict, optional] - Request body content (converted to JSON if dict)\n- `**params`: [dict, optional] - Additional query parameters\n\nOutput:\n- [dict] - Parsed JSON response from the API if successful\n- Raises Exception if HTTP status code indicates failure (status \u2265 300)\n\nNote: The function relies on global configuration in `Config` class (ID, TOKEN, PROXY) and API settings in `API` class (SCHEME, SITE). It performs the following operations:\n1. Constructs proper request headers with timestamp\n2. Generates HMAC-SHA256 signature\n3. Establishes HTTPS connection (with proxy support if configured)\n4. Sends request and processes response\n5. Handles errors and returns parsed data",
    "Canonical_solution": "from hashlib import sha256\nfrom hmac import new as hmac\nfrom binascii import hexlify\nfrom json import loads as jsondecode, dumps as jsonencode\nfrom logging import debug, info, warning\nfrom datetime import datetime\nfrom http.client import HTTPSConnection\nfrom urllib.parse import urlencode\n\nBasicDateFormat = \"%Y%m%dT%H%M%SZ\"\nAlgorithm = \"SDK-HMAC-SHA256\"\n\nclass Config:\n    ID = \"id\"  # AK\n    TOKEN = \"TOKEN\"  # AS\n    PROXY = None  # \u4ee3\u7406\u8bbe\u7f6e\n    TTL = None\n\nclass API:\n    SCHEME = 'https'\n    SITE = 'dns.myhuaweicloud.com'  # API endpoint\n\ndef request(method, path, param=None, body=None, **params):\n    if param:\n        params.update(param)\n\n    query = urlencode(sorted(params.items()))\n    headers = {\"content-type\": \"application/json\"}\n    headers[\"X-Sdk-Date\"] = datetime.strftime(datetime.utcnow(), BasicDateFormat)\n    headers[\"host\"] = API.SITE\n    sign_headers = []\n    for key in headers:\n        sign_headers.append(key.lower())\n    sign_headers.sort()\n\n    if body is None:\n        body = \"\"\n\n    hex_encode = HexEncodeSHA256Hash(body.encode('utf-8'))\n    canonical_headers = CanonicalHeaders(headers, sign_headers)\n\n    if path[-1] != '/':\n        sign_path = path + \"/\"\n    else:\n        sign_path = path\n\n    canonical_request = \"%s\\n%s\\n%s\\n%s\\n%s\\n%s\" % (method.upper(), sign_path, query,\n                                                    canonical_headers, \";\".join(sign_headers), hex_encode)\n\n    hashed_canonical_request = HexEncodeSHA256Hash(canonical_request.encode('utf-8'))\n    str_to_sign = \"%s\\n%s\\n%s\" % (Algorithm, headers['X-Sdk-Date'], hashed_canonical_request)\n\n    secret = Config.TOKEN\n    signature = hmac(secret.encode('utf-8'), str_to_sign.encode('utf-8'), digestmod=sha256).digest()\n    signature = hexlify(signature).decode()\n    auth_header = \"%s Access=%s, SignedHeaders=%s, Signature=%s\" % (\n        Algorithm, Config.ID, \";\".join(sign_headers), signature)\n    headers['Authorization'] = auth_header\n\n    if Config.PROXY:\n        conn = HTTPSConnection(Config.PROXY)\n        conn.set_tunnel(API.SITE, 443)\n    else:\n        conn = HTTPSConnection(API.SITE)\n    conn.request(method, API.SCHEME + \"://\" + API.SITE + path + '?' + query, body, headers)\n    info(API.SCHEME + \"://\" + API.SITE + path + '?' + query, body)\n    resp = conn.getresponse()\n    data = resp.read().decode('utf8')\n    resp.close()\n    if resp.status < 200 or resp.status >= 300:\n        warning('%s : error[%d]: %s', path, resp.status, data)\n        raise Exception(data)\n    else:\n        data = jsondecode(data)\n        debug('%s : result:%s', path, data)\n        return data\n\ndef HexEncodeSHA256Hash(data):\n    sha = sha256()\n    sha.update(data)\n    return sha.hexdigest()\n\ndef CanonicalHeaders(headers, signed_headers):\n    a = []\n    __headers = {}\n    for key in headers:\n        key_encoded = key.lower()\n        value = headers[key]\n        value_encoded = value.strip()\n        __headers[key_encoded] = value_encoded\n    for key in signed_headers:\n        a.append(key + \":\" + __headers[key])\n    return '\\n'.join(a) + \"\\n\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 56,
    "wrong_result": "import base64\nimport hashlib\nimport hmac\nimport json\nimport time\nimport urllib.parse\nfrom datetime import datetime\nfrom urllib.request import Request, build_opener, ProxyHandler, HTTPSHandler\n\nclass Config:\n    ID = ''\n    TOKEN = ''\n    PROXY = None\n\nclass API:\n    SCHEME = 'https'\n    SITE = ''\n\ndef request(method, path, param=None, body=None, **params):\n    if param is None:\n        param = {}\n    param.update(params)\n    query = urllib.parse.urlencode(param)\n    url = f\"{API.SCHEME}://{API.SITE}{path}?{query}\" if query else f\"{API.SCHEME}://{API.SITE}{path}\"\n\n    timestamp = datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')\n    headers = {\n        'Host': API.SITE,\n        'X-Sdk-Date': timestamp,\n        'Content-Type': 'application/json',\n    }\n\n    if body is not None:\n        if isinstance(body, dict):\n            body = json.dumps(body)\n        headers['Content-Length'] = str(len(body))\n\n    canonical_request = f\"{method}\\n{path}\\n{query}\\nhost:{API.SITE}\\nx-sdk-date:{timestamp}\\n\\nhost;x-sdk-date\\n\"\n    if 'Content-Length' in headers:\n        canonical_request += hashlib.sha256(body.encode()).hexdigest()\n    else:\n        canonical_request += hashlib.sha256(''.encode()).hexdigest()\n\n    string_to_sign = f\"SDK-HMAC-SHA256\\n{timestamp}\\n\" + hashlib.sha256(canonical_request.encode()).hexdigest()\n    sign_key = hmac.new(Config.TOKEN.encode(), timestamp.encode(), hashlib.sha256).digest()\n    signature = hmac.new(sign_key, string_to_sign.encode(), hashlib.sha256).hexdigest()\n    headers['Authorization'] = f\"SDK-HMAC-SHA256 Access={Config.ID}, SignedHeaders=host;x-sdk-date, Signature={signature}\"\n\n    req = Request(url, data=body.encode() if body else None, headers=headers, method=method)\n    opener = build_opener(HTTPSHandler())\n    if Config.PROXY:\n        opener.add_handler(ProxyHandler({'https': Config.PROXY}))\n\n    try:\n        with opener.open(req) as response:\n            result = response.read().decode()\n            if response.status >= 300:\n                raise Exception(f\"Request failed with status {response.status}: {result}\")\n            return json.loads(result) if result else {}\n    except Exception as e:\n        raise Exception(f\"Request failed: {str(e)}\")"
  },
  {
    "Task_id": 136,
    "Instruction": "Task Description: Create a simple Flask web application that serves responses to HTTP requests on different routes.\n\nFunction Description: The application should have two main functions:\n1. A root route handler that returns a welcome message with Python version information\n2. A static file route handler that returns a simple text response\n\nInput:\n- For the root route (\"/\"): No input parameters required\n- For the static route (\"/static/test.txt\"): No input parameters required\n\nOutput:\n- For the root route: A string containing a welcome message with Python version information\n- For the static route: A string containing a static text response",
    "Canonical_solution": "from flask import Flask\n\napp = Flask(__name__)\n\n@app.route(\"/\")\ndef hello():\n    version = \"{}.{}\".format(sys.version_info.major, sys.version_info.minor)\n    message = \"Hello World from Flask in a uWSGI Nginx Docker container with Python {} - testing\".format(\n        version\n    )\n    return message\n\n@app.route(\"/static/test.txt\")\ndef static_test():\n    return \"Static, from Flask\"\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", debug=True, port=80)",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask\nimport sys\n\napp = Flask(__name__)\n\n@app.route('/')\ndef root():\n    return f\"Welcome to the Flask app! Running on Python {sys.version_info.major}.{sys.version_info.minor}.{sys.version_info.micro}\"\n\n@app.route('/static/test.txt')\ndef static_file():\n    return \"This is a static text response.\""
  },
  {
    "Task_id": 14,
    "Instruction": "Task Description: Create a Python class to interact with Cloudflare's API for managing DNS records, including retrieving zone IDs, listing DNS records, and updating DNS records.\n\nClass Description: The CloudflareAPI class provides methods to interact with Cloudflare's API v4 for DNS management. It handles authentication, request/response processing, and includes caching for DNS records to optimize performance.\n\nAttributes:\n- ID: [str] - Cloudflare account email used for authentication\n- TOKEN: [str] - Cloudflare API key or bearer token for authentication\n- PROXY: [str] - Optional proxy server address\n- TTL: [int] - Time-to-live value for DNS records\n- SITE: [str] - Cloudflare API endpoint (default: \"api.cloudflare.com\")\n- _records_cache: [dict] - Internal cache for storing DNS records to reduce API calls\n\nMethods:\n- _request: [method](method: str, action: str, param: dict=None, **params: dict) -> dict - Internal method to make HTTP requests to Cloudflare API. Handles connection, authentication, request formatting, and response processing.\n- get_zone_id: [method](domain: str) -> str - Retrieves the zone ID for a given domain by checking progressively broader subdomains.\n- get_records: [method](zoneid: str, **conditions: dict) -> dict - Retrieves DNS records matching given conditions from a specified zone, using cache when possible.\n- update_record: [method](domain: str, value: str, record_type: str=\"A\") -> dict - Updates or creates a DNS record for the specified domain with the given value and record type.",
    "Canonical_solution": "from http.client import HTTPSConnection\nfrom urllib.parse import urlencode\nfrom json import loads as jsondecode, dumps as jsonencode\nfrom logging import debug, info, warning\n\nclass CloudflareAPI:\n    def __init__(self, auth_email=None, api_key=None, proxy=None, ttl=None):\n        self.ID = auth_email\n        self.TOKEN = api_key\n        self.PROXY = proxy\n        self.TTL = ttl\n        self.SITE = \"api.cloudflare.com\"\n        self._records_cache = {}\n\n    def _request(self, method, action, param=None, **params):\n        if param:\n            params.update(param)\n\n        params = dict((k, params[k]) for k in params if params[k] is not None)\n        info(\"%s/%s : %s\", self.SITE, action, params)\n        \n        if self.PROXY:\n            conn = HTTPSConnection(self.PROXY)\n            conn.set_tunnel(self.SITE, 443)\n        else:\n            conn = HTTPSConnection(self.SITE)\n\n        if method in ['PUT', 'POST', 'PATCH']:\n            params['content'] = str(params.get('content'))\n            params = jsonencode(params)\n        else:\n            if params:\n                action += '?' + urlencode(params)\n            params = None\n\n        if not self.ID:\n            headers = {\"Content-type\": \"application/json\",\n                      \"Authorization\": \"Bearer \" + self.TOKEN}\n        else:\n            headers = {\"Content-type\": \"application/json\",\n                      \"X-Auth-Email\": self.ID, \"X-Auth-Key\": self.TOKEN}\n\n        conn.request(method, '/client/v4/zones' + action, params, headers)\n        response = conn.getresponse()\n        res = response.read().decode('utf8')\n        conn.close()\n\n        if response.status < 200 or response.status >= 300:\n            warning('%s : error[%d]:%s', action, response.status, res)\n            raise Exception(res)\n        else:\n            data = jsondecode(res)\n            debug('%s : result:%s', action, data)\n            if not data:\n                raise Exception(\"Empty Response\")\n            elif data.get('success'):\n                return data.get('result', [{}])\n            else:\n                raise Exception(data.get('errors', [{}]))\n\n    def get_zone_id(self, domain):\n        zoneid = None\n        domain_slice = domain.split('.')\n        index = 2\n        while (not zoneid) and (index <= len(domain_slice)):\n            zones = self._request('GET', '', name='.'.join(domain_slice[-index:]))\n            zone = next((z for z in zones if domain.endswith(z.get('name'))), None)\n            zoneid = zone and zone['id']\n            index += 1\n        return zoneid\n\n    def get_records(self, zoneid, **conditions):\n        cache_key = zoneid + \"_\" + conditions.get('name', \"\") + \"_\" + conditions.get('type', \"\")\n        if cache_key not in self._records_cache:\n            self._records_cache[cache_key] = {}\n            data = self._request('GET', '/' + zoneid + '/dns_records', per_page=100, **conditions)\n            if data:\n                for record in data:\n                    self._records_cache[cache_key][record['id']] = {\n                        'id': record['id'],\n                        'type': record['type'],\n                        'name': record['name'],\n                        'content': record['content'],\n                        'proxied': record['proxied'],\n                        'ttl': record['ttl']\n                    }\n\n        records = {}\n        for (zid, record) in self._records_cache[cache_key].items():\n            for (k, value) in conditions.items():\n                if record.get(k) != value:\n                    break\n            else:\n                records[zid] = record\n        return records\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        zoneid = self.get_zone_id(domain)\n        if not zoneid:\n            raise Exception(\"invalid domain: [ %s ] \" % domain)\n\n        records = self.get_records(zoneid, name=domain, type=record_type)\n        cache_key = zoneid + \"_\" + domain + \"_\" + record_type\n        result = {}\n\n        if records:\n            for (rid, record) in records.items():\n                if record['content'] != value:\n                    res = self._request('PUT', '/' + zoneid + '/dns_records/' + record['id'],\n                                      type=record_type, content=value, name=domain, \n                                      proxied=record['proxied'], ttl=self.TTL)\n                    if res:\n                        self._records_cache[cache_key][rid]['content'] = value\n                        result[rid] = res.get(\"name\")\n                    else:\n                        result[rid] = \"Update fail!\\n\" + str(res)\n                else:\n                    result[rid] = domain\n        else:\n            res = self._request('POST', '/' + zoneid + '/dns_records',\n                              type=record_type, name=domain, content=value, \n                              proxied=False, ttl=self.TTL)\n            if res:\n                self._records_cache[cache_key][res['id']] = res\n                result = res\n            else:\n                result = domain + \" created fail!\"\n        return result",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nimport json\nfrom urllib.parse import urljoin\n\nclass CloudflareAPI:\n    def __init__(self, ID, TOKEN, PROXY=None, TTL=1, SITE=\"api.cloudflare.com\"):\n        self.ID = ID\n        self.TOKEN = TOKEN\n        self.PROXY = PROXY\n        self.TTL = TTL\n        self.SITE = SITE\n        self._records_cache = {}\n\n    def _request(self, method, action, param=None, **params):\n        headers = {\n            \"X-Auth-Email\": self.ID,\n            \"X-Auth-Key\": self.TOKEN,\n            \"Content-Type\": \"application/json\"\n        }\n        url = urljoin(f\"https://{self.SITE}/client/v4/\", action)\n        proxies = {\"https\": self.PROXY} if self.PROXY else None\n\n        try:\n            if method == \"GET\":\n                response = requests.get(url, headers=headers, params=params, proxies=proxies)\n            elif method == \"POST\":\n                response = requests.post(url, headers=headers, json=param, proxies=proxies)\n            elif method == \"PUT\":\n                response = requests.put(url, headers=headers, json=param, proxies=proxies)\n            elif method == \"DELETE\":\n                response = requests.delete(url, headers=headers, json=param, proxies=proxies)\n            else:\n                raise ValueError(\"Invalid HTTP method\")\n\n            response.raise_for_status()\n            return response.json()\n        except requests.exceptions.RequestException as e:\n            raise Exception(f\"Request failed: {e}\")\n\n    def get_zone_id(self, domain):\n        parts = domain.split('.')\n        for i in range(len(parts)):\n            subdomain = '.'.join(parts[i:])\n            response = self._request(\"GET\", f\"zones\", name=subdomain)\n            if response['result']:\n                return response['result'][0]['id']\n        raise Exception(f\"No zone found for domain: {domain}\")\n\n    def get_records(self, zoneid, **conditions):\n        cache_key = f\"{zoneid}_{json.dumps(conditions, sort_keys=True)}\"\n        if cache_key in self._records_cache:\n            return self._records_cache[cache_key]\n\n        params = {\"per_page\": 100}\n        params.update(conditions)\n        response = self._request(\"GET\", f\"zones/{zoneid}/dns_records\", **params)\n        self._records_cache[cache_key] = response['result']\n        return response['result']\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        zoneid = self.get_zone_id(domain)\n        records = self.get_records(zoneid, name=domain, type=record_type)\n        data = {\n            \"type\": record_type,\n            \"name\": domain,\n            \"content\": value,\n            \"ttl\": self.TTL,\n            \"proxied\": False\n        }\n\n        if records:\n            record_id = records[0]['id']\n            response = self._request(\"PUT\", f\"zones/{zoneid}/dns_records/{record_id}\", param=data)\n        else:\n            response = self._request(\"POST\", f\"zones/{zoneid}/dns_records\", param=data)\n\n        self._records_cache.clear()\n        return response"
  },
  {
    "Task_id": 143,
    "Instruction": "Task Description: Create a Flask-based HTTP server that manages and controls multiple learner and collector processes, allowing dynamic scaling of these processes through API calls.\n\nClass Description: FlaskOperatorServer is a class that wraps a Flask application to provide RESTful API endpoints for managing learner and collector process replicas. It interacts with a Creator instance to handle the actual process management.\n\nAttributes:\n- app: Flask - The Flask application instance\n- creator: Creator - The process manager that handles learner and collector instances\n- host: str - The host address where the server will run\n- port: int - The port number where the server will listen\n- api_version: str - The version prefix for API endpoints\n\nMethods:\n- __init__(creator, host: str, port: int, api_version: str = 'v1alpha1') -> None - Initializes the Flask server with process manager, host, port, and API version. Sets up route handlers.\n- run() -> None - Starts the Flask application server on the specified host and port.\n- post_replicas() -> dict - Handles POST requests to set the desired number of replicas (route: /{api_version}/replicas). Returns a response dictionary.\n- get_replicas() -> dict - Handles GET requests to retrieve current replica information (route: /{api_version}/replicas). Returns a response dictionary with current resources.",
    "Canonical_solution": "from flask import Flask, request\nimport json\nfrom threading import Thread\nfrom queue import Queue\n\nclass Creator:\n    def __init__(self, learner_addr, collector_addr):\n        self.learner_addr = learner_addr\n        self.collector_addr = collector_addr\n        self.collector_demand = Queue()\n        self.learner_demand = Queue()\n        self.learners = {}\n        self.collectors = {}\n        self.end_flag = False\n\n    def set_target_source(self, learner_target, collector_target):\n        self.collector_demand.put(collector_target)\n        self.learner_demand.put(learner_target)\n\n    def start(self):\n        while not self.end_flag:\n            if self.learner_demand.empty() and self.collector_demand.empty():\n                continue\n            else:\n                learner_demand, collector_demand = None, None\n                if not self.learner_demand.empty():\n                    learner_demand = self.learner_demand.get()\n                if not self.collector_demand.empty():\n                    collector_demand = self.collector_demand.get()\n\n                for i in range(collector_demand):\n                    name, host, port = self.collector_addr[i]\n                    self.collectors[name] = NaiveCollector(host, port, prefix=DATA_PREFIX)\n                    self.collectors[name].start()\n                for i in range(learner_demand):\n                    name, host, port = self.learner_addr[i]\n                    self.learners[name] = NaiveLearner(host, port, prefix=DATA_PREFIX)\n                    self.learners[name].start()\n\n    def close(self):\n        self.end_flag = True\n        for t in self.learners.values():\n            t.close()\n        for t in self.collectors.values():\n            t.close()\n\n    @property\n    def current_resource(self):\n        collectors = {k: {} for k in self.collectors}\n        learners = {k: {} for k in self.learners}\n        return {\"collectors\": collectors, 'learners': learners}\n\nclass FlaskOperatorServer:\n    def __init__(self, creator, host: str, port: int, api_version: str = 'v1alpha1'):\n        self.app = Flask(__name__)\n        self.creator = creator\n        self.host = host\n        self.port = port\n        self.api_version = api_version\n        \n        @self.app.route('/{}/replicas'.format(self.api_version), methods=['POST'])\n        def post_replicas():\n            data = json.loads(request.data.decode())\n            collectors = data['collectors'][\"replicas\"]\n            learners = data['learners'][\"replicas\"]\n            self.creator.set_target_source(learners, collectors)\n            return {'success': True, 'code': 0, 'message': '', 'data': ''}\n\n        @self.app.route('/{}/replicas'.format(self.api_version), methods=['GET'])\n        def get_replicas():\n            data = json.loads(request.data.decode())\n            return {'success': True, 'code': 0, 'message': '', 'data': self.creator.current_resource}\n\n    def run(self):\n        self.app.run(host=self.host, port=self.port)",
    "Code_level": "Class-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, request, jsonify\n\nclass FlaskOperatorServer:\n    def __init__(self, creator, host: str, port: int, api_version: str = 'v1alpha1'):\n        self.app = Flask(__name__)\n        self.creator = creator\n        self.host = host\n        self.port = port\n        self.api_version = api_version\n\n        self.app.add_url_rule(f'/{self.api_version}/replicas', 'post_replicas', self.post_replicas, methods=['POST'])\n        self.app.add_url_rule(f'/{self.api_version}/replicas', 'get_replicas', self.get_replicas, methods=['GET'])\n\n    def run(self):\n        self.app.run(host=self.host, port=self.port)\n\n    def post_replicas(self):\n        data = request.get_json()\n        if not data or 'replicas' not in data:\n            return jsonify({'error': 'Invalid request body'}), 400\n\n        replicas = data['replicas']\n        if not isinstance(replicas, dict):\n            return jsonify({'error': 'Replicas must be a dictionary'}), 400\n\n        required_keys = {'learners', 'collectors'}\n        if not required_keys.issubset(replicas.keys()):\n            return jsonify({'error': 'Missing required replica keys'}), 400\n\n        try:\n            self.creator.set_replicas(replicas['learners'], replicas['collectors'])\n            return jsonify({'status': 'success'})\n        except Exception as e:\n            return jsonify({'error': str(e)}), 500\n\n    def get_replicas(self):\n        try:\n            learners = self.creator.get_learners()\n            collectors = self.creator.get_collectors()\n            return jsonify({\n                'learners': len(learners),\n                'collectors': len(collectors),\n                'resources': {\n                    'learners': [p.pid for p in learners],\n                    'collectors': [p.pid for p in collectors]\n                }\n            })\n        except Exception as e:\n            return jsonify({'error': str(e)}), 500"
  },
  {
    "Task_id": 15,
    "Instruction": "Task Description: Implement an HTTP connection pool class to manage and reuse HTTP connections efficiently, handling connection creation, request execution, and response processing with support for retries, timeouts, and proxy configurations.\n\nClass Description: HTTPConnectionPool is a connection pooling class that manages HTTP connections to a specific host, providing methods to create connections, make requests, and handle responses with configurable timeouts and retries.\n\nAttributes:\n\nscheme: str - The URL scheme (default \"http\")\nConnectionCls: class - The connection class to use (default HTTPConnection)\nhost: str - The target host for connections\nport: int - The target port for connections (default None)\ntimeout: Timeout - The timeout configuration for connections\nretries: Retry - The retry configuration for failed requests\nproxy: str - The proxy URL if configured (default None)\nproxy_headers: dict - Headers to use for proxy connections (default {})\nconn_kw: dict - Additional connection keyword arguments\n\nMethods:\n\n_new_conn() -> HTTPConnection - Creates and returns a new HTTP connection instance, incrementing the connection counter.\n\n_make_request(conn: HTTPConnection, method: str, url: str, timeout: Timeout = _Default, chunked: bool = False, **httplib_request_kw) -> HTTPResponse - Makes an HTTP request using the specified connection, handling timeouts and errors. Returns the HTTP response.\n\nurlopen(method: str, url: str, body: Optional[bytes] = None, headers: Optional[dict] = None, retries: Optional[Retry] = None, redirect: bool = True, assert_same_host: bool = True, timeout: Timeout = _Default, pool_timeout: Optional[float] = None, release_conn: Optional[bool] = None, chunked: bool = False, **response_kw) -> HTTPResponse - Main method to open a URL using the connection pool, handling connection management, request execution, and response processing. Returns an HTTPResponse object.",
    "Canonical_solution": "from socket import error as SocketError\nfrom socket import timeout as SocketTimeout\nfrom .connection import HTTPConnection, HTTPSConnection\nfrom .exceptions import (\n    EmptyPoolError,\n    MaxRetryError,\n    ProtocolError,\n    TimeoutError,\n    SSLError,\n    ProxyError,\n    ClosedPoolError,\n    NewConnectionError,\n)\nfrom .util.connection import is_connection_dropped\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.retry import Retry\nfrom .util.timeout import Timeout\nfrom .util.url import get_host, parse_url\nfrom .response import HTTPResponse\nimport logging\nimport socket\nimport sys\nimport warnings\n\nlog = logging.getLogger(__name__)\n\nclass HTTPConnectionPool:\n    scheme = \"http\"\n    ConnectionCls = HTTPConnection\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        timeout=Timeout.DEFAULT_TIMEOUT,\n        maxsize=1,\n        block=False,\n        headers=None,\n        retries=None,\n        _proxy=None,\n        _proxy_headers=None,\n        **conn_kw\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = Timeout.from_float(timeout) if not isinstance(timeout, Timeout) else timeout\n        self.retries = retries if retries is not None else Retry.DEFAULT\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.conn_kw = conn_kw\n\n    def _new_conn(self):\n        self.num_connections += 1\n        log.debug(\n            \"Starting new HTTP connection (%d): %s:%s\",\n            self.num_connections,\n            self.host,\n            self.port or \"80\",\n        )\n        return self.ConnectionCls(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout.connect_timeout,\n            **self.conn_kw\n        )\n\n    def _make_request(\n        self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw\n    ):\n        self.num_requests += 1\n        timeout_obj = self._get_timeout(timeout)\n        timeout_obj.start_connect()\n        conn.timeout = timeout_obj.connect_timeout\n\n        try:\n            if chunked:\n                conn.request_chunked(method, url, **httplib_request_kw)\n            else:\n                conn.request(method, url, **httplib_request_kw)\n        except (SocketTimeout, BaseSSLError) as e:\n            self._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n            raise\n\n        read_timeout = timeout_obj.read_timeout\n        if getattr(conn, \"sock\", None):\n            if read_timeout == 0:\n                raise ReadTimeoutError(\n                    self, url, \"Read timed out. (read timeout=%s)\" % read_timeout\n                )\n            conn.sock.settimeout(read_timeout)\n\n        try:\n            httplib_response = conn.getresponse()\n        except (SocketTimeout, BaseSSLError, SocketError) as e:\n            self._raise_timeout(err=e, url=url, timeout_value=read_timeout)\n            raise\n\n        return httplib_response\n\n    def urlopen(\n        self,\n        method,\n        url,\n        body=None,\n        headers=None,\n        retries=None,\n        redirect=True,\n        assert_same_host=True,\n        timeout=_Default,\n        pool_timeout=None,\n        release_conn=None,\n        chunked=False,\n        **response_kw\n    ):\n        parsed_url = parse_url(url)\n        destination_scheme = parsed_url.scheme\n        headers = headers or self.headers\n        retries = retries or self.retries\n\n        conn = None\n        release_this_conn = release_conn\n        http_tunnel_required = connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, destination_scheme\n        )\n\n        try:\n            timeout_obj = self._get_timeout(timeout)\n            conn = self._get_conn(timeout=pool_timeout)\n            conn.timeout = timeout_obj.connect_timeout\n\n            if http_tunnel_required:\n                self._prepare_proxy(conn)\n\n            httplib_response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout_obj,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n            )\n\n            response = HTTPResponse.from_httplib(\n                httplib_response,\n                pool=self,\n                connection=conn if not release_conn else None,\n                retries=retries,\n                **response_kw\n            )\n\n        except (EmptyPoolError, TimeoutError, SSLError, ProxyError, ProtocolError) as e:\n            if conn:\n                conn.close()\n                conn = None\n            raise\n\n        finally:\n            if release_this_conn and conn:\n                self._put_conn(conn)\n\n        return response",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import socket\nimport time\nfrom typing import Optional, Dict, Any, Union\nfrom urllib.parse import urlparse\n\nclass HTTPConnectionPool:\n    def __init__(\n        self,\n        host: str,\n        port: Optional[int] = None,\n        scheme: str = \"http\",\n        ConnectionCls=None,\n        timeout: Optional[float] = None,\n        retries: Optional[int] = None,\n        proxy: Optional[str] = None,\n        proxy_headers: Optional[Dict[str, str]] = None,\n        **conn_kw: Any\n    ):\n        self.scheme = scheme\n        self.ConnectionCls = ConnectionCls or HTTPConnection\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.retries = retries\n        self.proxy = proxy\n        self.proxy_headers = proxy_headers or {}\n        self.conn_kw = conn_kw\n        self.pool = []\n        self.num_connections = 0\n\n    def _new_conn(self) -> HTTPConnection:\n        self.num_connections += 1\n        if self.proxy:\n            proxy_parsed = urlparse(self.proxy)\n            proxy_host = proxy_parsed.hostname\n            proxy_port = proxy_parsed.port\n            conn = self.ConnectionCls(\n                proxy_host,\n                proxy_port,\n                timeout=self.timeout,\n                **self.conn_kw\n            )\n            conn.set_tunnel(self.host, self.port, headers=self.proxy_headers)\n        else:\n            conn = self.ConnectionCls(\n                self.host,\n                self.port,\n                timeout=self.timeout,\n                **self.conn_kw\n            )\n        return conn\n\n    def _make_request(\n        self,\n        conn: HTTPConnection,\n        method: str,\n        url: str,\n        timeout: Optional[float] = None,\n        chunked: bool = False,\n        **httplib_request_kw: Any\n    ) -> HTTPResponse:\n        try:\n            conn.request(method, url, **httplib_request_kw)\n            response = conn.getresponse()\n            return response\n        except (socket.timeout, socket.error) as e:\n            conn.close()\n            raise\n\n    def urlopen(\n        self,\n        method: str,\n        url: str,\n        body: Optional[bytes] = None,\n        headers: Optional[Dict[str, str]] = None,\n        retries: Optional[int] = None,\n        redirect: bool = True,\n        assert_same_host: bool = True,\n        timeout: Optional[float] = None,\n        pool_timeout: Optional[float] = None,\n        release_conn: Optional[bool] = None,\n        chunked: bool = False,\n        **response_kw: Any\n    ) -> HTTPResponse:\n        if headers is None:\n            headers = {}\n        if retries is None:\n            retries = self.retries or 0\n        if timeout is None:\n            timeout = self.timeout\n        if release_conn is None:\n            release_conn = True\n\n        conn = None\n        start_time = time.time()\n        try:\n            if self.pool:\n                conn = self.pool.pop()\n            else:\n                conn = self._new_conn()\n\n            response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout,\n                chunked=chunked,\n                body=body,\n                headers=headers,\n                **response_kw\n            )\n            return response\n        except Exception as e:\n            if conn:\n                conn.close()\n            if retries > 0:\n                return self.urlopen(\n                    method,\n                    url,\n                    body,\n                    headers,\n                    retries=retries - 1,\n                    redirect=redirect,\n                    assert_same_host=assert_same_host,\n                    timeout=timeout,\n                    pool_timeout=pool_timeout,\n                    release_conn=release_conn,\n                    chunked=chunked,\n                    **response_kw\n                )\n            raise\n        finally:\n            if conn and release_conn:\n                self.pool.append(conn)"
  },
  {
    "Task_id": 160,
    "Instruction": "Task Description: Create a custom Flask application class that extends the base Flask class to provide enhanced request handling and response generation capabilities, particularly for API responses.\n\nClass Description: FlaskApp is a subclass of Flask that customizes request handling and response generation. It provides automatic JSON serialization of responses and convenience methods for defining routes.\n\nAttributes:\n- request_class: [Request] - Custom request class to use for incoming requests\n- json_data_class: [tuple] - Tuple of classes that should be automatically converted to JSON responses\n\nMethods:\n- make_response(rv) -> Response - Overrides the base make_response method to automatically convert supported data types to JSON responses\n  Input: \n    rv: [Any] - The response value to process\n  Output:\n    Response object with appropriate content type and serialized data\n\n- get(rule, **options) -> decorator - Convenience method for creating GET routes\n  Input:\n    rule: [str] - URL rule as string\n    **options: [dict] - Additional options to pass to route()\n  Output:\n    Route decorator for GET method\n\n- post(rule, **options) -> decorator - Convenience method for creating POST routes\n  Input:\n    rule: [str] - URL rule as string\n    **options: [dict] - Additional options to pass to route()\n  Output:\n    Route decorator for POST method",
    "Canonical_solution": "from flask import Flask, Response\nfrom peewee import ModelSelect, Model\nfrom domain_admin.compat import Iterator\nfrom domain_admin.utils.flask_ext.api_result import ApiResult\nfrom domain_admin.utils.flask_ext.request import Request\nimport six\n\nclass FlaskApp(Flask):\n    request_class = Request\n\n    json_data_class = (\n        ModelSelect,\n        Model,\n        Iterator,\n        list,\n        dict,\n        six.integer_types,\n        six.text_type\n    )\n\n    def make_response(self, rv):\n        if isinstance(rv, self.json_data_class) or rv is None:\n            rv = ApiResult.success(rv)\n\n        if isinstance(rv, ApiResult):\n            return Response(rv.to_json(), content_type='application/json;charset=utf-8')\n\n        return super(FlaskApp, self).make_response(rv)\n\n    def get(self, rule, **options):\n        options.setdefault('methods', ['GET'])\n        return super(FlaskApp, self).route(rule, **options)\n\n    def post(self, rule, **options):\n        options.setdefault('methods', ['POST'])\n        return super(FlaskApp, self).route(rule, **options)",
    "Code_level": "Class-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, jsonify, Response, request as flask_request\nfrom werkzeug.local import LocalProxy\n\nclass FlaskApp(Flask):\n    request_class = LocalProxy(lambda: flask_request)\n    json_data_classes = (dict, list, tuple)\n\n    def make_response(self, rv):\n        if isinstance(rv, self.json_data_classes):\n            rv = jsonify(rv)\n        return super().make_response(rv)\n\n    def get(self, rule, **options):\n        def decorator(f):\n            self.add_url_rule(rule, view_func=f, methods=['GET'], **options)\n            return f\n        return decorator\n\n    def post(self, rule, **options):\n        def decorator(f):\n            self.add_url_rule(rule, view_func=f, methods=['POST'], **options)\n            return f\n        return decorator"
  },
  {
    "Task_id": 166,
    "Instruction": "Task Description: Create a Flask-based HTTP server that implements a GraphQL endpoint with both GET and POST methods, including a GraphiQL playground interface for testing queries.\n\nFunction Description: The code implements two Flask route handlers for a GraphQL server. The first handles GET requests to serve a GraphiQL playground interface, while the second handles POST requests to process GraphQL queries and mutations.\n\nInput:\n- For GET /graphql: No input parameters (uses Flask's request object internally)\n- For POST /graphql: Expects JSON payload containing GraphQL query/mutation\n\nOutput:\n- For GET /graphql: Returns HTML content for GraphiQL playground (status 200)\n- For POST /graphql: Returns JSON response containing either:\n  - Successful query result (status 200)\n  - Error response (status 400) if query fails",
    "Canonical_solution": "from flask import Flask, jsonify, request\nfrom ariadne import graphql_sync\nfrom ariadne.explorer import ExplorerGraphiQL\n\napp = Flask(__name__)\napp.config.update(\n    {\n        \"TESTING\": True,\n    }\n)\n\nexplorer = ExplorerGraphiQL(title=\"My Flask GraphQL\")\n\n@app.route(\"/graphql\", methods=[\"GET\"])\ndef graphql_playground():\n    html = explorer.html(request)\n    return html, 200\n\n@app.route(\"/graphql\", methods=[\"POST\"])\ndef graphql_server():\n    data = request.get_json()\n    success, result = graphql_sync(\n        schema,\n        data,\n        context_value={\"request\": request},\n        root_value={\"hello\": \"Hello Flask!\"},\n        debug=app.debug,\n    )\n    status_code = 200 if success else 400\n    return jsonify(result), status_code",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, request, jsonify\nfrom graphql import graphql_sync\nfrom graphql.type import GraphQLSchema\nfrom flask_graphql import GraphQLView\n\napp = Flask(__name__)\n\nschema = GraphQLSchema(query=None, mutation=None)\n\napp.add_url_rule(\n    '/graphql',\n    view_func=GraphQLView.as_view(\n        'graphql',\n        schema=schema,\n        graphiql=True\n    )\n)\n\nif __name__ == '__main__':\n    app.run()"
  },
  {
    "Task_id": 167,
    "Instruction": "The provided reference code is at the function level, specifically focusing on a Flask web application's route handling. Here's the structured instruction:\n\nTask Description: Create a Flask web application route handler that processes both GET and POST requests, implements various command-based functionalities, and returns appropriate HTML templates based on the input commands.\n\nFunction Description: The `index()` function serves as the main route handler for the root URL (\"/\"). It processes incoming HTTP requests, executes different actions based on the received commands (for POST requests), and returns corresponding HTML templates. The function handles feature activation, information display, and system operations through a command interface.\n\nInput:\n- HTTP request object (implicit through Flask)\n- For POST requests:\n  - Form data containing a command string (key: \"in\")\n- For GET requests:\n  - No explicit input parameters\n- Global variables (used in the function but not passed as parameters):\n  - redirectionMicrosoft (bool)\n  - redirectionMicrosoftFailed (bool)\n  - phone_number (str)\n  - email (str)\n  - password (str)\n  - Various feature flags (social_media, get_links, etc.)\n\nOutput:\n- HTTP responses containing:\n  - Rendered HTML templates (for successful operations)\n  - Redirects to other routes (for certain conditions)\n  - Error pages (for failed operations)\n\nThe function returns different responses based on:\n1. Request method (GET/POST)\n2. Specific command received (for POST requests)\n3. System state (global variables)\n4. Success/failure of operations",
    "Canonical_solution": "import os\nfrom flask import Flask, render_template, request, redirect, url_for\nimport threading\nimport subprocess\n\napp = Flask(__name__)\n\ndef run_flask_server():\n    app.run(str(subprocess.check_output(\"hostname -I | awk '{print $1}'\", shell=True).decode().strip()), 8080, debug=True)\n\n@app.route(\"/\", methods=[\"POST\", \"GET\"])\ndef index():\n    if request.method == \"GET\":\n        if redirectionMicrosoft == True:\n            return render_template(\"findOwnerSuccess.html\", phone_number=phone_number)\n        if redirectionMicrosoftFailed == True:\n            return render_template(\"failed.html\")\n        return render_template(\"index.html\")\n    if request.method == \"POST\":\n        command = request.form[\"in\"]\n        if command == \"help\":\n            return render_template(\"help.html\")\n        elif command[0:15] == \"add PhoneNumber\":\n            phone_number = str(\"+\" + re.search(\"\\d+\", command).group(0))\n            return render_template(\"phoneNumberSuccess.html\", phone_number=phone_number)\n        elif command[0:21] == \"add feature FindOwner\":\n            try:\n                if email != \"\" or password != \"\":\n                    find_owner = \"Added\"\n                    return render_template(\"findOwnerSuccess.html\", phone_number=phone_number)\n                else:\n                    return redirect(url_for(\"microsoftMail\"))\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:23] == \"add feature SocialMedia\":\n            social_media = \"Added\"\n            try:\n                return render_template(\"socialMediaSuccess.html\", phone_number=phone_number)\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:20] == \"add feature GetLinks\":\n            get_links = \"Added\"\n            try:\n                return render_template(\"GetLinksSuccess.html\", phone_number=phone_number)\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:20] == \"add feature SpamRisk\":\n            spam_risk = \"Added\"\n            try:\n                return render_template(\"SpamRiskSuccess.html\", phone_number=phone_number)\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:23] == \"add feature GetComments\":\n            get_comments = \"Added\"\n            try:\n                return render_template(\"GetCommentsSuccess.html\", phone_number=phone_number)\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:12] == \"show options\":\n            try:\n                return render_template(\"showoptions.html\", phone_number=phone_number, social_media=social_media, get_links=get_links, spam_risk=spam_risk, find_owner=find_owner, get_comments=get_comments)\n            except:\n                phone_number = \"Not Provided\"\n                return render_template(\"showoptions.html\", phone_number=phone_number, social_media=social_media, get_links=get_links, spam_risk=spam_risk, find_owner=find_owner, get_comments=get_comments)\n        elif command[0:20] == \"add feature *\":\n            runall = \"Added\"\n            social_media = \"Added\"\n            spam_risk = \"Added\"\n            get_comments = \"Added\"\n            get_links = \"Added\"\n            try:\n                if email != \"\" or password != \"\":\n                    find_owner = \"Added\"\n                    return render_template(\"featureAll.html\", phone_number=phone_number)\n                else:\n                    return redirect(url_for(\"microsoftMail\"))\n            except:\n                return render_template(\"failed.html\")\n        elif command[0:3] == \"run\":\n            Investigation.general.location(phone_number)\n            country = Investigation.general.returnCountry()\n            operator = Investigation.general.returnOperator()\n            timeZone = Investigation.general.returnTimeZone()\n            errNumber = Investigation.general.return_errNumber_()\n            currentTime = Investigation.general.returnCurrentTime()\n            if errNumber != \"False\":\n                threading.Thread(target=runScripts).start()\n                time.sleep(2)\n                return redirect(url_for(\"investigation\"))\n            else:\n                return redirect(url_for(\"investigationErr\"))\n        else:\n            return render_template(\"unkownCommand.html\")\n\nif __name__ == \"__main__\":\n    run_flask_server()",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, render_template, request, redirect, url_for\n\napp = Flask(__name__)\n\nredirectionMicrosoft = False\nredirectionMicrosoftFailed = False\nphone_number = \"\"\nemail = \"\"\npassword = \"\"\nsocial_media = False\nget_links = False\nget_contacts = False\nget_emails = False\nget_passwords = False\nget_credit_cards = False\nget_history = False\nget_cookies = False\nget_bank_accounts = False\nget_system_info = False\nget_wifi_passwords = False\nget_screenshot = False\nget_webcam = False\nget_microphone = False\nget_keylogger = False\nget_clipboard = False\nget_files = False\nget_documents = False\nget_downloads = False\nget_desktop = False\nget_music = False\nget_pictures = False\nget_videos = False\nget_startup = False\nget_processes = False\nget_installed_programs = False\nget_services = False\nget_firewall = False\nget_antivirus = False\nget_windows_version = False\nget_windows_key = False\nget_windows_update = False\nget_windows_defender = False\nget_windows_firewall = False\nget_windows_credentials = False\nget_windows_tasks = False\nget_windows_events = False\nget_windows_logs = False\nget_windows_network = False\nget_windows_drivers = False\nget_windows_hotfixes = False\nget_windows_environment = False\nget_windows_users = False\nget_windows_groups = False\nget_windows_services = False\nget_windows_processes = False\nget_windows_ports = False\nget_windows_shares = False\nget_windows_sessions = False\nget_windows_patches = False\nget_windows_software = False\nget_windows_hardware = False\nget_windows_bios = False\nget_windows_cpu = False\nget_windows_memory = False\nget_windows_disk = False\nget_windows_network = False\nget_windows_printers = False\nget_windows_sound = False\nget_windows_video = False\nget_windows_input = False\nget_windows_monitors = False\nget_windows_power = False\nget_windows_storage = False\nget_windows_usb = False\nget_windows_bluetooth = False\nget_windows_pci = False\nget_windows_smbios = False\nget_windows_acpi = False\nget_windows_wmi = False\nget_windows_registry = False\nget_windows_security = False\nget_windows_audit = False\nget_windows_application = False\nget_windows_system = False\nget_windows_setup = False\nget_windows_error = False\nget_windows_dns = False\nget_windows_dhcp = False\nget_windows_ie = False\nget_windows_office = False\nget_windows_dotnet = False\nget_windows_java = False\nget_windows_python = False\nget_windows_node = False\nget_windows_ruby = False\nget_windows_perl = False\nget_windows_php = False\nget_windows_go = False\nget_windows_rust = False\nget_windows_c = False\nget_windows_cpp = False\nget_windows_csharp = False\nget_windows_vb = False\nget_windows_fsharp = False\nget_windows_powershell = False\nget_windows_bash = False\nget_windows_cmd = False\nget_windows_batch = False\nget_windows_vbs = False\nget_windows_js = False\nget_windows_html = False\nget_windows_css = False\nget_windows_xml = False\nget_windows_json = False\nget_windows_sql = False\nget_windows_mysql = False\nget_windows_postgresql = False\nget_windows_oracle = False\nget_windows_sqlserver = False\nget_windows_mongodb = False\nget_windows_redis = False\nget_windows_memcached = False\nget_windows_cassandra = False\nget_windows_elasticsearch = False\nget_windows_solr = False\nget_windows_splunk = False\nget_windows_kibana = False\nget_windows_logstash = False\nget_windows_grafana = False\nget_windows_prometheus = False\nget_windows_influxdb = False\nget_windows_telegraf = False\nget_windows_nginx = False\nget_windows_apache = False\nget_windows_iis = False\nget_windows_tomcat = False\nget_windows_jetty = False\nget_windows_glassfish = False\nget_windows_wildfly = False\nget_windows_weblogic = False\nget_windows_websphere = False\nget_windows_jboss = False\nget_windows_nodejs = False\nget_windows_express = False\nget_windows_koa = False\nget_windows_hapi = False\nget_windows_sails = False\nget_windows_meteor = False\nget_windows_django = False\nget_windows_flask = False\nget_windows_pyramid = False\nget_windows_bottle = False\nget_windows_tornado = False\nget_windows_falcon = False\nget_windows_sanic = False\nget_windows_fastapi = False\nget_windows_starlette = False\nget_windows_quart = False\nget_windows_aiohttp = False\nget_windows_requests = False\nget_windows_urllib = False\nget_windows_httpx = False\nget_windows_websockets = False\nget_windows_socketio = False\nget_windows_grpc = False\nget_windows_thrift = False\nget_windows_avro = False\nget_windows_protobuf = False\nget_windows_jsonrpc = False\nget_windows_xmlrpc = False\nget_windows_soap = False\nget_windows_rest = False\nget_windows_graphql = False\nget_windows_falcor = False\nget_windows_odata = False\nget_windows_openapi = False\nget_windows_swagger = False\nget_windows_raml = False\nget_windows_blueprint = False\nget_windows_apiary = False\nget_windows_postman = False\nget_windows_insomnia = False\nget_windows_paw = False\nget_windows_httpie = False\nget_windows_curl = False\nget_windows_wget = False\nget_windows_telnet = False\nget_windows_ssh = False\nget_windows_ftp = False\nget_windows_sftp = False\nget_windows_scp = False\nget_windows_rsync = False\nget_windows_smb = False\nget_windows_nfs = False\nget_windows_afp = False\nget_windows_webdav = False\nget_windows_tftp = False\nget_windows_dns = False\nget_windows_dhcp = False\nget_windows_ntp = False\nget_windows_snmp = False\nget_windows_ldap = False\nget_windows_kerberos = False\nget_windows_radius = False\nget_windows_tacacs = False\nget_windows_saml = False\nget_windows_oauth = False\nget_windows_openid = False\nget_windows_jwt = False\nget_windows_pam = False\nget_windows_acl = False\nget_windows_rbac = False\nget_windows_abac = False\nget_windows_mac = False\nget_windows_dac = False\nget_windows_sac = False\nget_windows_mls = False\nget_windows_selinux = False\nget_windows_apparmor = False\nget_windows_tomoyo = False\nget_windows_yama = False\nget_windows_grsecurity = False\nget_windows_pax = False\nget_windows_capabilities = False\nget_windows_namespaces = False\nget_windows_cgroups = False\nget_windows_seccomp = False\nget_windows_landlock = False\nget_windows_bpf = False\nget_windows_ebpf = False\nget_windows_xdp = False\nget_windows_af_xdp = False\nget_windows_io_uring = False\nget_windows_kqueue = False\nget_windows_epoll = False\nget_windows_poll = False\nget_windows_select = False\nget_windows_sigaction = False\nget_windows_signal = False\nget_windows_sigprocmask = False\nget_windows_sigpending = False\nget_windows_sigsuspend = False\nget_windows_sigwait = False\nget_windows_sigqueue = False\nget_windows_kill = False\nget_windows_raise = False\nget_windows_alarm = False\nget_windows_sleep = False\nget_windows_pause = False\nget_windows_nanosleep = False\nget_windows_clock = False\nget_windows_time = False\nget_windows_gettimeofday = False\nget_windows_settimeofday = False\nget_windows_adjtime = False\nget_windows_ntp_adjtime = False\nget_windows_clock_gettime = False\nget_windows_clock_settime = False\nget_windows_clock_getres = False\nget_windows_clock_nanosleep = False\nget_windows_timer_create = False\nget_windows_timer_settime = False\nget_windows_timer_gettime = False\nget_windows_timer_getoverrun = False\nget_windows_timer_delete = False\nget_windows_nanosleep = False\nget_windows_clock_nanosleep = False\nget_windows_timer_create = False\nget_windows_timer_settime = False\nget_windows_timer_gettime = False\nget_windows_timer_getoverrun = False\nget_windows_timer_delete = False\nget_windows_sched_yield = False\nget_windows_sched_setparam = False\nget_windows_sched_getparam = False\nget_windows_sched_setscheduler = False\nget_windows_sched_getscheduler = False\nget_windows_sched_get_priority_max = False\nget_windows_sched_get_priority_min = False\nget_windows_sched_rr_get_interval = False\nget_windows_sched_setaffinity = False\nget_windows_sched_getaffinity = False\nget_windows_pthread_create = False\nget_windows_pthread_exit = False\nget_windows_pthread_join = False\nget_windows_pthread_detach = False\nget_windows_pthread_self = False\nget_windows_pthread_equal = False\nget_windows_pthread_attr_init = False\nget_windows_pthread_attr_destroy = False\nget_windows_pthread_attr_getdetachstate = False\nget_windows_pthread_attr_setdetachstate = False\nget_windows_pthread_attr_getstacksize = False\nget_windows_pthread_attr_setstacksize = False\nget_windows_pthread_attr_getstackaddr = False\nget_windows_pthread_attr_setstackaddr = False\nget_windows_pthread_attr_getscope = False\nget_windows_pthread_attr_setscope = False\nget_windows_pthread_attr_getschedparam = False\nget_windows_pthread_attr_setschedparam = False\nget_windows_pthread_attr_getschedpolicy = False\nget_windows_pthread_attr_setschedpolicy = False\nget_windows_pthread_attr_getinheritsched = False\nget_windows_pthread_attr_setinheritsched = False\nget_windows_pthread_attr_getguardsize = False\nget_windows_pthread_attr_setguardsize = False\nget_windows_pthread_attr_getstack = False\nget_windows_pthread_attr_setstack = False\nget_windows_pthread_mutex_init = False\nget_windows_pthread_mutex_destroy = False\nget_windows_pthread_mutex_lock = False\nget_windows_pthread_mutex_trylock = False\nget_windows_pthread_mutex_unlock = False\nget_windows_pthread_mutexattr_init = False\nget_windows_pthread_mutexattr_destroy = False\nget_windows_pthread_mutexattr_gettype = False\nget_windows_pthread_mutexattr_settype = False\nget_windows_pthread_mutexattr_getpshared = False\nget_windows_pthread_mutexattr_setpshared = False\nget_windows_pthread_mutexattr_getprotocol = False\nget_windows_pthread_mutexattr_setprotocol = False\nget_windows_pthread_mutexattr_getprioceiling = False\nget_windows_pthread_mutexattr_setprioceiling = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_cond_init = False\nget_windows_pthread_cond_destroy = False\nget_windows_pthread_cond_wait = False\nget_windows_pthread_cond_timedwait = False\nget_windows_pthread_cond_signal = False\nget_windows_pthread_cond_broadcast = False\nget_windows_pthread_condattr_init = False\nget_windows_pthread_condattr_destroy = False\nget_windows_pthread_condattr_getpshared = False\nget_windows_pthread_condattr_setpshared = False\nget_windows_pthread_condattr_getclock = False\nget_windows_pthread_condattr_setclock = False\nget_windows_pthread_rwlock_init = False\nget_windows_pthread_rwlock_destroy = False\nget_windows_pthread_rwlock_rdlock = False\nget_windows_pthread_rwlock_tryrdlock = False\nget_windows_pthread_rwlock_wrlock = False\nget_windows_pthread_rwlock_trywrlock = False\nget_windows_pthread_rwlock_unlock = False\nget_windows_pthread_rwlockattr_init = False\nget_windows_pthread_rwlockattr_destroy = False\nget_windows_pthread_rwlockattr_getpshared = False\nget_windows_pthread_rwlockattr_setpshared = False\nget_windows_pthread_spin_init = False\nget_windows_pthread_spin_destroy = False\nget_windows_pthread_spin_lock = False\nget_windows_pthread_spin_trylock = False\nget_windows_pthread_spin_unlock = False\nget_windows_pthread_barrier_init = False\nget_windows_pthread_barrier_destroy = False\nget_windows_pthread_barrier_wait = False\nget_windows_pthread_barrierattr_init = False\nget_windows_pthread_barrierattr_destroy = False\nget_windows_pthread_barrierattr_getpshared = False\nget_windows_pthread_barrierattr_setpshared = False\nget_windows_pthread_once = False\nget_windows_pthread_key_create = False\nget_windows_pthread_key_delete = False\nget_windows_pthread_setspecific = False\nget_windows_pthread_getspecific = False\nget_windows_pthread_atfork = False\nget_windows_pthread_kill = False\nget_windows_pthread_sigmask = False\nget_windows_pthread_sigqueue = False\nget_windows_pthread_cancel = False\nget_windows_pthread_testcancel = False\nget_windows_pthread_setcancelstate = False\nget_windows_pthread_setcanceltype = False\nget_windows_pthread_getcancelstate = False\nget_windows_pthread_getcanceltype = False\nget_windows_pthread_cleanup_push = False\nget_windows_pthread_cleanup_pop = False\nget_windows_pthread_getschedparam = False\nget_windows_pthread_setschedparam = False\nget_windows_pthread_getconcurrency = False\nget_windows_pthread_setconcurrency = False\nget_windows_pthread_getcpuclockid = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget_windows_pthread_mutexattr_setrobust = False\nget_windows_pthread_mutex_consistent = False\nget_windows_pthread_mutexattr_getrobust = False\nget"
  },
  {
    "Task_id": 17,
    "Instruction": "Task Description: Create a custom HTTPConnection class that extends Python's http.client.HTTPConnection to provide enhanced functionality including proxy support, socket options, and improved error handling.\n\nClass Description: HTTPConnection is a subclass of http.client.HTTPConnection that adds additional features for making HTTP requests, including proxy support, configurable socket options, and better error handling. It maintains connection state and provides methods for making requests and processing responses.\n\nAttributes:\n- default_port: typing.ClassVar[int] - Default port for HTTP connections (80)\n- default_socket_options: typing.ClassVar[connection._TYPE_SOCKET_OPTIONS] - Default TCP socket options (TCP_NODELAY enabled)\n- socket_options: Optional[connection._TYPE_SOCKET_OPTIONS] - Custom socket options for the connection\n- proxy: Optional[Url] - Proxy URL for the connection\n- proxy_config: Optional[ProxyConfig] - Configuration for proxy connections\n- _has_connected_to_proxy: bool - Flag indicating if connected to proxy\n- _response_options: Optional[_ResponseOptions] - Stores response processing options\n- _tunnel_host: Optional[str] - Host for tunnel connection\n- _tunnel_port: Optional[int] - Port for tunnel connection\n- _tunnel_scheme: Optional[str] - Scheme for tunnel connection\n\nMethods:\n- __init__(host: str, port: Optional[int] = None, *, timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT, source_address: Optional[tuple[str, int]] = None, blocksize: int = 16384, socket_options: Optional[connection._TYPE_SOCKET_OPTIONS] = default_socket_options, proxy: Optional[Url] = None, proxy_config: Optional[ProxyConfig] = None) -> None - Initializes the HTTP connection with specified parameters\n- _new_conn() -> socket.socket - Creates a new socket connection with error handling\n- connect() -> None - Establishes the connection to the server or proxy\n- request(method: str, url: str, body: Optional[_TYPE_BODY] = None, headers: Optional[Mapping[str, str]] = None, *, chunked: bool = False, preload_content: bool = True, decode_content: bool = True, enforce_content_length: bool = True) -> None - Sends an HTTP request with the specified method, URL, body, and headers\n- getresponse() -> HTTPResponse - Gets the response from the server and returns an HTTPResponse object\n- close() -> None - Closes the connection and cleans up resources",
    "Canonical_solution": "import socket\nimport typing\nfrom http.client import HTTPConnection as _HTTPConnection\nfrom typing import Optional, Mapping, Union, Literal\nfrom .util.timeout import _TYPE_TIMEOUT, Timeout\nfrom ._base_connection import _TYPE_BODY\nfrom .util import connection\nfrom .util.url import Url\n\nclass HTTPConnection(_HTTPConnection):\n    default_port: typing.ClassVar[int] = port_by_scheme[\"http\"]\n    default_socket_options: typing.ClassVar[connection._TYPE_SOCKET_OPTIONS] = [\n        (socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n    ]\n\n    def __init__(\n        self,\n        host: str,\n        port: Optional[int] = None,\n        *,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        source_address: Optional[tuple[str, int]] = None,\n        blocksize: int = 16384,\n        socket_options: Optional[connection._TYPE_SOCKET_OPTIONS] = default_socket_options,\n        proxy: Optional[Url] = None,\n        proxy_config: Optional[ProxyConfig] = None,\n    ) -> None:\n        super().__init__(\n            host=host,\n            port=port,\n            timeout=Timeout.resolve_default_timeout(timeout),\n            source_address=source_address,\n            blocksize=blocksize,\n        )\n        self.socket_options = socket_options\n        self.proxy = proxy\n        self.proxy_config = proxy_config\n        self._has_connected_to_proxy = False\n        self._response_options = None\n        self._tunnel_host: Optional[str] = None\n        self._tunnel_port: Optional[int] = None\n        self._tunnel_scheme: Optional[str] = None\n\n    def _new_conn(self) -> socket.socket:\n        try:\n            sock = connection.create_connection(\n                (self._dns_host, self.port),\n                self.timeout,\n                source_address=self.source_address,\n                socket_options=self.socket_options,\n            )\n        except socket.gaierror as e:\n            raise NameResolutionError(self.host, self, e) from e\n        except SocketTimeout as e:\n            raise ConnectTimeoutError(\n                self,\n                f\"Connection to {self.host} timed out. (connect timeout={self.timeout})\",\n            ) from e\n        except OSError as e:\n            raise NewConnectionError(\n                self, f\"Failed to establish a new connection: {e}\"\n            ) from e\n        return sock\n\n    def connect(self) -> None:\n        self.sock = self._new_conn()\n        if self._tunnel_host:\n            self._has_connected_to_proxy = True\n            self._tunnel()\n        self._has_connected_to_proxy = bool(self.proxy)\n\n    def request(\n        self,\n        method: str,\n        url: str,\n        body: Optional[_TYPE_BODY] = None,\n        headers: Optional[Mapping[str, str]] = None,\n        *,\n        chunked: bool = False,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        enforce_content_length: bool = True,\n    ) -> None:\n        if self.sock is not None:\n            self.sock.settimeout(self.timeout)\n\n        self._response_options = _ResponseOptions(\n            request_method=method,\n            request_url=url,\n            preload_content=preload_content,\n            decode_content=decode_content,\n            enforce_content_length=enforce_content_length,\n        )\n\n        if headers is None:\n            headers = {}\n        header_keys = frozenset(to_str(k.lower()) for k in headers)\n        skip_accept_encoding = \"accept-encoding\" in header_keys\n        skip_host = \"host\" in header_keys\n        self.putrequest(\n            method, url, skip_accept_encoding=skip_accept_encoding, skip_host=skip_host\n        )\n\n        chunks_and_cl = body_to_chunks(body, method=method, blocksize=self.blocksize)\n        chunks = chunks_and_cl.chunks\n        content_length = chunks_and_cl.content_length\n\n        if chunked:\n            if \"transfer-encoding\" not in header_keys:\n                self.putheader(\"Transfer-Encoding\", \"chunked\")\n        else:\n            if \"content-length\" in header_keys:\n                chunked = False\n            elif \"transfer-encoding\" in header_keys:\n                chunked = True\n            else:\n                chunked = False\n                if content_length is None:\n                    if chunks is not None:\n                        chunked = True\n                        self.putheader(\"Transfer-Encoding\", \"chunked\")\n                else:\n                    self.putheader(\"Content-Length\", str(content_length))\n\n        if \"user-agent\" not in header_keys:\n            self.putheader(\"User-Agent\", _get_default_user_agent())\n        for header, value in headers.items():\n            self.putheader(header, value)\n        self.endheaders()\n\n        if chunks is not None:\n            for chunk in chunks:\n                if not chunk:\n                    continue\n                if isinstance(chunk, str):\n                    chunk = chunk.encode(\"utf-8\")\n                if chunked:\n                    self.send(b\"%x\\r\\n%b\\r\\n\" % (len(chunk), chunk))\n                else:\n                    self.send(chunk)\n\n        if chunked:\n            self.send(b\"0\\r\\n\\r\\n\")\n\n    def getresponse(self) -> HTTPResponse:\n        if self._response_options is None:\n            raise ResponseNotReady()\n\n        resp_options = self._response_options\n        self._response_options = None\n        self.sock.settimeout(self.timeout)\n\n        httplib_response = super().getresponse()\n        headers = HTTPHeaderDict(httplib_response.msg.items())\n\n        response = HTTPResponse(\n            body=httplib_response,\n            headers=headers,\n            status=httplib_response.status,\n            version=httplib_response.version,\n            version_string=getattr(self, \"_http_vsn_str\", \"HTTP/?\"),\n            reason=httplib_response.reason,\n            preload_content=resp_options.preload_content,\n            decode_content=resp_options.decode_content,\n            original_response=httplib_response,\n            enforce_content_length=resp_options.enforce_content_length,\n            request_method=resp_options.request_method,\n            request_url=resp_options.request_url,\n        )\n        return response\n\n    def close(self) -> None:\n        try:\n            super().close()\n        finally:\n            self.sock = None\n            self.is_verified = False\n            self.proxy_is_verified = None\n            self._has_connected_to_proxy = False\n            self._response_options = None\n            self._tunnel_host = None\n            self._tunnel_port = None\n            self._tunnel_scheme = None",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import socket\nimport typing\nfrom http import client\nfrom urllib.parse import ParseResult as Url\nfrom typing import Optional, Mapping, Tuple, ClassVar\n\n_TYPE_BODY = Optional[bytes]\n_TYPE_TIMEOUT = Optional[float]\n_DEFAULT_TIMEOUT = object()\n_TYPE_SOCKET_OPTIONS = typing.List[typing.Tuple[int, int, typing.Union[int, bytes]]]\n\nclass ProxyConfig:\n    pass\n\nclass _ResponseOptions:\n    pass\n\nclass HTTPConnection(client.HTTPConnection):\n    default_port: ClassVar[int] = 80\n    default_socket_options: ClassVar[_TYPE_SOCKET_OPTIONS] = [\n        (socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n    ]\n\n    def __init__(\n        self,\n        host: str,\n        port: Optional[int] = None,\n        *,\n        timeout: _TYPE_TIMEOUT = _DEFAULT_TIMEOUT,\n        source_address: Optional[Tuple[str, int]] = None,\n        blocksize: int = 16384,\n        socket_options: Optional[_TYPE_SOCKET_OPTIONS] = None,\n        proxy: Optional[Url] = None,\n        proxy_config: Optional[ProxyConfig] = None,\n    ) -> None:\n        super().__init__(\n            host=host,\n            port=port,\n            timeout=timeout,\n            source_address=source_address,\n            blocksize=blocksize,\n        )\n        self.socket_options = socket_options or self.default_socket_options\n        self.proxy = proxy\n        self.proxy_config = proxy_config\n        self._has_connected_to_proxy = False\n        self._response_options = None\n        self._tunnel_host = None\n        self._tunnel_port = None\n        self._tunnel_scheme = None\n\n    def _new_conn(self) -> socket.socket:\n        try:\n            sock = socket.create_connection(\n                (self.host, self.port),\n                self.timeout,\n                self.source_address,\n            )\n            for option in self.socket_options:\n                sock.setsockopt(*option)\n            return sock\n        except socket.error as e:\n            raise client.ConnectError(\n                f\"Failed to establish a new connection: {e}\"\n            ) from e\n\n    def connect(self) -> None:\n        if self.proxy and not self._has_connected_to_proxy:\n            self._connect_to_proxy()\n        else:\n            super().connect()\n\n    def _connect_to_proxy(self) -> None:\n        proxy_host = self.proxy.hostname\n        proxy_port = self.proxy.port or 80\n        self.sock = socket.create_connection(\n            (proxy_host, proxy_port),\n            self.timeout,\n            self.source_address,\n        )\n        for option in self.socket_options:\n            self.sock.setsockopt(*option)\n        self._has_connected_to_proxy = True\n\n    def request(\n        self,\n        method: str,\n        url: str,\n        body: _TYPE_BODY = None,\n        headers: Optional[Mapping[str, str]] = None,\n        *,\n        chunked: bool = False,\n        preload_content: bool = True,\n        decode_content: bool = True,\n        enforce_content_length: bool = True,\n    ) -> None:\n        if self.proxy and not self._has_connected_to_proxy:\n            self._tunnel_host = self.host\n            self._tunnel_port = self.port\n            self._tunnel_scheme = \"http\"\n            self._response_options = {\n                \"preload_content\": preload_content,\n                \"decode_content\": decode_content,\n                \"enforce_content_length\": enforce_content_length,\n            }\n            super().request(method, url, body, headers, chunked=chunked)\n        else:\n            super().request(\n                method,\n                url,\n                body,\n                headers,\n                chunked=chunked,\n                preload_content=preload_content,\n                decode_content=decode_content,\n                enforce_content_length=enforce_content_length,\n            )\n\n    def getresponse(self) -> client.HTTPResponse:\n        response = super().getresponse()\n        if self._response_options:\n            response.preload_content = self._response_options[\"preload_content\"]\n            response.decode_content = self._response_options[\"decode_content\"]\n            response.enforce_content_length = self._response_options[\"enforce_content_length\"]\n            self._response_options = None\n        return response\n\n    def close(self) -> None:\n        self._has_connected_to_proxy = False\n        self._tunnel_host = None\n        self._tunnel_port = None\n        self._tunnel_scheme = None\n        self._response_options = None\n        super().close()"
  },
  {
    "Task_id": 170,
    "Instruction": "Task Description: Create a simple web application using Flask that interacts with Redis to track and display the number of times the homepage has been accessed.\n\nFunction Description: The function `hello()` is a Flask route handler for the root URL ('/'). It increments a counter in Redis each time the route is accessed and returns a greeting message along with the current count of visits.\n\nInput: \n- None (The function handles HTTP GET requests to the root URL without requiring explicit input parameters)\n\nOutput: \n- A string response containing \"Hello World!\" followed by the current visit count in the format: \"Hello World! \u8be5\u9875\u9762\u5df2\u88ab\u8bbf\u95ee {count} \u6b21\u3002\\n\" where {count} is the number of times the page has been accessed.",
    "Canonical_solution": "from flask import Flask\nfrom redis import Redis\n\napp = Flask(__name__)\nredis = Redis(host='redis', port=6379)\n\n@app.route('/')\ndef hello():\n    count = redis.incr('hits')\n    return 'Hello World! \u8be5\u9875\u9762\u5df2\u88ab\u8bbf\u95ee {} \u6b21\u3002\\n'.format(count)\n\nif __name__ == \"__main__\":\n    app.run(host=\"0.0.0.0\", debug=True)",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask\nimport redis\n\napp = Flask(__name__)\nr = redis.Redis(host='localhost', port=6379, db=0)\n\n@app.route('/')\ndef hello():\n    count = r.incr('hits')\n    return f\"Hello World! \u8be5\u9875\u9762\u5df2\u88ab\u8bbf\u95ee {count} \u6b21\u3002\\n\""
  },
  {
    "Task_id": 172,
    "Instruction": "Task Description: Create a Python class that implements an API server using Flask and gevent WSGI server, capable of serving web UI and handling REST API requests with proper error handling.\n\nClass Description: APIServer is a class that encapsulates a Flask web server with gevent WSGI capabilities. It handles server lifecycle management, web UI serving, and error handling for REST API endpoints.\n\nAttributes:\n- stop_event: [Event] - A gevent event used to signal server shutdown\n- config: [RestApiConfig] - Configuration object containing server settings (host, port, etc.)\n- rest_api: [RestAPI] - Reference to the main REST API implementation\n- wsgiserver: [Optional[WSGIServer]] - The gevent WSGI server instance\n- flask_app: [Flask] - The Flask application instance\n\nMethods:\n- __init__(rest_api: \"RestAPI\", config: RestApiConfig, eth_rpc_endpoint: str = None) -> None - Initializes the API server with configuration and dependencies\n- _serve_webui(file_name: str = \"index.html\") -> Response - Serves web UI files from configured directory (handles fallback to index.html)\n- _run() -> None - Internal method to run the WSGI server indefinitely\n- start() -> None - Starts the API server and initializes the WSGI server\n- stop() -> None - Stops the API server gracefully\n- unhandled_exception(exception: Exception) -> Response - Global exception handler that returns error responses and logs critical errors",
    "Canonical_solution": "import errno\nimport logging\nimport socket\nfrom typing import Optional\n\nimport gevent\nimport gevent.pool\nfrom flask import Flask\nfrom gevent.event import Event\nfrom gevent.pywsgi import WSGIServer\nfrom werkzeug.exceptions import NotFound\n\nclass APIServer:\n    def __init__(\n        self,\n        rest_api: \"RestAPI\",\n        config: RestApiConfig,\n        eth_rpc_endpoint: str = None,\n    ) -> None:\n        self.stop_event = Event()\n        self.config = config\n        self.rest_api = rest_api\n        self.wsgiserver: Optional[WSGIServer] = None\n\n        flask_app = Flask(__name__)\n        self.flask_app = flask_app\n\n        self.flask_app.register_error_handler(HTTPStatus.NOT_FOUND, endpoint_not_found)\n        self.flask_app.register_error_handler(Exception, self.unhandled_exception)\n\n        if config.web_ui_enabled:\n            for route in (\"/ui/<path:file_name>\", \"/ui\", \"/ui/\", \"/index.html\", \"/\"):\n                self.flask_app.add_url_rule(\n                    route, route, view_func=self._serve_webui, methods=(\"GET\",)\n                )\n\n    def _serve_webui(self, file_name: str = \"index.html\") -> Response:\n        try:\n            if not file_name:\n                raise NotFound\n\n            response = send_from_directory(self.flask_app.config[\"WEBUI_PATH\"], file_name)\n        except (NotFound, AssertionError):\n            if file_name.endswith(\".json\"):\n                response = api_error(\n                    \"Service unavailable, try again later\", HTTPStatus.SERVICE_UNAVAILABLE\n                )\n            else:\n                response = send_from_directory(self.flask_app.config[\"WEBUI_PATH\"], \"index.html\")\n        return response\n\n    def _run(self) -> None:\n        try:\n            if self.wsgiserver is not None:\n                self.wsgiserver.serve_forever()\n        except gevent.GreenletExit:\n            raise\n        except Exception:\n            self.stop()\n            raise\n\n    def start(self) -> None:\n        self.stop_event.clear()\n\n        wsgi_log = logging.getLogger(__name__ + \".pywsgi\")\n        pool = gevent.pool.Pool()\n        wsgiserver = WSGIServer(\n            (self.config.host, self.config.port),\n            self.flask_app,\n            log=wsgi_log,\n            error_log=wsgi_log,\n            spawn=pool,\n        )\n\n        try:\n            wsgiserver.init_socket()\n        except socket.error as e:\n            if e.errno == errno.EADDRINUSE:\n                raise APIServerPortInUseError(f\"{self.config.host}:{self.config.port}\")\n            raise\n\n        self.wsgiserver = wsgiserver\n        super().start()\n\n    def stop(self) -> None:\n        self.stop_event.set()\n\n        if self.wsgiserver is not None:\n            self.wsgiserver.stop(timeout=5)\n            self.wsgiserver = None\n\n    def unhandled_exception(self, exception: Exception) -> Response:\n        log.critical(\n            \"Unhandled exception when processing endpoint request\",\n            exc_info=True,\n            node=self.rest_api.checksum_address,\n        )\n        self.greenlet.kill(exception)\n        return api_error([str(exception)], HTTPStatus.INTERNAL_SERVER_ERROR)",
    "Code_level": "Class-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, Response, send_from_directory\nfrom gevent import pywsgi\nfrom gevent.event import Event\nfrom typing import Optional\nimport logging\nimport os\n\nclass APIServer:\n    def __init__(self, rest_api: \"RestAPI\", config: \"RestApiConfig\", eth_rpc_endpoint: str = None) -> None:\n        self.stop_event = Event()\n        self.config = config\n        self.rest_api = rest_api\n        self.wsgiserver = None\n        self.flask_app = Flask(__name__)\n        self.eth_rpc_endpoint = eth_rpc_endpoint\n\n        @self.flask_app.route(\"/\", defaults={\"file_name\": \"index.html\"})\n        @self.flask_app.route(\"/<path:file_name>\")\n        def serve_webui(file_name: str) -> Response:\n            return self._serve_webui(file_name)\n\n        @self.flask_app.errorhandler(Exception)\n        def handle_exception(exception: Exception) -> Response:\n            return self.unhandled_exception(exception)\n\n    def _serve_webui(self, file_name: str = \"index.html\") -> Response:\n        try:\n            return send_from_directory(self.config.webui_dir, file_name)\n        except Exception:\n            return send_from_directory(self.config.webui_dir, \"index.html\")\n\n    def _run(self) -> None:\n        self.wsgiserver = pywsgi.WSGIServer(\n            (self.config.host, self.config.port),\n            self.flask_app,\n            log=None,\n            error_log=logging.getLogger()\n        )\n        self.wsgiserver.serve_forever()\n\n    def start(self) -> None:\n        self.stop_event.clear()\n        self._run()\n\n    def stop(self) -> None:\n        if self.wsgiserver:\n            self.wsgiserver.stop()\n        self.stop_event.set()\n\n    def unhandled_exception(self, exception: Exception) -> Response:\n        logging.critical(f\"Unhandled exception: {str(exception)}\", exc_info=True)\n        return Response(\n            response=f\"Internal server error: {str(exception)}\",\n            status=500,\n            mimetype=\"text/plain\"\n        )"
  },
  {
    "Task_id": 177,
    "Instruction": "Task Description: Create a RESTful API for a Todo application using Flask and Flask-RESTx that supports CRUD operations (Create, Read, Update, Delete) for todo items.\n\nClass Description: TodoAPI is a class that encapsulates the functionality of a Todo REST API. It initializes a Flask application, sets up API routes, defines data models, and manages todo items through a data access object (DAO).\n\nAttributes:\n- app: Flask - The Flask application instance\n- api: Api - The Flask-RESTx API instance for route registration\n- ns: Namespace - The namespace for organizing todo-related routes\n- todo: Model - The data model for todo items\n- todo_dao: TodoDAO - The data access object for managing todo items\n\nMethods:\n- __init__() -> None - Initializes the Flask app, API, namespace, data model, and sample data\n- _setup_routes() -> None - Defines all API routes and their handlers\n- run() -> None - Starts the Flask development server\n\nClass Description: TodoDAO is a helper class that manages the storage and retrieval of todo items in memory.\n\nAttributes:\n- counter: int - Auto-incrementing counter for todo IDs\n- todos: list - List storing all todo items\n\nMethods:\n- get(id: int) -> dict - Retrieves a todo item by ID or aborts with 404 if not found\n- create(data: dict) -> dict - Creates a new todo item with an auto-generated ID\n- update(id: int, data: dict) -> dict - Updates an existing todo item\n- delete(id: int) -> None - Deletes a todo item by ID",
    "Canonical_solution": "from flask import Flask\nfrom flask_restx import Api, Resource, fields\nfrom werkzeug.middleware.proxy_fix import ProxyFix\n\nclass TodoAPI:\n    def __init__(self):\n        self.app = Flask(__name__)\n        self.app.wsgi_app = ProxyFix(self.app.wsgi_app)\n        self.api = Api(\n            self.app,\n            version=\"1.0\",\n            title=\"TodoMVC API\",\n            description=\"A simple TodoMVC API\",\n        )\n        self.ns = self.api.namespace(\"todos\", description=\"TODO operations\")\n        \n        self.todo = self.api.model(\n            \"Todo\",\n            {\n                \"id\": fields.Integer(readonly=True, description=\"The task unique identifier\"),\n                \"task\": fields.String(required=True, description=\"The task details\"),\n            },\n        )\n        \n        self.todo_dao = TodoDAO()\n        self.todo_dao.create({\"task\": \"Build an API\"})\n        self.todo_dao.create({\"task\": \"?????\"})\n        self.todo_dao.create({\"task\": \"profit!\"})\n        \n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.ns.route(\"/\")\n        class TodoList(Resource):\n            @self.ns.doc(\"list_todos\")\n            @self.ns.marshal_list_with(self.todo)\n            def get(self):\n                return self.todo_dao.todos\n\n            @self.ns.doc(\"create_todo\")\n            @self.ns.expect(self.todo)\n            @self.ns.marshal_with(self.todo, code=201)\n            def post(self):\n                return self.todo_dao.create(self.api.payload), 201\n\n        @self.ns.route(\"/<int:id>\")\n        @self.ns.response(404, \"Todo not found\")\n        @self.ns.param(\"id\", \"The task identifier\")\n        class Todo(Resource):\n            @self.ns.doc(\"get_todo\")\n            @self.ns.marshal_with(self.todo)\n            def get(self, id):\n                return self.todo_dao.get(id)\n\n            @self.ns.doc(\"delete_todo\")\n            @self.ns.response(204, \"Todo deleted\")\n            def delete(self, id):\n                self.todo_dao.delete(id)\n                return \"\", 204\n\n            @self.ns.expect(self.todo)\n            @self.ns.marshal_with(self.todo)\n            def put(self, id):\n                return self.todo_dao.update(id, self.api.payload)\n\n    def run(self):\n        self.app.run(debug=True)\n\nclass TodoDAO:\n    def __init__(self):\n        self.counter = 0\n        self.todos = []\n\n    def get(self, id):\n        for todo in self.todos:\n            if todo[\"id\"] == id:\n                return todo\n        self.api.abort(404, \"Todo {} doesn't exist\".format(id))\n\n    def create(self, data):\n        todo = data\n        todo[\"id\"] = self.counter = self.counter + 1\n        self.todos.append(todo)\n        return todo\n\n    def update(self, id, data):\n        todo = self.get(id)\n        todo.update(data)\n        return todo\n\n    def delete(self, id):\n        todo = self.get(id)\n        self.todos.remove(todo)",
    "Code_level": "Class-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask\nfrom flask_restx import Api, Resource, fields, Namespace, abort\n\nclass TodoDAO:\n    def __init__(self):\n        self.counter = 0\n        self.todos = []\n\n    def get(self, id):\n        for todo in self.todos:\n            if todo['id'] == id:\n                return todo\n        abort(404, message=\"Todo {} doesn't exist\".format(id))\n\n    def create(self, data):\n        todo = data\n        todo['id'] = self.counter = self.counter + 1\n        self.todos.append(todo)\n        return todo\n\n    def update(self, id, data):\n        todo = self.get(id)\n        todo.update(data)\n        return todo\n\n    def delete(self, id):\n        todo = self.get(id)\n        self.todos.remove(todo)\n\nclass TodoAPI:\n    def __init__(self):\n        self.app = Flask(__name__)\n        self.api = Api(self.app, version='1.0', title='Todo API', description='A simple Todo API')\n        self.ns = self.api.namespace('todos', description='TODO operations')\n\n        self.todo = self.api.model('Todo', {\n            'id': fields.Integer(readonly=True, description='The task unique identifier'),\n            'task': fields.String(required=True, description='The task details'),\n            'completed': fields.Boolean(required=False, description='The task completion status')\n        })\n\n        self.todo_dao = TodoDAO()\n        self.todo_dao.create({'task': 'Build an API', 'completed': False})\n        self.todo_dao.create({'task': '????', 'completed': False})\n        self.todo_dao.create({'task': 'profit!', 'completed': False})\n\n        self._setup_routes()\n\n    def _setup_routes(self):\n        @self.ns.route('/')\n        class TodoList(Resource):\n            @self.ns.doc('list_todos')\n            @self.ns.marshal_list_with(self.todo)\n            def get(self):\n                return self.todo_dao.todos\n\n            @self.ns.doc('create_todo')\n            @self.ns.expect(self.todo)\n            @self.ns.marshal_with(self.todo, code=201)\n            def post(self):\n                return self.todo_dao.create(self.api.payload), 201\n\n        @self.ns.route('/<int:id>')\n        @self.ns.response(404, 'Todo not found')\n        @self.ns.param('id', 'The task identifier')\n        class Todo(Resource):\n            @self.ns.doc('get_todo')\n            @self.ns.marshal_with(self.todo)\n            def get(self, id):\n                return self.todo_dao.get(id)\n\n            @self.ns.doc('delete_todo')\n            @self.ns.response(204, 'Todo deleted')\n            def delete(self, id):\n                self.todo_dao.delete(id)\n                return '', 204\n\n            @self.ns.expect(self.todo)\n            @self.ns.marshal_with(self.todo)\n            def put(self, id):\n                return self.todo_dao.update(id, self.api.payload)\n\n    def run(self):\n        self.app.run(debug=True)"
  },
  {
    "Task_id": 19,
    "Instruction": "Task Description: Create a Python class that simulates multiple HTTP clients making concurrent requests to a target server, tracking successful and failed requests, with support for both HTTP and HTTPS protocols.\n\nClass Description: Striker is a Process subclass designed to simulate multiple concurrent HTTP clients. It creates and manages multiple socket connections to a target server, sends randomized HTTP requests, tracks request counts, and handles connection cleanup.\n\nAttributes:\n- counter: [list] - Shared counter for tracking successful and failed requests (index 0 for success, 1 for failures)\n- nr_socks: [int] - Number of concurrent socket connections to maintain\n- runnable: [bool] - Control flag for the main execution loop\n- socks: [list] - List of active socket connections\n- ssl: [bool] - Flag indicating whether to use HTTPS\n- host: [str] - Target server hostname\n- url: [str] - Base URL path for requests\n- port: [int] - Target server port number\n- referers: [list] - List of referer URLs for request randomization\n\nMethods:\n- __init__(url, nr_sockets, counter) -> [None] - Initializes the Striker with target URL, connection count, and shared counter\n- run() -> [None] - Main execution loop that creates connections, sends requests, and processes responses\n- closeConnections() -> [None] - Closes all active socket connections\n- createPayload() -> [tuple] - Generates randomized request payload (URL and headers)\n- generateData() -> [tuple] - Creates request URL with random query string and headers\n- stop() -> [None] - Stops the execution loop and cleans up resources\n- incCounter() -> [None] - Increments the success counter\n- incFailed() -> [None] - Increments the failure counter",
    "Canonical_solution": "from multiprocessing import Process, Manager\nimport urlparse\nimport random\nimport sys\n\nif sys.version_info < (3,0):\n    import httplib\n    HTTPCLIENT = httplib\nelse:\n    import http.client\n    HTTPCLIENT = http.client\n\nclass Striker(Process):\n    def __init__(self, url, nr_sockets, counter):\n        super(Striker, self).__init__()\n        self.counter = counter\n        self.nr_socks = nr_sockets\n        self.runnable = True\n        self.socks = []\n        \n        parsedUrl = urlparse.urlparse(url)\n        self.ssl = parsedUrl.scheme == 'https'\n        self.host = parsedUrl.netloc.split(':')[0]\n        self.url = parsedUrl.path\n        self.port = parsedUrl.port or (443 if self.ssl else 80)\n        self.referers = [\n            'http://www.google.com/',\n            'http://www.bing.com/',\n            'http://' + self.host + '/'\n        ]\n\n    def run(self):\n        while self.runnable:\n            try:\n                for i in range(self.nr_socks):\n                    if self.ssl:\n                        c = HTTPCLIENT.HTTPSConnection(self.host, self.port)\n                    else:\n                        c = HTTPCLIENT.HTTPConnection(self.host, self.port)\n                    self.socks.append(c)\n\n                for conn_req in self.socks:\n                    (url, headers) = self.createPayload()\n                    conn_req.request('GET', url, None, headers)\n\n                for conn_resp in self.socks:\n                    resp = conn_resp.getresponse()\n                    self.incCounter()\n\n                self.closeConnections()\n            except:\n                self.incFailed()\n\n    def closeConnections(self):\n        for conn in self.socks:\n            try:\n                conn.close()\n            except:\n                pass\n\n    def createPayload(self):\n        req_url, headers = self.generateData()\n        random_keys = headers.keys()\n        random.shuffle(random_keys)\n        random_headers = {}\n        for header_name in random_keys:\n            random_headers[header_name] = headers[header_name]\n        return (req_url, random_headers)\n\n    def generateData(self):\n        param_joiner = \"&\" if self.url.count(\"?\") > 0 else \"?\"\n        request_url = self.url + param_joiner + self.generateQueryString(random.randint(1,5))\n        http_headers = self.generateRandomHeaders()\n        return (request_url, http_headers)\n\n    def stop(self):\n        self.runnable = False\n        self.closeConnections()\n        self.terminate()\n\n    def incCounter(self):\n        try:\n            self.counter[0] += 1\n        except:\n            pass\n\n    def incFailed(self):\n        try:\n            self.counter[1] += 1\n        except:\n            pass",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import socket\nimport ssl\nimport random\nimport time\nfrom multiprocessing import Process\n\nclass Striker(Process):\n    def __init__(self, url, nr_sockets, counter):\n        super().__init__()\n        self.counter = counter\n        self.nr_socks = nr_sockets\n        self.runnable = True\n        self.socks = []\n        self.ssl = False\n        self.host = ''\n        self.url = ''\n        self.port = 80\n        self.referers = [\n            'http://www.google.com/?q=',\n            'http://www.usatoday.com/search/results?q=',\n            'http://engadget.search.aol.com/search?q='\n        ]\n        self._parse_url(url)\n\n    def _parse_url(self, url):\n        if url.startswith('https://'):\n            self.ssl = True\n            self.port = 443\n            url = url[8:]\n        elif url.startswith('http://'):\n            url = url[7:]\n        parts = url.split('/', 1)\n        self.host = parts[0]\n        self.url = '/' + parts[1] if len(parts) > 1 else '/'\n\n    def run(self):\n        while self.runnable:\n            try:\n                self._maintain_connections()\n                self._send_requests()\n                time.sleep(0.1)\n            except Exception:\n                self.incFailed()\n                self.closeConnections()\n\n    def _maintain_connections(self):\n        while len(self.socks) < self.nr_socks:\n            try:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n                sock.settimeout(4)\n                sock.connect((self.host, self.port))\n                if self.ssl:\n                    sock = ssl.wrap_socket(sock)\n                self.socks.append(sock)\n            except Exception:\n                self.incFailed()\n\n    def _send_requests(self):\n        for sock in list(self.socks):\n            try:\n                path, headers = self.createPayload()\n                request = f\"GET {path} HTTP/1.1\\r\\nHost: {self.host}\\r\\n\"\n                for header in headers:\n                    request += f\"{header}\\r\\n\"\n                request += \"Connection: keep-alive\\r\\n\\r\\n\"\n                sock.send(request.encode())\n                self.incCounter()\n            except Exception:\n                self.incFailed()\n                self.socks.remove(sock)\n                sock.close()\n\n    def createPayload(self):\n        path, headers = self.generateData()\n        return path, headers\n\n    def generateData(self):\n        param_joiner = \"?\" if \"?\" not in self.url else \"&\"\n        path = f\"{self.url}{param_joiner}{random.randint(0, 1000)}\"\n        headers = [\n            f\"User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\",\n            f\"Referer: {random.choice(self.referers)}{random.randint(0, 1000)}\",\n            \"Accept-Language: en-US,en;q=0.9\"\n        ]\n        return path, headers\n\n    def closeConnections(self):\n        for sock in self.socks:\n            try:\n                sock.close()\n            except Exception:\n                pass\n        self.socks = []\n\n    def stop(self):\n        self.runnable = False\n        self.closeConnections()\n\n    def incCounter(self):\n        with self.counter.get_lock():\n            self.counter[0] += 1\n\n    def incFailed(self):\n        with self.counter.get_lock():\n            self.counter[1] += 1"
  },
  {
    "Task_id": 190,
    "Instruction": "Task Description: Create a WhatsApp client class that interacts with the WhatsApp Business API to send text messages to users.\n\nClass Description: The WhatsAppClient class encapsulates functionality for sending text messages through the WhatsApp Business API. It handles authentication, API endpoint construction, and message payload preparation.\n\nAttributes:\nAPI_URL: str - Base URL for the WhatsApp Business API (default: \"https://graph.facebook.com/v17.0/\")\nWHATSAPP_API_TOKEN: str - Temporary access token for authenticating with the WhatsApp API\nWHATSAPP_CLOUD_NUMBER_ID: str - Phone number ID associated with the WhatsApp Business account\nheaders: dict - HTTP headers containing authorization and content type information\n\nMethods:\n__init__: () -> None - Initializes the WhatsAppClient instance by setting up authorization headers and constructing the base API URL\nsend_text_message: (message: str, phone_number: str) -> int - Sends a text message to the specified phone number via WhatsApp API and returns the HTTP status code",
    "Canonical_solution": "from flask import Flask\nfrom flask import request\nimport os\nimport requests\nimport json\n\nclass WhatsAppClient:\n    API_URL = \"https://graph.facebook.com/v17.0/\"\n    WHATSAPP_API_TOKEN = \"<Temporary access token from your WhatsApp API Setup>\"\n    WHATSAPP_CLOUD_NUMBER_ID = \"<Phone number ID from your WhatsApp API Setup>\"\n\n    def __init__(self):\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.WHATSAPP_API_TOKEN}\",\n            \"Content-Type\": \"application/json\",\n        }\n        self.API_URL = self.API_URL + self.WHATSAPP_CLOUD_NUMBER_ID\n\n    def send_text_message(self, message, phone_number):\n        payload = {\n            \"messaging_product\": 'whatsapp',\n            \"to\": phone_number,\n            \"type\": \"text\",\n            \"text\": {\n                \"preview_url\": False,\n                \"body\": message\n            }\n        }\n        response = requests.post(f\"{self.API_URL}/messages\", json=payload, headers=self.headers)\n        print(response.status_code)\n        assert response.status_code == 200, \"Error sending message\"\n        return response.status_code\n\napp = Flask(__name__)\n\n@app.route('/msgrcvd', methods=['POST', 'GET'])\ndef msgrcvd():\n    message = request.args.get('message')\n    answer = llm(message)\n    print(message)\n    print(answer)\n    client.send_text_message(llm(message), \"<your phone number>\")\n    return message + \"<p/>\" + answer",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "import requests\n\nclass WhatsAppClient:\n    def __init__(self, api_token, cloud_number_id, api_url=\"https://graph.facebook.com/v17.0/\"):\n        self.API_URL = api_url\n        self.WHATSAPP_API_TOKEN = api_token\n        self.WHATSAPP_CLOUD_NUMBER_ID = cloud_number_id\n        self.headers = {\n            \"Authorization\": f\"Bearer {self.WHATSAPP_API_TOKEN}\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def send_text_message(self, message, phone_number):\n        endpoint = f\"{self.API_URL}{self.WHATSAPP_CLOUD_NUMBER_ID}/messages\"\n        payload = {\n            \"messaging_product\": \"whatsapp\",\n            \"recipient_type\": \"individual\",\n            \"to\": phone_number,\n            \"type\": \"text\",\n            \"text\": {\n                \"body\": message\n            }\n        }\n        response = requests.post(endpoint, json=payload, headers=self.headers)\n        return response.status_code"
  },
  {
    "Task_id": 194,
    "Instruction": "Task Description: Create a Flask web application that serves as a code beautifier service. The application should take a directory path as input, and for each request with a specific slug, it should locate the corresponding text file in the directory, apply syntax highlighting, and return the formatted HTML.\n\nFunction Description: The `beautify` function is a Flask route handler that processes HTTP GET requests with a slug parameter. It checks if the slug is valid, locates the corresponding text file in the specified directory, reads the file content, applies syntax highlighting using Pygments, and returns the formatted HTML. If the slug is invalid or the file is not found, it returns a 404 error.\n\nInput: \n- `slug` (str): A URL path segment representing the name of the directory containing the text file to be beautified.\n\nOutput: \n- If successful: Returns HTML with syntax-highlighted code.\n- If unsuccessful: Returns a 404 error response.",
    "Canonical_solution": "from flask import Flask, abort, redirect\napp = Flask(__name__)\n\nimport argparse, os, pygments\nfrom pygments import highlight\nfrom pygments.lexers import guess_lexer\nfrom pygments.formatters import HtmlFormatter\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"root_dir\", help=\"Path to directory with pastes\")\nargs = parser.parse_args()\n\n@app.route('/<slug>')\ndef beautify(slug):\n    if len(slug) > 64:\n        abort(404)\n\n    target_dir = os.path.join(args.root_dir, slug)\n\n    if not target_dir.startswith(args.root_dir):\n        abort(404)\n\n    if os.path.isdir(target_dir):\n        target_file = os.path.join(target_dir, \"index.txt\")\n        \n        with open(target_file) as f:\n            code = f.read()\n            lexer = guess_lexer(code)\n            formatter = HtmlFormatter(linenos=True, full=True)\n            return highlight(code, lexer, formatter)\n\n    abort(404)",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, render_template, abort\nfrom pygments import highlight\nfrom pygments.lexers import get_lexer_by_name\nfrom pygments.formatters import HtmlFormatter\nimport os\n\napp = Flask(__name__)\n\n@app.route('/beautify/<slug>')\ndef beautify(slug):\n    directory = 'text_files'\n    file_path = os.path.join(directory, f\"{slug}.txt\")\n    \n    if not os.path.exists(file_path):\n        abort(404)\n    \n    with open(file_path, 'r') as file:\n        code = file.read()\n    \n    lexer = get_lexer_by_name('python', stripall=True)\n    formatter = HtmlFormatter(linenos=True, cssclass='source')\n    highlighted_code = highlight(code, lexer, formatter)\n    \n    return f\"\"\"\n    <!DOCTYPE html>\n    <html>\n    <head>\n        <title>Code Beautifier</title>\n        <style>{formatter.get_style_defs('.highlight')}</style>\n    </head>\n    <body>\n        {highlighted_code}\n    </body>\n    </html>\n    \"\"\""
  },
  {
    "Task_id": 199,
    "Instruction": "Task Description: Create a Flask-based HTTP server to visualize and navigate through a robot dataset, allowing users to view different episodes and associated metadata.\n\nFunction Description: The `run_server` function initializes and runs a Flask web server that provides a user interface for exploring robot datasets. It handles dataset navigation, episode selection, and displays relevant information including videos and task descriptions. The server supports both local datasets and remote datasets hosted on Hugging Face.\n\nInput:\n- dataset: Union[LeRobotDataset, IterableNamespace, None] - The dataset to visualize (can be None if loading from URL)\n- episodes: Optional[List[int]] - List of episode IDs to make available (None for all episodes)\n- host: str - Host address to run the server on\n- port: str - Port number to run the server on\n- static_folder: Path - Path to static files directory\n- template_folder: Path - Path to HTML templates directory\n\nOutput: None (runs a Flask server indefinitely until interrupted)\n\nMethods (route handlers):\n1. homepage() -> Union[Redirect, Response] - Handles root URL, redirects to first episode or shows dataset selection page\n2. show_first_episode(dataset_namespace: str, dataset_name: str) -> Redirect - Redirects to the first episode of a specified dataset\n3. show_episode(dataset_namespace: str, dataset_name: str, episode_id: int) -> Union[Response, Tuple[str, int]] - Displays detailed information for a specific episode including videos and task data\n\nThe server provides the following features:\n- Dataset version validation\n- Episode navigation\n- Video playback\n- Task description display\n- Data visualization\n- Support for both local and remote datasets\n- Featured datasets showcase",
    "Canonical_solution": "import argparse\nimport json\nimport logging\nimport re\nimport shutil\nimport tempfile\nfrom pathlib import Path\n\nimport requests\nfrom flask import Flask, redirect, render_template, request, url_for\n\ndef run_server(\n    dataset: LeRobotDataset | IterableNamespace | None,\n    episodes: list[int] | None,\n    host: str,\n    port: str,\n    static_folder: Path,\n    template_folder: Path,\n):\n    app = Flask(__name__, static_folder=static_folder.resolve(), template_folder=template_folder.resolve())\n    app.config[\"SEND_FILE_MAX_AGE_DEFAULT\"] = 0  # specifying not to cache\n\n    @app.route(\"/\")\n    def hommepage(dataset=dataset):\n        if dataset:\n            dataset_namespace, dataset_name = dataset.repo_id.split(\"/\")\n            return redirect(\n                url_for(\n                    \"show_episode\",\n                    dataset_namespace=dataset_namespace,\n                    dataset_name=dataset_name,\n                    episode_id=0,\n                )\n            )\n\n        dataset_param, episode_param = None, None\n        all_params = request.args\n        if \"dataset\" in all_params:\n            dataset_param = all_params[\"dataset\"]\n        if \"episode\" in all_params:\n            episode_param = int(all_params[\"episode\"])\n\n        if dataset_param:\n            dataset_namespace, dataset_name = dataset_param.split(\"/\")\n            return redirect(\n                url_for(\n                    \"show_episode\",\n                    dataset_namespace=dataset_namespace,\n                    dataset_name=dataset_name,\n                    episode_id=episode_param if episode_param is not None else 0,\n                )\n            )\n\n        featured_datasets = [\n            \"lerobot/aloha_static_cups_open\",\n            \"lerobot/columbia_cairlab_pusht_real\",\n            \"lerobot/taco_play\",\n        ]\n        return render_template(\n            \"visualize_dataset_homepage.html\",\n            featured_datasets=featured_datasets,\n            lerobot_datasets=available_datasets,\n        )\n\n    @app.route(\"/<string:dataset_namespace>/<string:dataset_name>\")\n    def show_first_episode(dataset_namespace, dataset_name):\n        first_episode_id = 0\n        return redirect(\n            url_for(\n                \"show_episode\",\n                dataset_namespace=dataset_namespace,\n                dataset_name=dataset_name,\n                episode_id=first_episode_id,\n            )\n        )\n\n    @app.route(\"/<string:dataset_namespace>/<string:dataset_name>/episode_<int:episode_id>\")\n    def show_episode(dataset_namespace, dataset_name, episode_id, dataset=dataset, episodes=episodes):\n        repo_id = f\"{dataset_namespace}/{dataset_name}\"\n        try:\n            if dataset is None:\n                dataset = get_dataset_info(repo_id)\n        except FileNotFoundError:\n            return (\n                \"Make sure to convert your LeRobotDataset to v2 & above. See how to convert your dataset at https://github.com/huggingface/lerobot/pull/461\",\n                400,\n            )\n        dataset_version = (\n            dataset.meta._version if isinstance(dataset, LeRobotDataset) else dataset.codebase_version\n        )\n        match = re.search(r\"v(\\d+)\\.\", dataset_version)\n        if match:\n            major_version = int(match.group(1))\n            if major_version < 2:\n                return \"Make sure to convert your LeRobotDataset to v2 & above.\"\n\n        episode_data_csv_str, columns = get_episode_data(dataset, episode_id)\n        dataset_info = {\n            \"repo_id\": f\"{dataset_namespace}/{dataset_name}\",\n            \"num_samples\": dataset.num_frames\n            if isinstance(dataset, LeRobotDataset)\n            else dataset.total_frames,\n            \"num_episodes\": dataset.num_episodes\n            if isinstance(dataset, LeRobotDataset)\n            else dataset.total_episodes,\n            \"fps\": dataset.fps,\n        }\n        if isinstance(dataset, LeRobotDataset):\n            video_paths = [\n                dataset.meta.get_video_file_path(episode_id, key) for key in dataset.meta.video_keys\n            ]\n            videos_info = [\n                {\"url\": url_for(\"static\", filename=video_path), \"filename\": video_path.parent.name}\n                for video_path in video_paths\n            ]\n            tasks = dataset.meta.episodes[0][\"tasks\"]\n        else:\n            video_keys = [key for key, ft in dataset.features.items() if ft[\"dtype\"] == \"video\"]\n            videos_info = [\n                {\n                    \"url\": f\"https://huggingface.co/datasets/{repo_id}/resolve/main/\"\n                    + dataset.video_path.format(\n                        episode_chunk=int(episode_id) // dataset.chunks_size,\n                        video_key=video_key,\n                        episode_index=episode_id,\n                    ),\n                    \"filename\": video_key,\n                }\n                for video_key in video_keys\n            ]\n\n            response = requests.get(\n                f\"https://huggingface.co/datasets/{repo_id}/resolve/main/meta/episodes.jsonl\"\n            )\n            response.raise_for_status()\n            # Split into lines and parse each line as JSON\n            tasks_jsonl = [json.loads(line) for line in response.text.splitlines() if line.strip()]\n\n            filtered_tasks_jsonl = [row for row in tasks_jsonl if row[\"episode_index\"] == episode_id]\n            tasks = filtered_tasks_jsonl[0][\"tasks\"]\n\n        videos_info[0][\"language_instruction\"] = tasks\n\n        if episodes is None:\n            episodes = list(\n                range(dataset.num_episodes if isinstance(dataset, LeRobotDataset) else dataset.total_episodes)\n            )\n\n        return render_template(\n            \"visualize_dataset_template.html\",\n            episode_id=episode_id,\n            episodes=episodes,\n            dataset_info=dataset_info,\n            videos_info=videos_info,\n            episode_data_csv_str=episode_data_csv_str,\n            columns=columns,\n        )\n\n    app.run(host=host, port=port)",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask, render_template, redirect, url_for, request, Response\nfrom pathlib import Path\nfrom typing import Union, List, Optional, Tuple\nfrom werkzeug.datastructures import Headers\nfrom werkzeug.wrappers import Response as WerkzeugResponse\nfrom lerobot.common.datasets.lerobot_dataset import LeRobotDataset\nfrom lerobot.common.datasets.utils import IterableNamespace\n\napp = Flask(__name__, static_folder=str(static_folder), template_folder=str(template_folder))\n\n@app.route('/')\ndef homepage():\n    if dataset is not None:\n        first_episode = episodes[0] if episodes else 0\n        return redirect(url_for('show_episode', episode_id=first_episode))\n    return render_template('dataset_selection.html')\n\n@app.route('/<dataset_namespace>/<dataset_name>/first')\ndef show_first_episode(dataset_namespace, dataset_name):\n    first_episode = episodes[0] if episodes else 0\n    return redirect(url_for('show_episode', dataset_namespace=dataset_namespace, dataset_name=dataset_name, episode_id=first_episode))\n\n@app.route('/<dataset_namespace>/<dataset_name>/<int:episode_id>')\ndef show_episode(dataset_namespace, dataset_name, episode_id):\n    if dataset is None:\n        return \"Dataset not loaded\", 404\n    \n    if episodes is not None and episode_id not in episodes:\n        return \"Episode not found\", 404\n    \n    try:\n        episode_data = dataset[episode_id]\n    except IndexError:\n        return \"Episode not found\", 404\n    \n    video_paths = []\n    if hasattr(episode_data, 'videos'):\n        for video_key in episode_data.videos.keys():\n            video_paths.append(f\"/static/videos/{episode_id}_{video_key}.mp4\")\n    \n    task_description = getattr(episode_data, 'task_description', 'No description available')\n    \n    return render_template('episode.html', \n                         episode_id=episode_id,\n                         video_paths=video_paths,\n                         task_description=task_description,\n                         prev_episode=episode_id - 1 if episode_id > 0 else None,\n                         next_episode=episode_id + 1 if (episodes is None or episode_id < max(episodes)) else None)\n\ndef run_server(\n    dataset: Union[LeRobotDataset, IterableNamespace, None] = None,\n    episodes: Optional[List[int]] = None,\n    host: str = '0.0.0.0',\n    port: str = '5000',\n    static_folder: Path = Path('static'),\n    template_folder: Path = Path('templates')\n):\n    app.config['dataset'] = dataset\n    app.config['episodes'] = episodes\n    app.run(host=host, port=port)"
  },
  {
    "Task_id": 200,
    "Instruction": "Task Description: Create a Flask-based HTTP server with multiple endpoints that return different responses, including a main API endpoint, a root endpoint, and a static file endpoint that will be overridden by Nginx.\n\nFunction Description: The code creates a Flask application with three routes:\n1. An API endpoint (\"/api\") that returns a greeting message with Python version information\n2. A root endpoint (\"/\") that returns a basic message\n3. A static file endpoint (\"/content/test.txt\") that returns a message which will be overridden by Nginx\n\nInput: None (Flask automatically handles incoming HTTP requests)\n\nOutput: \n- For \"/api\" endpoint: A string containing a greeting message with Python version\n- For \"/\" endpoint: A string \"API response overriden by Nginx\"\n- For \"/content/test.txt\" endpoint: A string \"Not run, Nginx overrides to serve static file\"",
    "Canonical_solution": "from flask import Flask\ncustom_app = Flask(__name__)\n\n@custom_app.route(\"/api\")\ndef hello():\n    version = \"{}.{}\".format(sys.version_info.major, sys.version_info.minor)\n    message = \"Hello World from Flask in a uWSGI Nginx Docker container with Python {} - testing\".format(\n        version\n    )\n    return message\n\n@custom_app.route(\"/\")\ndef main():\n    return \"API response overriden by Nginx\"\n\n@custom_app.route(\"/content/test.txt\")\ndef static_test():\n    return \"Not run, Nginx overrides to serve static file\"\n\nif __name__ == \"__main__\":\n    custom_app.run(host=\"0.0.0.0\", debug=True, port=80)",
    "Code_level": "Function-level",
    "FSMID_for_test": 60,
    "wrong_result": "from flask import Flask\nimport sys\n\napp = Flask(__name__)\n\n@app.route('/api')\ndef api():\n    return f\"Hello from Python {sys.version_info.major}.{sys.version_info.minor}\"\n\n@app.route('/')\ndef root():\n    return \"API response overriden by Nginx\"\n\n@app.route('/content/test.txt')\ndef static_content():\n    return \"Not run, Nginx overrides to serve static file\"\n\nif __name__ == '__main__':\n    app.run()"
  },
  {
    "Task_id": 208,
    "Instruction": "Task Description: Create a Python class that manages a queue-based system for processing image style transfer requests using a pre-trained PyTorch model, with asynchronous batch processing capabilities.\n\nClass Description: ModelRunner is a class that handles asynchronous processing of image style transfer requests using a PyTorch model. It manages a queue system with configurable batch processing, queue limits, and wait times to efficiently process incoming requests.\n\nAttributes:\n- model_name: str - Path to the pre-trained PyTorch model file\n- queue: list - Stores pending processing tasks\n- queue_lock: asyncio.Lock - Synchronization lock for queue access\n- model: torch.jit.ScriptModule - Loaded PyTorch model\n- needs_processing: asyncio.Event - Signals when processing is needed\n- needs_processing_timer: asyncio.TimerHandle - Timer for delayed processing\n\nMethods:\n- __init__(model_name: str) -> None - Initializes the model runner with the specified model file\n- schedule_processing_if_needed() -> None - Checks queue and triggers processing if conditions are met\n- process_input(input: torch.Tensor) -> torch.Tensor - Adds input to queue and waits for processed output\n- run_model(batch: torch.Tensor) -> torch.Tensor - Executes the model on the input batch (synchronous)\n- model_runner() -> None - Main processing loop that handles batch processing asynchronously",
    "Canonical_solution": "import sys\nimport asyncio\nimport functools\nfrom sanic import Sanic\nfrom sanic.response import raw, text\nfrom sanic.log import logger\nimport sanic\nimport torch\nimport torchvision\nimport PIL.Image\nimport io\n\nclass ModelRunner:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.queue = []\n        self.queue_lock = None\n        self.model = torch.jit.load(self.model_name, map_location=torch.device('cpu'))\n        self.needs_processing = None\n        self.needs_processing_timer = None\n\n    def schedule_processing_if_needed(self):\n        if len(self.queue) >= 2:  # MAX_BATCH_SIZE\n            logger.debug(\"next batch ready when processing a batch\")\n            self.needs_processing.set()\n        elif self.queue:\n            logger.debug(\"queue nonempty when processing a batch, setting next timer\")\n            self.needs_processing_timer = app.loop.call_at(self.queue[0][\"time\"] + 1, self.needs_processing.set)  # MAX_WAIT\n\n    async def process_input(self, input):\n        our_task = {\"done_event\": asyncio.Event(loop=app.loop),\n                    \"input\": input,\n                    \"time\": app.loop.time()}\n        async with self.queue_lock:\n            if len(self.queue) >= 3:  # MAX_QUEUE_SIZE\n                raise HandlingError(\"I'm too busy\", code=503)\n            self.queue.append(our_task)\n            logger.debug(\"enqueued task. new queue size {}\".format(len(self.queue)))\n            self.schedule_processing_if_needed()\n        await our_task[\"done_event\"].wait()\n        return our_task[\"output\"]\n\n    def run_model(self, batch):\n        return self.model(batch.to(torch.device('cpu'))).to('cpu')\n\n    async def model_runner(self):\n        self.queue_lock = asyncio.Lock(loop=app.loop)\n        self.needs_processing = asyncio.Event(loop=app.loop)\n        logger.info(\"started model runner for {}\".format(self.model_name))\n        while True:\n            await self.needs_processing.wait()\n            self.needs_processing.clear()\n            if self.needs_processing_timer is not None:\n                self.needs_processing_timer.cancel()\n                self.needs_processing_timer = None\n            async with self.queue_lock:\n                if self.queue:\n                    longest_wait = app.loop.time() - self.queue[0][\"time\"]\n                else:\n                    longest_wait = None\n                logger.debug(\"launching processing. queue size: {}. longest wait: {}\".format(len(self.queue), longest_wait))\n                to_process = self.queue[:2]  # MAX_BATCH_SIZE\n                del self.queue[:len(to_process)]\n                self.schedule_processing_if_needed()\n            batch = torch.stack([t[\"input\"] for t in to_process], dim=0)\n            result = await app.loop.run_in_executor(\n                None, functools.partial(self.run_model, batch)\n            for t, r in zip(to_process, result):\n                t[\"output\"] = r\n                t[\"done_event\"].set()\n            del to_process\n\nclass HandlingError(Exception):\n    def __init__(self, msg, code=500):\n        super().__init__()\n        self.handling_code = code\n        self.handling_msg = msg\n\napp = Sanic(__name__)\n\n@app.route('/image', methods=['PUT'], stream=True)\nasync def image(request):\n    try:\n        content_length = int(request.headers.get('content-length', '0'))\n        MAX_SIZE = 2**22\n        if content_length:\n            if content_length > MAX_SIZE:\n                raise HandlingError(\"Too large\")\n            data = bytearray(content_length)\n        else:\n            data = bytearray(MAX_SIZE)\n        pos = 0\n        while True:\n            data_part = await request.stream.read()\n            if data_part is None:\n                break\n            data[pos: len(data_part) + pos] = data_part\n            pos += len(data_part)\n            if pos > MAX_SIZE:\n                raise HandlingError(\"Too large\")\n\n        im = PIL.Image.open(io.BytesIO(data))\n        im = torchvision.transforms.functional.resize(im, (228, 228))\n        im = torchvision.transforms.functional.to_tensor(im)\n        im = im[:3]\n        if im.dim() != 3 or im.size(0) < 3 or im.size(0) > 4:\n            raise HandlingError(\"need rgb image\")\n        out_im = await style_transfer_runner.process_input(im)\n        out_im = torchvision.transforms.functional.to_pil_image(out_im)\n        imgByteArr = io.BytesIO()\n        out_im.save(imgByteArr, format='JPEG')\n        return raw(imgByteArr.getvalue(), status=200, content_type='image/jpeg')\n    except HandlingError as e:\n        return text(e.handling_msg, status=e.handling_code)\n\nstyle_transfer_runner = ModelRunner(sys.argv[1])\napp.add_task(style_transfer_runner.model_runner())",
    "Code_level": "Class-level",
    "FSMID_for_test": 62,
    "wrong_result": "import asyncio\nimport torch\nfrom typing import Optional\n\nclass ModelRunner:\n    def __init__(self, model_name: str):\n        self.model_name = model_name\n        self.queue = []\n        self.queue_lock = asyncio.Lock()\n        self.model = torch.jit.load(model_name)\n        self.needs_processing = asyncio.Event()\n        self.needs_processing_timer: Optional[asyncio.TimerHandle] = None\n        asyncio.create_task(self.model_runner())\n\n    async def schedule_processing_if_needed(self):\n        async with self.queue_lock:\n            if len(self.queue) >= 1:\n                if self.needs_processing_timer is not None:\n                    self.needs_processing_timer.cancel()\n                self.needs_processing_timer = asyncio.get_event_loop().call_later(0.1, self.needs_processing.set)\n\n    async def process_input(self, input: torch.Tensor) -> torch.Tensor:\n        future = asyncio.get_event_loop().create_future()\n        async with self.queue_lock:\n            self.queue.append((input, future))\n        await self.schedule_processing_if_needed()\n        return await future\n\n    def run_model(self, batch: torch.Tensor) -> torch.Tensor:\n        with torch.no_grad():\n            return self.model(batch)\n\n    async def model_runner(self):\n        while True:\n            await self.needs_processing.wait()\n            async with self.queue_lock:\n                if self.needs_processing_timer is not None:\n                    self.needs_processing_timer.cancel()\n                    self.needs_processing_timer = None\n                inputs = [item[0] for item in self.queue]\n                futures = [item[1] for item in self.queue]\n                self.queue = []\n                self.needs_processing.clear()\n            batch = torch.stack(inputs)\n            results = self.run_model(batch)\n            for result, future in zip(results, futures):\n                future.set_result(result)"
  },
  {
    "Task_id": 209,
    "Instruction": "Task Description: Implement an asynchronous HTTP server using Sanic framework that processes image style transfer requests, with batching and queue management for efficient model inference.\n\nClass Description: ModelRunner is a class that manages a queue of image processing tasks, batches them efficiently, and runs them through a pretrained PyTorch model for style transfer.\n\nAttributes:\n- model_name: str - Name of the pretrained model to use\n- queue: list - List of pending image processing tasks\n- queue_lock: asyncio.Lock - Lock for thread-safe queue operations\n- model: torch.nn.Module - Loaded pretrained model\n- needs_processing: asyncio.Event - Event signaling when processing is needed\n- needs_processing_timer: asyncio.TimerHandle - Timer for delayed processing\n\nMethods:\n- __init__(model_name: str) -> None - Initializes the model runner with specified model name\n- schedule_processing_if_needed() -> None - Checks queue and triggers processing if conditions are met\n- process_input(input: torch.Tensor) -> torch.Tensor - Adds input to queue and waits for processed output\n- run_model(batch: torch.Tensor) -> torch.Tensor - Runs the model on a batch of inputs (synchronous)\n- model_runner() -> None - Main processing loop that handles batching and model execution\n\nFunction Description: The image endpoint handles HTTP PUT requests containing image data, processes them through the style transfer model, and returns the transformed image.\n\nInput:\n- request: sanic.Request - HTTP request object containing image data in the body\n\nOutput:\n- sanic.response.raw - Processed image in JPEG format, or error message if processing fails",
    "Canonical_solution": "import sys\nimport asyncio\nimport functools\nfrom sanic import Sanic\nfrom sanic.response import raw\nfrom sanic.log import logger\nimport sanic\nimport torch\nimport torchvision\nimport PIL.Image\nimport io\n\nclass ModelRunner:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.queue = []\n        self.queue_lock = None\n        self.model = get_pretrained_model(self.model_name, map_location=torch.device('cpu'))\n        self.needs_processing = None\n        self.needs_processing_timer = None\n\n    def schedule_processing_if_needed(self):\n        if len(self.queue) >= 2:  # MAX_BATCH_SIZE\n            logger.debug(\"next batch ready when processing a batch\")\n            self.needs_processing.set()\n        elif self.queue:\n            logger.debug(\"queue nonempty when processing a batch, setting next timer\")\n            self.needs_processing_timer = app.loop.call_at(self.queue[0][\"time\"] + 1, self.needs_processing.set)  # MAX_WAIT\n\n    async def process_input(self, input):\n        our_task = {\n            \"done_event\": asyncio.Event(loop=app.loop),\n            \"input\": input,\n            \"time\": app.loop.time()\n        }\n        async with self.queue_lock:\n            if len(self.queue) >= 3:  # MAX_QUEUE_SIZE\n                raise HandlingError(\"I'm too busy\", code=503)\n            self.queue.append(our_task)\n            logger.debug(\"enqueued task. new queue size {}\".format(len(self.queue)))\n            self.schedule_processing_if_needed()\n\n        await our_task[\"done_event\"].wait()\n        return our_task[\"output\"]\n\n    def run_model(self, batch):\n        return self.model(batch.to(torch.device('cpu'))).to('cpu')\n\n    async def model_runner(self):\n        self.queue_lock = asyncio.Lock(loop=app.loop)\n        self.needs_processing = asyncio.Event(loop=app.loop)\n        logger.info(\"started model runner for {}\".format(self.model_name))\n        while True:\n            await self.needs_processing.wait()\n            self.needs_processing.clear()\n            if self.needs_processing_timer is not None:\n                self.needs_processing_timer.cancel()\n                self.needs_processing_timer = None\n            async with self.queue_lock:\n                if self.queue:\n                    longest_wait = app.loop.time() - self.queue[0][\"time\"]\n                else:\n                    longest_wait = None\n                logger.debug(\"launching processing. queue size: {}. longest wait: {}\".format(len(self.queue), longest_wait))\n                to_process = self.queue[:2]  # MAX_BATCH_SIZE\n                del self.queue[:len(to_process)]\n                self.schedule_processing_if_needed()\n            batch = torch.stack([t[\"input\"] for t in to_process], dim=0)\n            result = await app.loop.run_in_executor(\n                None, functools.partial(self.run_model, batch)\n            for t, r in zip(to_process, result):\n                t[\"output\"] = r\n                t[\"done_event\"].set()\n            del to_process\n\nclass HandlingError(Exception):\n    def __init__(self, msg, code=500):\n        super().__init__()\n        self.handling_code = code\n        self.handling_msg = msg\n\napp = Sanic(__name__)\n\n@app.route('/image', methods=['PUT'], stream=True)\nasync def image(request):\n    try:\n        content_length = int(request.headers.get('content-length', '0'))\n        MAX_SIZE = 2**22\n        if content_length:\n            if content_length > MAX_SIZE:\n                raise HandlingError(\"Too large\")\n            data = bytearray(content_length)\n        else:\n            data = bytearray(MAX_SIZE)\n        pos = 0\n        while True:\n            data_part = await request.stream.read()\n            if data_part is None:\n                break\n            data[pos: len(data_part) + pos] = data_part\n            pos += len(data_part)\n            if pos > MAX_SIZE:\n                raise HandlingError(\"Too large\")\n\n        im = PIL.Image.open(io.BytesIO(data))\n        im = torchvision.transforms.functional.resize(im, (228, 228))\n        im = torchvision.transforms.functional.to_tensor(im)\n        im = im[:3]\n        if im.dim() != 3 or im.size(0) < 3 or im.size(0) > 4:\n            raise HandlingError(\"need rgb image\")\n        out_im = await style_transfer_runner.process_input(im)\n        out_im = torchvision.transforms.functional.to_pil_image(out_im)\n        imgByteArr = io.BytesIO()\n        out_im.save(imgByteArr, format='JPEG')\n        return raw(imgByteArr.getvalue(), status=200, content_type='image/jpeg')\n    except HandlingError as e:\n        return sanic.response.text(e.handling_msg, status=e.handling_code)",
    "Code_level": "Class-level",
    "FSMID_for_test": 62,
    "wrong_result": "from sanic import Sanic, response\nfrom sanic.log import logger\nimport torch\nimport asyncio\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\n\nclass ModelRunner:\n    def __init__(self, model_name):\n        self.model_name = model_name\n        self.queue = []\n        self.queue_lock = asyncio.Lock()\n        self.model = torch.hub.load('pytorch/vision:v0.10.0', model_name, pretrained=True)\n        self.model.eval()\n        self.needs_processing = asyncio.Event()\n        self.needs_processing_timer = None\n\n    async def schedule_processing_if_needed(self):\n        async with self.queue_lock:\n            if len(self.queue) >= 4 or (self.queue and self.needs_processing_timer is None):\n                if self.needs_processing_timer:\n                    self.needs_processing_timer.cancel()\n                self.needs_processing.set()\n            elif self.queue and not self.needs_processing.is_set():\n                self.needs_processing_timer = asyncio.get_event_loop().call_later(\n                    0.1, lambda: self.needs_processing.set()\n                )\n\n    async def process_input(self, input_tensor):\n        async with self.queue_lock:\n            future = asyncio.get_event_loop().create_future()\n            self.queue.append((input_tensor, future))\n            await self.schedule_processing_if_needed()\n        return await future\n\n    def run_model(self, batch):\n        with torch.no_grad():\n            return self.model(batch)\n\n    async def model_runner(self):\n        while True:\n            await self.needs_processing.wait()\n            async with self.queue_lock:\n                if not self.queue:\n                    self.needs_processing.clear()\n                    continue\n                batch_tensors = []\n                futures = []\n                while self.queue and len(batch_tensors) < 8:\n                    tensor, future = self.queue.pop(0)\n                    batch_tensors.append(tensor)\n                    futures.append(future)\n                self.needs_processing.clear()\n                self.needs_processing_timer = None\n            batch = torch.stack(batch_tensors)\n            try:\n                outputs = await asyncio.get_event_loop().run_in_executor(\n                    None, self.run_model, batch\n                )\n                for output, future in zip(outputs, futures):\n                    future.set_result(output)\n            except Exception as e:\n                for future in futures:\n                    future.set_exception(e)\n\napp = Sanic(\"StyleTransferServer\")\nmodel_runner = ModelRunner(\"resnet18\")\n\n@app.before_server_start\nasync def setup_model_runner(app, loop):\n    asyncio.create_task(model_runner.model_runner())\n\n@app.put(\"/image\")\nasync def process_image(request):\n    try:\n        image_data = BytesIO(request.body)\n        image = Image.open(image_data).convert(\"RGB\")\n        image_tensor = torch.from_numpy(np.array(image)).permute(2, 0, 1).float() / 255.0\n        image_tensor = image_tensor.unsqueeze(0)\n        output_tensor = await model_runner.process_input(image_tensor)\n        output_image = (output_tensor.squeeze(0).permute(1, 2, 0).numpy() * 255).astype(np.uint8)\n        output_buffer = BytesIO()\n        Image.fromarray(output_image).save(output_buffer, format=\"JPEG\")\n        return response.raw(output_buffer.getvalue(), content_type=\"image/jpeg\")\n    except Exception as e:\n        logger.error(f\"Error processing image: {e}\")\n        return response.json({\"error\": str(e)}, status=500)"
  },
  {
    "Task_id": 210,
    "Instruction": "Task Description: Create a Python application using Sanic framework that serves a basic web page and integrates a ReactPy component. The application should have a root route serving an HTML file and a separate route prefix for ReactPy components.\n\nFunction Description: The code sets up a Sanic web server with two main functionalities: serving static content (index.html) and hosting ReactPy components. It demonstrates basic route handling and ReactPy integration with Sanic.\n\nInput: None (The functions don't take explicit parameters but rely on framework-specific objects)\n\nOutput: A running Sanic web server that responds to HTTP requests\n\nMethods:\n\nconfigure_reactpy: None -> None - Configures the ReactPy integration with Sanic, setting up the necessary routes and options for ReactPy components under the \"/_reactpy\" prefix.\n\nrun_server: None -> None - Starts the Sanic web server on localhost (127.0.0.1) port 5000.\n\nRoute Handlers:\n\nindex: (request: sanic.Request) -> sanic.response.HTTPResponse - Asynchronous route handler for the root path (\"/\") that serves the index.html file.\n\nReactPyView: None -> reactpy.types.Component - ReactPy component function that returns basic HTML content to be rendered by the ReactPy backend.",
    "Canonical_solution": "from sanic import Sanic\nfrom sanic.response import file\nfrom reactpy import component, html\nfrom reactpy.backend.sanic import Options, configure\n\napp = Sanic(\"MyApp\")\n\n@app.route(\"/\")\nasync def index(request):\n    return await file(\"index.html\")\n\n@component\ndef ReactPyView():\n    return html.code(\"This text came from an ReactPy App\")\n\ndef configure_reactpy():\n    configure(app, ReactPyView, Options(url_prefix=\"/_reactpy\"))\n\ndef run_server():\n    app.run(host=\"127.0.0.1\", port=5000)\n\nif __name__ == \"__main__\":\n    configure_reactpy()\n    run_server()",
    "Code_level": "Function-level",
    "FSMID_for_test": 62,
    "wrong_result": "from pathlib import Path\nfrom sanic import Sanic, response\nfrom sanic.response import html\nimport reactpy\nfrom reactpy.backend.sanic import configure\nfrom reactpy import component\n\napp = Sanic(\"ReactPySanicApp\")\n\n@component\ndef ReactPyView():\n    return reactpy.html.div(\n        reactpy.html.h1(\"Hello from ReactPy!\"),\n        reactpy.html.p(\"This is a ReactPy component rendered in Sanic.\")\n    )\n\nasync def index(request):\n    index_path = Path(__file__).parent / \"index.html\"\n    with open(index_path, \"r\") as f:\n        return html(f.read())\n\ndef configure_reactpy():\n    configure(app, ReactPyView, options={\"url_prefix\": \"/_reactpy\"})\n\ndef run_server():\n    app.add_route(index, \"/\")\n    configure_reactpy()\n    app.run(host=\"127.0.0.1\", port=5000)\n\nif __name__ == \"__main__\":\n    run_server()"
  },
  {
    "Task_id": 211,
    "Instruction": "Task Description: Create a Sanic-based HTTP server for handling text embedding requests using an ONNX backend, with configurable GPU usage and worker processes.\n\nFunction Description: The function `setup_embedding_server` initializes and runs a Sanic web server that provides an embedding service. It sets up a single POST endpoint `/embedding` that processes text inputs and returns their embeddings using an ONNX-based backend. The server can be configured to use GPU acceleration and multiple worker processes.\n\nInput:\n- `use_gpu`: [Boolean] - Optional flag indicating whether to use GPU acceleration (default: False)\n- `workers`: [Integer] - Optional number of worker processes to spawn (default: 1)\n\nOutput:\n- None (The function runs indefinitely as a web server)\n- The server responds to POST requests at `/embedding` with JSON-formatted embedding results",
    "Canonical_solution": "import sys\nimport os\nfrom sanic import Sanic\nfrom sanic.response import json\nfrom qanything_kernel.dependent_server.embedding_server.embedding_onnx_backend import EmbeddingOnnxBackend\nimport argparse\n\ndef setup_embedding_server(use_gpu=False, workers=1):\n    app = Sanic(\"embedding_server\")\n\n    @app.route(\"/embedding\", methods=[\"POST\"])\n    async def embedding(request):\n        data = request.json\n        texts = data.get('texts')\n        onnx_backend: EmbeddingOnnxBackend = request.app.ctx.onnx_backend\n        result_data = onnx_backend.predict(texts)\n        return json(result_data)\n\n    @app.listener('before_server_start')\n    async def setup_onnx_backend(app, loop):\n        app.ctx.onnx_backend = EmbeddingOnnxBackend(use_cpu=not use_gpu)\n\n    app.run(host=\"0.0.0.0\", port=9001, workers=workers)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--use_gpu', action=\"store_true\", help='use gpu or not')\n    parser.add_argument('--workers', type=int, default=1, help='workers')\n    args = parser.parse_args()\n    setup_embedding_server(use_gpu=args.use_gpu, workers=args.workers)",
    "Code_level": "Function-level",
    "FSMID_for_test": 62,
    "wrong_result": "from sanic import Sanic\nfrom sanic.response import json\nimport numpy as np\nimport onnxruntime as ort\nfrom typing import List, Dict, Any\n\napp = Sanic(\"EmbeddingServer\")\n\ndef initialize_onnx_session(use_gpu: bool = False):\n    providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if use_gpu else ['CPUExecutionProvider']\n    session = ort.InferenceSession(\"model.onnx\", providers=providers)\n    return session\n\n@app.post(\"/embedding\")\nasync def get_embedding(request):\n    try:\n        texts = request.json.get(\"texts\", [])\n        if not isinstance(texts, list) or not all(isinstance(text, str) for text in texts):\n            return json({\"error\": \"Invalid input format. Expected a list of strings.\"}, status=400)\n\n        inputs = {\"input\": np.array(texts)}\n        outputs = app.ctx.session.run(None, inputs)\n        embeddings = outputs[0].tolist()\n\n        return json({\"embeddings\": embeddings})\n    except Exception as e:\n        return json({\"error\": str(e)}, status=500)\n\ndef setup_embedding_server(use_gpu: bool = False, workers: int = 1):\n    app.ctx.session = initialize_onnx_session(use_gpu)\n    app.run(host=\"0.0.0.0\", port=8000, workers=workers)"
  },
  {
    "Task_id": 213,
    "Instruction": "Task Description: Create a Python class that implements an HTTP client using the Sanic framework, capable of handling GraphQL requests and standard HTTP methods (GET, POST, PATCH, PUT, DELETE). The class should support both query parameters and file uploads, and provide appropriate response handling.\n\nClass Description: The `SanicHttpClient` class is a wrapper around Sanic's ASGI client that simplifies making HTTP requests, particularly for GraphQL endpoints. It handles request construction, header management, and response formatting.\n\nAttributes:\n- app: Sanic - The Sanic application instance used for handling requests\n- graphiql: Optional[bool] - Flag to enable GraphiQL interface\n- graphql_ide: Optional[str] - The GraphQL IDE to use (default: \"graphiql\")\n- allow_queries_via_get: bool - Whether to allow GraphQL queries via GET method\n- result_override: Optional[Any] - Optional result override for testing\n- multipart_uploads_enabled: bool - Flag to enable multipart file uploads\n\nMethods:\n- __init__(graphiql: Optional[bool], graphql_ide: Optional[str], allow_queries_via_get: bool, result_override: Optional[Any], multipart_uploads_enabled: bool) -> None - Initializes the Sanic application and sets up the GraphQL route\n- _graphql_request(method: Literal[\"get\", \"post\"], query: Optional[str], variables: Optional[Dict[str, object]], files: Optional[Dict[str, BytesIO]], headers: Optional[Dict[str, str]], **kwargs: Any) -> Response - Internal method to handle GraphQL requests with proper body construction\n- request(url: str, method: Literal[\"get\", \"post\", \"patch\", \"put\", \"delete\"], headers: Optional[Dict[str, str]]) -> Response - General HTTP request handler\n- get(url: str, headers: Optional[Dict[str, str]]) -> Response - Convenience method for GET requests\n- post(url: str, data: Optional[bytes], json: Optional[Dict], headers: Optional[Dict[str, str]]) -> Response - Convenience method for POST requests with data or JSON payload",
    "Canonical_solution": "from sanic import Sanic\nfrom sanic.request import Request as SanicRequest\nfrom typing import Any, Optional, Literal, Dict\nfrom io import BytesIO\nfrom json import dumps\nfrom random import randint\n\nclass SanicHttpClient:\n    def __init__(\n        self,\n        graphiql: Optional[bool] = None,\n        graphql_ide: Optional[str] = \"graphiql\",\n        allow_queries_via_get: bool = True,\n        result_override: Optional[Any] = None,\n        multipart_uploads_enabled: bool = False,\n    ):\n        self.app = Sanic(f\"test_{int(randint(0, 1000))}\")\n        view = GraphQLView.as_view(\n            schema=schema,\n            graphiql=graphiql,\n            graphql_ide=graphql_ide,\n            allow_queries_via_get=allow_queries_via_get,\n            result_override=result_override,\n            multipart_uploads_enabled=multipart_uploads_enabled,\n        )\n        self.app.add_route(view, \"/graphql\")\n\n    async def _graphql_request(\n        self,\n        method: Literal[\"get\", \"post\"],\n        query: Optional[str] = None,\n        variables: Optional[Dict[str, object]] = None,\n        files: Optional[Dict[str, BytesIO]] = None,\n        headers: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> Response:\n        body = self._build_body(\n            query=query, variables=variables, files=files, method=method\n        )\n\n        if body:\n            if method == \"get\":\n                kwargs[\"params\"] = body\n            elif files:\n                kwargs[\"data\"] = body\n            else:\n                kwargs[\"content\"] = dumps(body)\n\n        request, response = await self.app.asgi_client.request(\n            method,\n            \"/graphql\",\n            headers=self._get_headers(method=method, headers=headers, files=files),\n            files=files,\n            **kwargs,\n        )\n\n        return Response(\n            status_code=response.status_code,\n            data=response.content,\n            headers=response.headers,\n        )\n\n    async def request(\n        self,\n        url: str,\n        method: Literal[\"get\", \"post\", \"patch\", \"put\", \"delete\"],\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Response:\n        request, response = await self.app.asgi_client.request(\n            method,\n            url,\n            headers=headers,\n        )\n\n        return Response(\n            status_code=response.status_code,\n            data=response.content,\n            headers=response.headers,\n        )\n\n    async def get(\n        self,\n        url: str,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Response:\n        return await self.request(url, \"get\", headers=headers)\n\n    async def post(\n        self,\n        url: str,\n        data: Optional[bytes] = None,\n        json: Optional[Dict] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> Response:\n        body = data or dumps(json)\n        request, response = await self.app.asgi_client.request(\n            \"post\", url, content=body, headers=headers\n        )\n\n        return Response(\n            status_code=response.status_code,\n            data=response.content,\n            headers=response.headers,\n        )",
    "Code_level": "Class-level",
    "FSMID_for_test": 62,
    "wrong_result": "from sanic import Sanic, response\nfrom sanic.response import HTTPResponse\nfrom sanic.testing import SanicTestClient\nfrom typing import Optional, Dict, Any, Literal\nfrom io import BytesIO\n\nclass SanicHttpClient:\n    def __init__(\n        self,\n        graphiql: Optional[bool] = True,\n        graphql_ide: Optional[str] = \"graphiql\",\n        allow_queries_via_get: bool = True,\n        result_override: Optional[Any] = None,\n        multipart_uploads_enabled: bool = True,\n    ) -> None:\n        self.app = Sanic(\"SanicHttpClient\")\n        self.graphiql = graphiql\n        self.graphql_ide = graphql_ide\n        self.allow_queries_via_get = allow_queries_via_get\n        self.result_override = result_override\n        self.multipart_uploads_enabled = multipart_uploads_enabled\n        self.client = SanicTestClient(self.app)\n\n    async def _graphql_request(\n        self,\n        method: Literal[\"get\", \"post\"],\n        query: Optional[str] = None,\n        variables: Optional[Dict[str, object]] = None,\n        files: Optional[Dict[str, BytesIO]] = None,\n        headers: Optional[Dict[str, str]] = None,\n        **kwargs: Any,\n    ) -> HTTPResponse:\n        if method == \"get\":\n            params = {\"query\": query}\n            if variables:\n                params[\"variables\"] = variables\n            return await self.client.get(\n                \"/graphql\",\n                params=params,\n                headers=headers,\n                **kwargs,\n            )\n        else:\n            data = {\"query\": query}\n            if variables:\n                data[\"variables\"] = variables\n            if files and self.multipart_uploads_enabled:\n                files_data = {k: (v.name, v.read()) for k, v in files.items()}\n                return await self.client.post(\n                    \"/graphql\",\n                    data=data,\n                    files=files_data,\n                    headers=headers,\n                    **kwargs,\n                )\n            return await self.client.post(\n                \"/graphql\",\n                json=data,\n                headers=headers,\n                **kwargs,\n            )\n\n    async def request(\n        self,\n        url: str,\n        method: Literal[\"get\", \"post\", \"patch\", \"put\", \"delete\"],\n        headers: Optional[Dict[str, str]] = None,\n    ) -> HTTPResponse:\n        return await getattr(self.client, method)(url, headers=headers)\n\n    async def get(\n        self,\n        url: str,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> HTTPResponse:\n        return await self.request(url, \"get\", headers=headers)\n\n    async def post(\n        self,\n        url: str,\n        data: Optional[bytes] = None,\n        json: Optional[Dict] = None,\n        headers: Optional[Dict[str, str]] = None,\n    ) -> HTTPResponse:\n        if data:\n            return await self.client.post(url, data=data, headers=headers)\n        elif json:\n            return await self.client.post(url, json=json, headers=headers)\n        return await self.client.post(url, headers=headers)"
  },
  {
    "Task_id": 216,
    "Instruction": "Task Description: Create a Sanic-based HTTP server that handles reranking requests by processing a query and a list of passages through an ONNX backend, and returns the reranked results in JSON format.\n\nFunction Description: The server consists of two main functions: a route handler for processing rerank requests and a listener for initializing the ONNX backend before server startup. The route handler accepts POST requests with JSON data containing a query and passages, processes them using the ONNX backend, and returns the reranked results. The listener sets up the ONNX backend with GPU or CPU configuration based on command-line arguments.\n\nInput:\n- For the rerank route handler:\n  - request: Sanic request object containing JSON data with:\n    - query: str - The search query to rerank against\n    - passages: List[str] - The list of passages to be reranked\n- For the setup_onnx_backend listener:\n  - app: Sanic application instance\n  - loop: Event loop (unused in this case)\n- Command-line arguments:\n  --use_gpu: bool - Flag to enable GPU acceleration\n  --workers: int - Number of worker processes\n\nOutput:\n- For the rerank route handler:\n  - Returns: JSON response containing the reranked results\n- For the server startup:\n  - Starts an HTTP server listening on port 8001 with specified number of workers",
    "Canonical_solution": "from sanic import Sanic\nfrom sanic.response import json\nimport argparse\n\napp = Sanic(\"rerank_server\")\n\n@app.route(\"/rerank\", methods=[\"POST\"])\nasync def rerank(request):\n    data = request.json\n    query = data.get('query')\n    passages = data.get('passages')\n\n    onnx_backend: RerankOnnxBackend = request.app.ctx.onnx_backend\n    result_data = onnx_backend.get_rerank(query, passages)\n\n    return json(result_data)\n\n@app.listener('before_server_start')\nasync def setup_onnx_backend(app, loop):\n    app.ctx.onnx_backend = RerankOnnxBackend(use_cpu=not args.use_gpu)\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--use_gpu', action=\"store_true\", help='use gpu or not')\n    parser.add_argument('--workers', type=int, default=1, help='workers')\n    args = parser.parse_args()\n    \n    app.run(host=\"0.0.0.0\", port=8001, workers=args.workers)",
    "Code_level": "Function-level",
    "FSMID_for_test": 62,
    "wrong_result": "from sanic import Sanic, json\nfrom sanic.worker.loader import AppLoader\nimport onnxruntime as ort\nfrom typing import List, Dict, Any\nimport numpy as np\nimport argparse\n\napp = Sanic(\"RerankServer\")\n\ndef setup_onnx_backend(use_gpu: bool):\n    providers = [\"CUDAExecutionProvider\", \"CPUExecutionProvider\"] if use_gpu else [\"CPUExecutionProvider\"]\n    sess_options = ort.SessionOptions()\n    sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL\n    session = ort.InferenceSession(\"model.onnx\", sess_options=sess_options, providers=providers)\n    return session\n\n@app.before_server_start\nasync def setup_backend(app: Sanic, _):\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--use_gpu\", action=\"store_true\", default=False)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    args = parser.parse_args()\n    app.ctx.onnx_session = setup_onnx_backend(args.use_gpu)\n\n@app.post(\"/rerank\")\nasync def rerank(request):\n    data = request.json\n    query = data[\"query\"]\n    passages = data[\"passages\"]\n\n    inputs = {\n        \"query\": np.array([query], dtype=np.str_),\n        \"passages\": np.array(passages, dtype=np.str_)\n    }\n\n    outputs = request.app.ctx.onnx_session.run(None, inputs)\n    scores = outputs[0].tolist()\n    ranked_indices = np.argsort(scores)[::-1].tolist()\n    ranked_passages = [passages[i] for i in ranked_indices]\n    ranked_scores = [scores[i] for i in ranked_indices]\n\n    return json({\n        \"passages\": ranked_passages,\n        \"scores\": ranked_scores\n    })\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"--use_gpu\", action=\"store_true\", default=False)\n    parser.add_argument(\"--workers\", type=int, default=1)\n    args = parser.parse_args()\n\n    loader = AppLoader(factory=lambda: app)\n    app.prepare(host=\"0.0.0.0\", port=8001, workers=args.workers, loader=loader)\n    Sanic.serve(primary=app, app_loader=loader)"
  },
  {
    "Task_id": 22,
    "Instruction": "Task Description: Implement a class for handling HTTP requests to a server, supporting both GET and POST methods, with optional proxy support and response parsing capabilities.\n\nClass Description: WARequest is a class designed to facilitate HTTP communication with a server. It provides methods for sending GET and POST requests, handling response parsing, and managing request parameters and headers. The class supports both direct connections and proxy connections.\n\nAttributes:\n\npvars: [list] - List of variables that can be parsed from the response\nport: [int] - Port number for the HTTP connection (default: 443)\ntype: [str] - HTTP request type (GET or POST)\nparser: [ResponseParser] - Parser object for processing the response\nparams: [list] - List of tuples containing request parameters\nheaders: [dict] - Dictionary of HTTP headers to be sent with the request\nsent: [bool] - Flag indicating if the request has been sent\nresponse: [HTTPResponse] - The response object from the server\nurl: [str] - The target URL for the request\n\nMethods:\n\nsetParsableVariables: [setParsableVariables](pvars: list) -> None - Sets the list of variables that can be parsed from the response\naddParam: [addParam](name: str, value: str) -> None - Adds a parameter to the request\nremoveParam: [removeParam](name: str) -> None - Removes a parameter from the request\naddHeaderField: [addHeaderField](name: str, value: str) -> None - Adds a header field to the request\nclearParams: [clearParams]() -> None - Clears all request parameters\ngetUserAgent: [getUserAgent]() -> str - Returns the user agent string\nsend: [send](parser: ResponseParser = None) -> dict - Sends the request (GET or POST based on type) and returns parsed response\nsetParser: [setParser](parser: ResponseParser) -> None - Sets the response parser\ngetConnectionParameters: [getConnectionParameters]() -> tuple - Returns host, port, and path from the URL\nsendGetRequest: [sendGetRequest](parser: ResponseParser = None) -> dict - Sends a GET request and returns parsed response\nsendPostRequest: [sendPostRequest](parser: ResponseParser = None) -> dict - Sends a POST request and returns parsed response\nsendRequest: [sendRequest](host: str, port: int, path: str, headers: dict, params: list, reqType: str = \"GET\") -> HTTPResponse - Static method to send a direct HTTP request\nsendRequestWithProxy: [sendRequestWithProxy](host: str, port: int, path: str, headers: dict, params: list, proxy: HttpProxy) -> dict - Static method to send a request through a proxy\nbuild_get_url: [build_get_url](host: str, path: str, params: list) -> str - Static method to build a GET URL\nbuild_headers: [build_headers](headers_tuple: dict) -> list - Static method to convert headers dictionary to curl format",
    "Canonical_solution": "import urllib\nimport sys\nimport os\nimport logging\nfrom http import client as httplib\nfrom urllib.parse import urlencode\nfrom .httpproxy import HttpProxy\nfrom yowsup.env import YowsupEnv\n\nlogger = logging.getLogger(__name__)\n\nclass WARequest:\n    OK = 200\n\n    def __init__(self):\n        self.pvars = []\n        self.port = 443\n        self.type = \"GET\"\n        self.parser = None\n        self.params = []\n        self.headers = {}\n        self.sent = False\n        self.response = None\n\n    def setParsableVariables(self, pvars):\n        self.pvars = pvars\n\n    def addParam(self, name, value):\n        self.params.append((name, value))\n\n    def removeParam(self, name):\n        for i in range(0, len(self.params)):\n            if self.params[i][0] == name:\n                del self.params[i]\n\n    def addHeaderField(self, name, value):\n        self.headers[name] = value\n\n    def clearParams(self):\n        self.params = []\n\n    def getUserAgent(self):\n        return YowsupEnv.getCurrent().getUserAgent()\n\n    def send(self, parser=None):\n        if self.type == \"POST\":\n            return self.sendPostRequest(parser)\n        return self.sendGetRequest(parser)\n\n    def setParser(self, parser):\n        if isinstance(parser, ResponseParser):\n            self.parser = parser\n        else:\n            logger.error(\"Invalid parser\")\n\n    def getConnectionParameters(self):\n        if not self.url:\n            return \"\", \"\", self.port\n\n        try:\n            url = self.url.split(\"://\", 1)\n            url = url[0] if len(url) == 1 else url[1]\n            host, path = url.split('/', 1)\n        except ValueError:\n            host = url\n            path = \"\"\n\n        path = \"/\" + path\n        return host, self.port, path\n\n    def sendGetRequest(self, parser=None):\n        self.response = None\n        params = self.params\n        parser = parser or self.parser or ResponseParser()\n        headers = dict(list({\"User-Agent\": self.getUserAgent(),\n                           \"Accept\": parser.getMeta()\n                          }.items()) + list(self.headers.items()))\n\n        host, port, path = self.getConnectionParameters()\n        proxy = HttpProxy.getFromEnviron()\n        if proxy is None:\n            self.response = WARequest.sendRequest(host, port, path, headers, params, \"GET\")\n            if not self.response.status == WARequest.OK:\n                logger.error(\"Request not success, status was %s\" % self.response.status)\n                return {}\n\n            data = self.response.read()\n            logger.info(data)\n            self.sent = True\n            return parser.parse(data.decode(), self.pvars)\n        else:\n            logger.info(\"Request with proxy\")\n            self.response = WARequest.sendRequestWithProxy(host, port, path, headers, params, proxy)\n            logger.info(self.response)\n            return self.response\n\n    def sendPostRequest(self, parser=None):\n        self.response = None\n        params = self.params\n        parser = parser or self.parser or ResponseParser()\n        headers = dict(list({\"User-Agent\": self.getUserAgent(),\n                           \"Accept\": parser.getMeta(),\n                           \"Content-Type\": \"application/x-www-form-urlencoded\"\n                          }.items()) + list(self.headers.items()))\n\n        host, port, path = self.getConnectionParameters()\n        self.response = WARequest.sendRequest(host, port, path, headers, params, \"POST\")\n\n        if not self.response.status == WARequest.OK:\n            logger.error(\"Request not success, status was %s\" % self.response.status)\n            return {}\n\n        data = self.response.read()\n        logger.info(data)\n        self.sent = True\n        return parser.parse(data.decode(), self.pvars)\n\n    @staticmethod\n    def sendRequest(host, port, path, headers, params, reqType=\"GET\"):\n        params = urlencode(params)\n        path = path + \"?\" + params if reqType == \"GET\" and params else path\n\n        if len(headers):\n            logger.debug(headers)\n        if len(params):\n            logger.debug(params)\n\n        logger.debug(\"Opening connection to %s\" % host)\n        conn = httplib.HTTPSConnection(host, port) if port == 443 else httplib.HTTPConnection(host, port)\n\n        logger.debug(\"Sending %s request to %s\" % (reqType, path))\n        conn.request(reqType, path, params, headers)\n        response = conn.getresponse()\n        return response\n\n    @staticmethod\n    def sendRequestWithProxy(host, port, path, headers, params, proxy):\n        import pycurl\n        import json\n        from io import BytesIO\n        logger.info(\"SENDING PROXY REQUEST WITH %s\" % proxy.getHost())\n        bytes_buffer = BytesIO()\n        c = pycurl.Curl()\n        c.setopt(pycurl.URL, WARequest.build_get_url(host, path, params))\n        c.setopt(pycurl.PROXY, proxy.getHost())\n        c.setopt(pycurl.PROXYPORT, proxy.getPort())\n        if proxy.getUserName() is not None:\n            c.setopt(pycurl.PROXYUSERPWD, \"%s:%s\" % (proxy.getUser(), proxy.getPassword()))\n        c.setopt(pycurl.PORT, port)\n        c.setopt(pycurl.HTTPHEADER, WARequest.build_headers(headers))\n        c.setopt(pycurl.WRITEDATA, bytes_buffer)\n        c.perform()\n        c.close()\n        data = bytes_buffer.getvalue().decode('utf-8')\n        return json.loads(data)\n\n    @staticmethod\n    def build_get_url(host, path, params):\n        params = urlencode(params)\n        url = 'https://' + host + path + \"?\" + params\n        return url\n\n    @staticmethod\n    def build_headers(headers_tuple):\n        headers_array = []\n        for idx in headers_tuple:\n            headers_array.append(idx + \":\" + headers_tuple[idx])\n        return headers_array",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import urllib.request\nimport urllib.parse\nfrom typing import Optional, List, Tuple, Dict, Union\n\nclass WARequest:\n    def __init__(self, url: str, req_type: str = \"GET\", port: int = 443):\n        self.pvars: List[str] = []\n        self.port: int = port\n        self.type: str = req_type.upper()\n        self.parser = None\n        self.params: List[Tuple[str, str]] = []\n        self.headers: Dict[str, str] = {}\n        self.sent: bool = False\n        self.response = None\n        self.url: str = url\n\n    def setParsableVariables(self, pvars: List[str]) -> None:\n        self.pvars = pvars\n\n    def addParam(self, name: str, value: str) -> None:\n        self.params.append((name, value))\n\n    def removeParam(self, name: str) -> None:\n        self.params = [(n, v) for n, v in self.params if n != name]\n\n    def addHeaderField(self, name: str, value: str) -> None:\n        self.headers[name] = value\n\n    def clearParams(self) -> None:\n        self.params = []\n\n    def getUserAgent(self) -> str:\n        return self.headers.get('User-Agent', '')\n\n    def setParser(self, parser) -> None:\n        self.parser = parser\n\n    def getConnectionParameters(self) -> Tuple[str, int, str]:\n        parsed = urllib.parse.urlparse(self.url)\n        host = parsed.netloc\n        path = parsed.path if parsed.path else '/'\n        return host, self.port, path\n\n    def send(self, parser=None) -> Dict:\n        if parser:\n            self.parser = parser\n        if self.type == \"GET\":\n            return self.sendGetRequest()\n        elif self.type == \"POST\":\n            return self.sendPostRequest()\n        else:\n            raise ValueError(\"Invalid request type\")\n\n    def sendGetRequest(self, parser=None) -> Dict:\n        if parser:\n            self.parser = parser\n        host, port, path = self.getConnectionParameters()\n        url = self.build_get_url(host, path, self.params)\n        self.response = self.sendRequest(host, port, path, self.headers, self.params, \"GET\")\n        self.sent = True\n        return self.parser.parse(self.response) if self.parser else {'response': self.response}\n\n    def sendPostRequest(self, parser=None) -> Dict:\n        if parser:\n            self.parser = parser\n        host, port, path = self.getConnectionParameters()\n        self.response = self.sendRequest(host, port, path, self.headers, self.params, \"POST\")\n        self.sent = True\n        return self.parser.parse(self.response) if self.parser else {'response': self.response}\n\n    @staticmethod\n    def sendRequest(host: str, port: int, path: str, headers: Dict[str, str], params: List[Tuple[str, str]], req_type: str = \"GET\") -> urllib.request.HTTPResponse:\n        url = f\"https://{host}:{port}{path}\"\n        data = None\n        if req_type == \"GET\":\n            url = WARequest.build_get_url(host, path, params)\n        elif req_type == \"POST\":\n            data = urllib.parse.urlencode(params).encode('utf-8')\n        \n        req = urllib.request.Request(url, data=data, headers=headers, method=req_type)\n        return urllib.request.urlopen(req)\n\n    @staticmethod\n    def sendRequestWithProxy(host: str, port: int, path: str, headers: Dict[str, str], params: List[Tuple[str, str]], proxy) -> Dict:\n        raise NotImplementedError(\"Proxy support not implemented\")\n\n    @staticmethod\n    def build_get_url(host: str, path: str, params: List[Tuple[str, str]]) -> str:\n        param_str = urllib.parse.urlencode(params)\n        return f\"https://{host}{path}?{param_str}\"\n\n    @staticmethod\n    def build_headers(headers_tuple: Dict[str, str]) -> List[str]:\n        return [f\"{k}: {v}\" for k, v in headers_tuple.items()]"
  },
  {
    "Task_id": 220,
    "Instruction": "Task Description: Create a Python class that implements an HTTP server using the `http.server` module to handle GET and POST requests, with custom response handling and error management.\n\nClass Description: The `YamlComposerHttp` class encapsulates an HTTP server that listens on a specified port and handles incoming GET and POST requests. It includes an inner class `_HttpServer` that extends `BaseHTTPRequestHandler` to define custom request handling logic.\n\nAttributes:\n\n- args: [object] - Configuration object containing server parameters, including the HTTP port to listen on.\n\nMethods:\n\n- __init__(args) -> None - Initializes the `YamlComposerHttp` instance with the provided configuration.\n  - Input: \n    - args: [object] - Configuration object containing server parameters.\n  - Output: None\n\n- run() -> None - Starts the HTTP server and begins listening for incoming requests indefinitely until interrupted.\n  - Input: None\n  - Output: None\n\nInner Class: _HttpServer(BaseHTTPRequestHandler)\n\nMethods:\n\n- _set_response(msg: str, code: int = 200) -> None - Helper method to send an HTTP response with the specified message and status code.\n  - Input:\n    - msg: [str] - The message to include in the response body.\n    - code: [int] - The HTTP status code (default: 200).\n  - Output: None\n\n- do_GET() -> None - Handles incoming GET requests. Responds with a default HTML page for the root path ('/') or an error for invalid paths.\n  - Input: None\n  - Output: None\n\n- do_POST() -> None - Handles incoming POST requests. Processes form data from '/generate' path or returns an error for invalid paths.\n  - Input: None\n  - Output: None",
    "Canonical_solution": "from http.server import BaseHTTPRequestHandler, HTTPServer\nfrom urllib.parse import parse_qs\n\nclass YamlComposerHttp:\n    def __init__(self, args):\n        self.args = args\n\n    class _HttpServer(BaseHTTPRequestHandler):\n        def _set_response(self, msg: str, code: int = 200):\n            self.send_response(code)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            self.wfile.write(msg.encode('utf-8'))\n\n        def do_GET(self):\n            if str(self.path) != '/':\n                self._set_response('<h1>\"%s\" is not a valid entrypoint</h1>' % self.path, 400)\n                return\n            self._set_response(self.default_html)\n\n        def do_POST(self):\n            if str(self.path) != '/generate':\n                self._set_response('<h1>\"%s\" is not a valid entrypoint</h1>' % self.path, 400)\n                return\n            content_length = int(self.headers['Content-Length'])\n            data = self.rfile.read(content_length)\n            data = {k: v[0] for k, v in parse_qs(data.decode('utf-8')).items()}\n            self._set_response(*parse_http_data(data, self.args))\n\n    def run(self):\n        httpd = HTTPServer(('0.0.0.0', self.args.http_port), self._HttpServer)\n        try:\n            httpd.serve_forever()\n        except KeyboardInterrupt:\n            pass\n        finally:\n            httpd.server_close()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import http.server\nimport socketserver\nimport json\nfrom urllib.parse import parse_qs\n\nclass YamlComposerHttp:\n    def __init__(self, args):\n        self.args = args\n        self.port = args.http_port\n\n    def run(self):\n        with socketserver.TCPServer((\"\", self.port), self._HttpServer) as httpd:\n            print(f\"HTTP server started on port {self.port}\")\n            try:\n                httpd.serve_forever()\n            except KeyboardInterrupt:\n                pass\n            httpd.server_close()\n            print(\"HTTP server stopped\")\n\n    class _HttpServer(http.server.BaseHTTPRequestHandler):\n        def _set_response(self, msg, code=200):\n            self.send_response(code)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n            self.wfile.write(msg.encode('utf-8'))\n\n        def do_GET(self):\n            if self.path == '/':\n                html = \"\"\"\n                <html>\n                    <body>\n                        <h1>YAML Composer</h1>\n                        <form action=\"/generate\" method=\"post\">\n                            <input type=\"text\" name=\"data\" placeholder=\"Enter data...\">\n                            <button type=\"submit\">Generate</button>\n                        </form>\n                    </body>\n                </html>\n                \"\"\"\n                self._set_response(html)\n            else:\n                self._set_response(\"Invalid path\", 404)\n\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length).decode('utf-8')\n            data = parse_qs(post_data)\n\n            if self.path == '/generate':\n                response = f\"Received data: {json.dumps(data)}\"\n                self._set_response(response)\n            else:\n                self._set_response(\"Invalid path\", 404)"
  },
  {
    "Task_id": 221,
    "Instruction": "Task Description: Create a Python class that implements an HTTP server capable of handling POST requests to receive and process file uploads with metadata, then store the files locally and register them via an API.\n\nClass Description: HTTPPostHandler is a class that sets up an HTTP server to handle file uploads via POST requests. It processes JSON payloads containing file data (base64 encoded), metadata, and stores the decoded files in a specified directory while registering them through an external API.\n\nAttributes:\n- OUTPUT_DIR: str - The base directory where uploaded files will be stored\n- port: int - The port number on which the HTTP server will listen\n\nMethods:\n- __init__(output_dir: str = '', port: int = 80) -> None - Initializes the HTTPPostHandler with optional output directory and port parameters\n- run() -> None - Starts the HTTP server and begins listening for requests indefinitely\n\nNested Class: Handler (extends BaseHTTPRequestHandler)\nMethods:\n- _set_headers() -> None - Sets the HTTP response headers for successful requests\n- do_POST() -> None - Handles POST requests, processes the incoming file data and metadata, saves the file to disk, and registers it via API",
    "Canonical_solution": "import os\nimport sys\nimport json\nimport string\nimport random\nimport base64\nimport requests\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nclass HTTPPostHandler:\n    def __init__(self, output_dir='', port=80):\n        self.OUTPUT_DIR = output_dir\n        self.port = port\n\n    class Handler(BaseHTTPRequestHandler):\n        def _set_headers(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n\n        def do_POST(self):\n            self._set_headers()\n            self.data_string = self.rfile.read(int(self.headers['Content-Length']))\n            self.send_response(200)\n            self.end_headers()\n\n            json_data = json.loads(self.data_string)\n            b64_data = json_data.get('data')\n            filetype = json_data.get('type')\n            owner = json_data.get('owner')\n            module = json_data.get('module')\n            session = json_data.get('session')\n            filename = json_data.get('filename')\n\n            data = base64.b64decode(b64_data)\n            if session.startswith('_b64'):\n                session = base64.b64decode(session[6:]).decode('ascii')\n\n            if not filetype.startswith('.'):\n                filetype = '.' + filetype\n\n            if not filename:\n                filename = str().join([random.choice(string.ascii_lowercase + string.digits) for _ in range(3)]) + filetype\n\n            output_path = os.path.join(self.OUTPUT_DIR, owner, 'files', filename)\n\n            requests.post(\"http://0.0.0.0/api/file/add\", {\"filename\": filename, \"owner\": owner, \"module\": module, \"session\": session})\n\n            with open(output_path, 'wb') as fp:\n                fp.write(data)\n\n    def run(self):\n        httpd = HTTPServer(('0.0.0.0', self.port), self.Handler)\n        httpd.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import json\nimport base64\nimport os\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nclass HTTPPostHandler:\n    def __init__(self, output_dir='', port=80):\n        self.OUTPUT_DIR = output_dir if output_dir else os.path.join(os.getcwd(), 'uploads')\n        self.port = port\n        if not os.path.exists(self.OUTPUT_DIR):\n            os.makedirs(self.OUTPUT_DIR)\n\n    def run(self):\n        server_address = ('', self.port)\n        httpd = HTTPServer(server_address, self.Handler)\n        httpd.RequestHandlerClass.OUTPUT_DIR = self.OUTPUT_DIR\n        httpd.serve_forever()\n\n    class Handler(BaseHTTPRequestHandler):\n        def _set_headers(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'application/json')\n            self.end_headers()\n\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            try:\n                data = json.loads(post_data.decode('utf-8'))\n                file_data = base64.b64decode(data['file'])\n                filename = data['metadata']['filename']\n                filepath = os.path.join(self.OUTPUT_DIR, filename)\n                \n                with open(filepath, 'wb') as f:\n                    f.write(file_data)\n                \n                self._register_file(filepath, data['metadata'])\n                response = {'status': 'success', 'message': 'File uploaded successfully'}\n                self._set_headers()\n                self.wfile.write(json.dumps(response).encode('utf-8'))\n            except Exception as e:\n                self.send_error(400, str(e))\n\n        def _register_file(self, filepath, metadata):\n            pass"
  },
  {
    "Task_id": 222,
    "Instruction": "Task Description: Create a Python class that implements a simple HTTP server using the http.server module, capable of handling GET requests with customizable response status codes and running in a separate thread.\n\nClass Description: The HttpServer class provides a lightweight HTTP server implementation that can be started and stopped programmatically. It handles GET requests, supports custom response status codes via URL paths, and runs in a daemon thread for non-blocking operation.\n\nAttributes:\n\nDEFAULT_RESPONSE: bytes - Default response body sent for successful requests (default: b\"Hello!\")\nserver: HTTPServer - Instance of the HTTP server\nserver_thread: Thread - Thread running the server's serve_forever loop\n\nMethods:\n\n__init__: () -> None - Initializes the HttpServer instance with server and server_thread set to None\ncreate_server: () -> HTTPServer - Creates and returns an HTTPServer instance bound to localhost on an available port\nrun_server: () -> Tuple[Thread, HTTPServer] - Starts the server in a daemon thread and returns both the thread and server objects\nstart: () -> None - Convenience method to start the server (calls run_server and stores the results)\nshutdown: () -> None - Stops the server and joins the server thread if they exist\n\nNested Class: Handler(BaseHTTPRequestHandler) - Custom request handler for processing HTTP requests\n\nHandler Attributes:\nprotocol_version: str - HTTP protocol version (default: \"HTTP/1.1\")\ntimeout: int - Connection timeout in seconds (default: 3)\nSTATUS_RE: Pattern - Regular expression for matching status code paths\n\nHandler Methods:\ndo_GET: () -> None - Handles GET requests, supports /status/{code} paths to return specific status codes",
    "Canonical_solution": "import re\nfrom http import HTTPStatus\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom threading import Thread\n\nclass HttpServer:\n    DEFAULT_RESPONSE = b\"Hello!\"\n\n    class Handler(BaseHTTPRequestHandler):\n        protocol_version = \"HTTP/1.1\"\n        timeout = 3\n\n        STATUS_RE = re.compile(r\"/status/(\\d+)\")\n\n        def do_GET(self):\n            status_match = self.STATUS_RE.fullmatch(self.path)\n            status = 200\n            if status_match:\n                status = int(status_match.group(1))\n            if status == 200:\n                body = HttpServer.DEFAULT_RESPONSE\n                self.send_response(HTTPStatus.OK)\n                self.send_header(\"Content-Length\", str(len(body)))\n                self.end_headers()\n                self.wfile.write(body)\n            else:\n                self.send_error(status)\n\n    def __init__(self):\n        self.server = None\n        self.server_thread = None\n\n    def create_server(self):\n        server_address = (\"127.0.0.1\", 0)\n        return HTTPServer(server_address, self.Handler)\n\n    def run_server(self):\n        self.server = self.create_server()\n        self.server_thread = Thread(\n            target=self.server.serve_forever, daemon=True, name=\"Test server worker\"\n        )\n        self.server_thread.start()\n        return self.server_thread, self.server\n\n    def start(self):\n        self.server_thread, self.server = self.run_server()\n\n    def shutdown(self):\n        if self.server:\n            self.server.shutdown()\n        if self.server_thread:\n            self.server_thread.join()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import re\nimport threading\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom typing import Tuple, Optional, Pattern\n\nclass HttpServer:\n    DEFAULT_RESPONSE = b\"Hello!\"\n\n    def __init__(self) -> None:\n        self.server: Optional[HTTPServer] = None\n        self.server_thread: Optional[threading.Thread] = None\n\n    def create_server(self) -> HTTPServer:\n        server_address = ('localhost', 0)\n        self.server = HTTPServer(server_address, self.Handler)\n        return self.server\n\n    def run_server(self) -> Tuple[threading.Thread, HTTPServer]:\n        if not self.server:\n            self.create_server()\n        self.server_thread = threading.Thread(target=self.server.serve_forever)\n        self.server_thread.daemon = True\n        self.server_thread.start()\n        return self.server_thread, self.server\n\n    def start(self) -> None:\n        self.server_thread, self.server = self.run_server()\n\n    def shutdown(self) -> None:\n        if self.server:\n            self.server.shutdown()\n            self.server.server_close()\n            if self.server_thread:\n                self.server_thread.join()\n            self.server = None\n            self.server_thread = None\n\n    class Handler(BaseHTTPRequestHandler):\n        protocol_version = \"HTTP/1.1\"\n        timeout = 3\n        STATUS_RE = re.compile(r'^/status/(\\d{3})$')\n\n        def do_GET(self) -> None:\n            status_match = self.STATUS_RE.match(self.path)\n            if status_match:\n                status_code = int(status_match.group(1))\n                self.send_response(status_code)\n                self.end_headers()\n                self.wfile.write(HttpServer.DEFAULT_RESPONSE)\n            else:\n                self.send_response(200)\n                self.end_headers()\n                self.wfile.write(HttpServer.DEFAULT_RESPONSE)"
  },
  {
    "Task_id": 223,
    "Instruction": "Task Description: Create a Python class that implements a simple HTTP server to serve static files (CSS, JavaScript, and images) with proper content types and error handling.\n\nClass Description: PVEDiscordDarkServer is a class that creates an HTTP server to serve static web resources including CSS files, JavaScript files, and various image formats (PNG, JPG, JPEG, SVG). It handles different file types with appropriate content headers and provides error responses for missing files or unsupported requests.\n\nAttributes:\nPORT: int - The port number on which the HTTP server will run\nDIR_SASS: str - The directory path containing CSS files\nDIR_IMAGES: str - The directory path containing image files\nDIR_JS: str - The directory path containing JavaScript files\n\nMethods:\n__init__(port=3000) -> None - Initializes the server with the specified port (default: 3000) and sets up directory paths for static files\nrun() -> None - Starts the HTTP server and begins serving requests indefinitely until interrupted\nRequestHandler: class - Inner class that handles HTTP requests (inherits from BaseHTTPRequestHandler)\n    __init__(*args, **kwargs) -> None - Initializes the request handler with directory paths\n    log_message(format, *args) -> None - Overrides default logging to suppress log messages\n    _set_headers(status, type) -> None - Helper method to set HTTP response headers\n    do_GET() -> None - Handles GET requests, serves appropriate files with correct content types, or returns error responses",
    "Canonical_solution": "from http.server import HTTPServer, BaseHTTPRequestHandler\nimport json\nimport os\n\nclass PVEDiscordDarkServer:\n    def __init__(self, port=3000):\n        self.PORT = port\n        self.DIR_SASS = os.path.join(os.path.dirname(__file__), \"sass\")\n        self.DIR_IMAGES = os.path.join(os.path.dirname(__file__), \"images\")\n        self.DIR_JS = os.path.join(os.path.dirname(__file__), \"js\")\n\n    class RequestHandler(BaseHTTPRequestHandler):\n        def __init__(self, *args, **kwargs):\n            self.DIR_SASS = kwargs.pop('DIR_SASS')\n            self.DIR_IMAGES = kwargs.pop('DIR_IMAGES')\n            self.DIR_JS = kwargs.pop('DIR_JS')\n            super().__init__(*args, **kwargs)\n\n        def log_message(self, format, *args):\n            return\n\n        def _set_headers(self, status, type):\n            self.send_response(status)\n            self.send_header(\"Content-type\", type)\n            self.end_headers()\n\n        def do_GET(self):\n            status = 200\n            type = \"application/json\"\n            data = None\n\n            file = self.path.rpartition(\"/\")[2]\n            ext = file.rpartition(\".\")[2]\n\n            if ext == \"css\":\n                data = open(os.path.join(self.DIR_SASS, \"PVEDiscordDark.css\"), \"rb\").read()\n                type = \"text/css\"\n            elif ext == \"js\":\n                data = open(os.path.join(self.DIR_JS, \"PVEDiscordDark.js\"), \"rb\").read()\n                type = \"application/javascript\"\n            elif ext in (\"png\", \"jpg\", \"jpeg\"):\n                try:\n                    data = open(os.path.join(self.DIR_IMAGES, file), \"rb\").read()\n                    type = f\"image/{ext}\"\n                except FileNotFoundError:\n                    status = 404\n            elif ext == \"svg\":\n                try:\n                    data = open(os.path.join(self.DIR_IMAGES, file), \"rb\").read()\n                    type = f\"image/svg+xml\"\n                except FileNotFoundError:\n                    status = 404\n            else:\n                status = 400\n            self._set_headers(status, type)\n            if status == 200:\n                self.wfile.write(data)\n            else:\n                self.wfile.write(json.dumps({\"error\": status}).encode())\n\n    def run(self):\n        print(f\"Serving on localhost:{self.PORT}\")\n        server = HTTPServer(\n            server_address=(\"\", self.PORT),\n            RequestHandlerClass=lambda *args, **kwargs: self.RequestHandler(\n                *args,\n                DIR_SASS=self.DIR_SASS,\n                DIR_IMAGES=self.DIR_IMAGES,\n                DIR_JS=self.DIR_JS,\n                **kwargs\n            )\n        )\n        try:\n            server.serve_forever()\n        except KeyboardInterrupt:\n            quit()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import os\nimport mimetypes\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\n\nclass PVEDiscordDarkServer:\n    PORT = 3000\n    DIR_SASS = os.path.join(os.path.dirname(__file__), 'sass')\n    DIR_IMAGES = os.path.join(os.path.dirname(__file__), 'images')\n    DIR_JS = os.path.join(os.path.dirname(__file__), 'js')\n\n    def __init__(self, port=3000):\n        self.PORT = port\n        self.server = HTTPServer(('', self.PORT), self.RequestHandler)\n\n    def run(self):\n        try:\n            self.server.serve_forever()\n        except KeyboardInterrupt:\n            self.server.server_close()\n\n    class RequestHandler(BaseHTTPRequestHandler):\n        def __init__(self, *args, **kwargs):\n            self.DIR_SASS = PVEDiscordDarkServer.DIR_SASS\n            self.DIR_IMAGES = PVEDiscordDarkServer.DIR_IMAGES\n            self.DIR_JS = PVEDiscordDarkServer.DIR_JS\n            super().__init__(*args, **kwargs)\n\n        def log_message(self, format, *args):\n            pass\n\n        def _set_headers(self, status=200, content_type='text/html'):\n            self.send_response(status)\n            self.send_header('Content-type', content_type)\n            self.end_headers()\n\n        def do_GET(self):\n            path = self.path.split('?')[0]\n            if path == '/':\n                self._set_headers(404)\n                self.wfile.write(b'404 Not Found')\n                return\n\n            file_path = None\n            if path.startswith('/sass/'):\n                file_path = os.path.join(self.DIR_SASS, path[6:])\n            elif path.startswith('/images/'):\n                file_path = os.path.join(self.DIR_IMAGES, path[8:])\n            elif path.startswith('/js/'):\n                file_path = os.path.join(self.DIR_JS, path[4:])\n\n            if not file_path or not os.path.exists(file_path):\n                self._set_headers(404)\n                self.wfile.write(b'404 Not Found')\n                return\n\n            mime_type, _ = mimetypes.guess_type(file_path)\n            if not mime_type:\n                mime_type = 'application/octet-stream'\n\n            try:\n                with open(file_path, 'rb') as file:\n                    self._set_headers(200, mime_type)\n                    self.wfile.write(file.read())\n            except Exception:\n                self._set_headers(500)\n                self.wfile.write(b'500 Internal Server Error')"
  },
  {
    "Task_id": 224,
    "Instruction": "Task Description: Create a Python class that implements an HTTP server for sharing and viewing log files from a specified directory. The server should handle GET requests to display available log files and serve their contents.\n\nClass Description: The LogServer class creates an HTTP server that serves log files from a specified directory. It uses a custom request handler (LogSharedHandler) to process HTTP requests and serve either a directory listing or file contents.\n\nAttributes:\n- log_path: str - The directory path where log files are stored\n- port: int - The port number on which the HTTP server will listen\n- httpd: Optional[HTTPServer] - The HTTP server instance (None until started)\n\nMethods:\n- __init__(log_path: str, port: int) -> None - Initializes the LogServer with the specified log directory path and port number\n- start() -> None - Starts the HTTP server on 0.0.0.0 (all interfaces) using the specified port and serves requests indefinitely\n\nThe LogSharedHandler (inner class) handles HTTP requests with these methods:\n- do_GET() -> None - Processes GET requests:\n  - For root path (\"/\"), returns an HTML page listing all available log files as clickable links\n  - For specific file paths, returns the content of the requested log file\n  - Returns 404 if requested file doesn't exist",
    "Canonical_solution": "from http.server import BaseHTTPRequestHandler, HTTPServer\nimport os\nfrom typing import Optional\n\nclass LogSharedHandler(BaseHTTPRequestHandler):\n    def do_GET(self):\n        log_path = os.getenv('LOG_PATH')\n        if self.path == \"/\":\n            html = \"<html><body>\"\n            for f in os.listdir(log_path):\n                html += f\"<p><a href=\\\"{f}\\\">{f}</a></p>\"\n            html += \"</body></html>\"\n\n            self.send_response(200)\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(html.encode())\n        else:\n            p = log_path + self.path\n            try:\n                with open(p, \"rb\") as file:\n                    self.send_response(200)\n                    self.send_header(\"Content-type\", \"text/plain\")\n                    self.end_headers()\n                    self.wfile.write(file.read())\n            except FileNotFoundError:\n                self.send_error(404, \"File not found\")\n\nclass LogServer:\n    def __init__(self, log_path: str, port: int):\n        self.log_path = log_path\n        self.port = port\n        self.httpd: Optional[HTTPServer] = None\n\n    def start(self):\n        self.httpd = HTTPServer(('0.0.0.0', self.port), LogSharedHandler)\n        self.httpd.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import os\nimport socket\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom typing import Optional\n\nclass LogServer:\n    def __init__(self, log_path: str, port: int):\n        self.log_path = log_path\n        self.port = port\n        self.httpd: Optional[HTTPServer] = None\n\n    def start(self):\n        server_address = ('0.0.0.0', self.port)\n        self.httpd = HTTPServer(server_address, self._LogSharedHandler)\n        self.httpd.serve_forever()\n\n    class _LogSharedHandler(BaseHTTPRequestHandler):\n        def do_GET(self):\n            log_path = self.server.log_path\n            if self.path == '/':\n                files = os.listdir(log_path)\n                files_html = ''.join(f'<li><a href=\"/{f}\">{f}</a></li>' for f in files)\n                response = f'''\n                <html>\n                    <head>\n                        <title>Log Files</title>\n                    </head>\n                    <body>\n                        <h1>Available Log Files</h1>\n                        <ul>{files_html}</ul>\n                    </body>\n                </html>\n                '''\n                self.send_response(200)\n                self.send_header('Content-type', 'text/html')\n                self.end_headers()\n                self.wfile.write(response.encode('utf-8'))\n            else:\n                file_path = os.path.join(log_path, self.path[1:])\n                if os.path.isfile(file_path):\n                    with open(file_path, 'rb') as f:\n                        content = f.read()\n                    self.send_response(200)\n                    self.send_header('Content-type', 'text/plain')\n                    self.end_headers()\n                    self.wfile.write(content)\n                else:\n                    self.send_response(404)\n                    self.send_header('Content-type', 'text/plain')\n                    self.end_headers()\n                    self.wfile.write(b'404 Not Found')"
  },
  {
    "Task_id": 226,
    "Instruction": "Task Description: Create a Python class that implements an HTTP server for resolving secret hashes to their corresponding secrets, specifically designed for XUD (Cross-chain Unidirectional Payment Channel) resolution.\n\nClass Description: The XUDResolver class is an HTTP server that handles POST requests containing secret hashes and responds with the corresponding secret if the hash matches a predefined secret. It uses Python's http.server module to create a simple HTTP server.\n\nAttributes:\n- host: str - The host address where the server will run (default: \"localhost\")\n- port: int - The port number for the server (default: 8000)\n- x_secret: str - A predefined secret in hexadecimal format\n- x_secret_hash: str - The SHA-256 hash of the predefined secret\n\nMethods:\n- __init__(host: str = \"localhost\", port: int = 8000) -> None - Initializes the XUDResolver with optional host and port parameters\n- resolve(request: dict) -> dict or None - Takes a request dictionary containing a \"secrethash\" and returns the corresponding secret if the hash matches, otherwise returns None\n- serve() -> None - Starts the HTTP server with a custom request handler that processes POST requests for secret resolution",
    "Canonical_solution": "import json\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom hashlib import sha256\nfrom eth_utils import to_bytes, to_hex\n\nclass XUDResolver:\n    def __init__(self, host=\"localhost\", port=8000):\n        self.host = host\n        self.port = port\n        self.x_secret = \"0x2ff886d47b156de00d4cad5d8c332706692b5b572adfe35e6d2f65e92906806e\"\n        self.x_secret_hash = to_hex(sha256(to_bytes(hexstr=self.x_secret)).digest())\n\n    def resolve(self, request):\n        if \"secrethash\" not in request:\n            return None\n\n        if request[\"secrethash\"] == self.x_secret_hash:\n            return {\"secret\": self.x_secret}\n        return None\n\n    def serve(self):\n        class SimpleHTTPRequestHandler(BaseHTTPRequestHandler):\n            def __init__(self, *args, **kwargs):\n                self.resolver = kwargs.pop('resolver')\n                super().__init__(*args, **kwargs)\n\n            def do_POST(self):\n                try:\n                    content_len = int(self.headers.get(\"Content-Length\"))\n                    body = self.rfile.read(content_len)\n\n                    preimage = self.resolver.resolve(json.loads(body.decode(\"utf8\")))\n                    if preimage is None:\n                        self.send_response(404)\n                        self.end_headers()\n                    else:\n                        response = to_bytes(text=json.dumps(preimage))\n                        self.send_response(200)\n                        self.end_headers()\n                        self.wfile.write(response)\n                except BaseException:\n                    self.send_response(400)\n                    self.end_headers()\n\n        httpd = HTTPServer((self.host, self.port), lambda *args, **kwargs: SimpleHTTPRequestHandler(*args, resolver=self, **kwargs))\n        httpd.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import http.server\nimport json\nimport hashlib\nfrom socketserver import ThreadingMixIn\n\nclass ThreadedHTTPServer(ThreadingMixIn, http.server.HTTPServer):\n    pass\n\nclass XUDResolver:\n    def __init__(self, host=\"localhost\", port=8000):\n        self.host = host\n        self.port = port\n        self.x_secret = \"a1b2c3d4e5f67890\"\n        self.x_secret_hash = hashlib.sha256(self.x_secret.encode()).hexdigest()\n\n    def resolve(self, request):\n        if \"secrethash\" in request and request[\"secrethash\"] == self.x_secret_hash:\n            return {\"secret\": self.x_secret}\n        return None\n\n    def serve(self):\n        class RequestHandler(http.server.BaseHTTPRequestHandler):\n            def __init__(self, *args, **kwargs):\n                self.resolver = kwargs.pop('resolver')\n                super().__init__(*args, **kwargs)\n\n            def do_POST(self):\n                content_length = int(self.headers['Content-Length'])\n                post_data = self.rfile.read(content_length)\n                try:\n                    request = json.loads(post_data.decode('utf-8'))\n                    response = self.resolver.resolve(request)\n                    self.send_response(200)\n                    self.send_header('Content-type', 'application/json')\n                    self.end_headers()\n                    if response:\n                        self.wfile.write(json.dumps(response).encode('utf-8'))\n                    else:\n                        self.wfile.write(json.dumps({\"error\": \"Invalid secret hash\"}).encode('utf-8'))\n                except Exception as e:\n                    self.send_response(400)\n                    self.send_header('Content-type', 'application/json')\n                    self.end_headers()\n                    self.wfile.write(json.dumps({\"error\": str(e)}).encode('utf-8'))\n\n        server = ThreadedHTTPServer((self.host, self.port), lambda *args, **kwargs: RequestHandler(*args, resolver=self, **kwargs))\n        print(f\"Server started at http://{self.host}:{self.port}\")\n        server.serve_forever()"
  },
  {
    "Task_id": 227,
    "Instruction": "Task Description: Create a Python class that implements an HTTP server to serve static files and render dynamic content from a profile file, with optional browser auto-launch functionality.\n\nClass Description: ProfileHTTPServer is a class that creates and manages an HTTP server to serve both static files from a 'web' directory and dynamically rendered content from profile data. It handles port selection, profile data reading, and optional browser launching.\n\nAttributes:\nprof_filename: str - The filename of the profile to be read and served\nstart_browser: bool - Flag indicating whether to automatically open a browser tab\nport: Optional[int] - The port number to use for the server (None for auto-selection)\ndata: Any - The profile data read from the file\n\nMethods:\n__init__(prof_filename: str, start_browser: bool = False, port: Optional[int] = None) -> None - Initializes the server with profile filename, browser flag, and optional port\n_read_profile() -> Any - Reads and returns profile data from the specified file\n_is_port_in_use(port: int) -> bool - Checks if a port is already in use\nstart() -> None - Starts the HTTP server on the selected port, optionally launching a browser\n\nStaticServer (nested class):\n__init__(data: Any, prof_filename: str, *args, **kwargs) -> None - Initializes the request handler with profile data and filename\ndo_GET() -> None - Handles GET requests, serving either static files or rendered HTML\n_render() -> str - Renders the HTML template with profile data and version information",
    "Canonical_solution": "import html\nimport json\nimport mimetypes\nimport socket\nimport threading\nimport webbrowser\nfrom http.server import BaseHTTPRequestHandler, HTTPServer\nfrom pathlib import Path\nfrom typing import Optional\n\nclass StaticServer(BaseHTTPRequestHandler):\n    def __init__(self, data, prof_filename, *args, **kwargs):\n        self.data = data\n        self.prof_filename = prof_filename\n        super().__init__(*args, **kwargs)\n\n    def do_GET(self):\n        self.send_response(200)\n\n        if self.path == \"/\":\n            self.send_header(\"Content-type\", \"text/html\")\n            self.end_headers()\n            self.wfile.write(self._render().encode())\n        else:\n            this_dir = Path(__file__).resolve().parent\n            filepath = this_dir / \"web\" / self.path[1:]\n\n            mimetype, _ = mimetypes.guess_type(str(filepath))\n            assert mimetype is not None\n            self.send_header(\"Content-type\", mimetype)\n            self.end_headers()\n\n            with open(filepath, \"rb\") as fh:\n                content = fh.read()\n            self.wfile.write(content)\n\n    def _render(self):\n        this_dir = Path(__file__).resolve().parent\n        with open(this_dir / \"web\" / \"index.html\", encoding=\"utf-8\") as _file:\n            template = string.Template(_file.read())\n\n        return template.substitute(\n            data=html.escape(json.dumps(self.data).replace(\"</\", \"<\\\\/\")),\n            version=html.escape(__version__),\n            filename=html.escape(self.prof_filename.replace(\"</\", \"<\\\\/\")),\n        )\n\nclass ProfileHTTPServer:\n    def __init__(self, prof_filename: str, start_browser: bool = False, port: Optional[int] = None):\n        self.prof_filename = prof_filename\n        self.start_browser = start_browser\n        self.port = port\n        self.data = self._read_profile()\n\n    def _read_profile(self):\n        try:\n            return read_import_profile(self.prof_filename)\n        except (TunaError, StopIteration):\n            pass\n        return read_runtime_profile(self.prof_filename)\n\n    def _is_port_in_use(self, port: int) -> bool:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            return s.connect_ex((\"localhost\", port)) == 0\n\n    def start(self):\n        if self.port is None:\n            self.port = 8000\n            while self._is_port_in_use(self.port):\n                self.port += 1\n\n        handler = lambda *args: StaticServer(self.data, self.prof_filename, *args)\n        httpd = HTTPServer((\"\", self.port), handler)\n\n        if self.start_browser:\n            address = f\"http://localhost:{self.port}\"\n            threading.Thread(target=lambda: webbrowser.open_new_tab(address)).start()\n\n        print(f\"Starting httpd on port {self.port}\")\n        httpd.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import http.server\nimport socketserver\nimport os\nimport json\nimport socket\nimport webbrowser\nfrom typing import Any, Optional\n\nclass ProfileHTTPServer:\n    def __init__(self, prof_filename: str, start_browser: bool = False, port: Optional[int] = None) -> None:\n        self.prof_filename = prof_filename\n        self.start_browser = start_browser\n        self.port = port\n        self.data = self._read_profile()\n\n    def _read_profile(self) -> Any:\n        with open(self.prof_filename, 'r') as f:\n            return json.load(f)\n\n    def _is_port_in_use(self, port: int) -> bool:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            return s.connect_ex(('localhost', port)) == 0\n\n    def start(self) -> None:\n        if self.port is None:\n            self.port = 8000\n            while self._is_port_in_use(self.port):\n                self.port += 1\n\n        handler = self.StaticServer\n        handler.data = self.data\n        handler.prof_filename = self.prof_filename\n\n        with socketserver.TCPServer((\"\", self.port), handler) as httpd:\n            if self.start_browser:\n                webbrowser.open_new_tab(f'http://localhost:{self.port}')\n            httpd.serve_forever()\n\n    class StaticServer(http.server.SimpleHTTPRequestHandler):\n        data: Any = None\n        prof_filename: str = \"\"\n\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, directory='web', **kwargs)\n\n        def do_GET(self):\n            if self.path == '/profile':\n                self.send_response(200)\n                self.send_header('Content-type', 'text/html')\n                self.end_headers()\n                self.wfile.write(self._render().encode('utf-8'))\n            else:\n                super().do_GET()\n\n        def _render(self) -> str:\n            version = os.path.getmtime(self.prof_filename)\n            return f\"\"\"\n            <html>\n                <head>\n                    <title>Profile</title>\n                </head>\n                <body>\n                    <h1>Profile Data</h1>\n                    <pre>{json.dumps(self.data, indent=2)}</pre>\n                    <p>Version: {version}</p>\n                </body>\n            </html>\n            \"\"\""
  },
  {
    "Task_id": 229,
    "Instruction": "Task Description: Create a simple HTTP server in Python that can handle GET, HEAD, and POST requests, with customizable port configuration.\n\nClass Description: SimpleHTTPServer is a class that implements a basic HTTP server capable of processing GET, HEAD, and POST requests. It includes an inner RequestHandler class that manages the actual request processing and response generation.\n\nAttributes:\n- port: int - The port number on which the server will listen (default: 8080)\n- server_address: tuple - A tuple containing the server address and port (default: ('', port))\n- handler_class: class - The request handler class that processes HTTP requests\n\nMethods:\n- __init__(port: int = 8080) -> None - Initializes the server with the specified port\n- run() -> None - Starts the HTTP server and begins serving requests indefinitely\n- RequestHandler._set_headers() -> None - Sets basic HTTP response headers (200 status and text/html content type)\n- RequestHandler.do_GET() -> None - Handles HTTP GET requests\n- RequestHandler.do_HEAD() -> None - Handles HTTP HEAD requests\n- RequestHandler.do_POST() -> None - Handles HTTP POST requests, reading and printing the posted data",
    "Canonical_solution": "from http.server import BaseHTTPRequestHandler, HTTPServer\n\nclass SimpleHTTPServer:\n    def __init__(self, port=8080):\n        self.port = port\n        self.server_address = ('', self.port)\n        self.handler_class = self.RequestHandler\n\n    class RequestHandler(BaseHTTPRequestHandler):\n        def _set_headers(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n\n        def do_GET(self):\n            self._set_headers()\n\n        def do_HEAD(self):\n            self._set_headers()\n\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            self._set_headers()\n            print(post_data)\n\n    def run(self):\n        httpd = HTTPServer(self.server_address, self.handler_class)\n        print('Starting httpd...')\n        httpd.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import http.server\nimport socketserver\n\nclass SimpleHTTPServer:\n    def __init__(self, port=8080):\n        self.port = port\n        self.server_address = ('', self.port)\n        self.handler_class = self.RequestHandler\n\n    def run(self):\n        with socketserver.TCPServer(self.server_address, self.handler_class) as httpd:\n            print(f\"Serving on port {self.port}\")\n            httpd.serve_forever()\n\n    class RequestHandler(http.server.BaseHTTPRequestHandler):\n        def _set_headers(self):\n            self.send_response(200)\n            self.send_header('Content-type', 'text/html')\n            self.end_headers()\n\n        def do_GET(self):\n            self._set_headers()\n            self.wfile.write(b\"<html><body><h1>GET request received</h1></body></html>\")\n\n        def do_HEAD(self):\n            self._set_headers()\n\n        def do_POST(self):\n            content_length = int(self.headers['Content-Length'])\n            post_data = self.rfile.read(content_length)\n            print(f\"Received POST data: {post_data.decode()}\")\n            self._set_headers()\n            self.wfile.write(b\"<html><body><h1>POST request received</h1></body></html>\")"
  },
  {
    "Task_id": 23,
    "Instruction": "Task Description: Implement an HTTP connection pool class to manage and reuse HTTP connections efficiently, reducing the overhead of establishing new connections for each request.\n\nClass Description: HTTPConnectionPool is a connection pooling mechanism for HTTP clients that maintains a pool of connections to a specific host, allowing for connection reuse and efficient resource management.\n\nAttributes:\n- host: str - The target host for the connections\n- port: int | None - The target port (defaults to None for standard ports)\n- timeout: Timeout | float | None - Connection and read timeout settings\n- maxsize: int - Maximum number of connections in the pool (default 1)\n- block: bool - Whether to block when pool is full (default False)\n- headers: typing.Mapping[str, str] | None - Default headers for connections\n- retries: Retry | bool | int | None - Retry configuration for failed requests\n- proxy: Url | None - Proxy server configuration\n- proxy_headers: typing.Mapping[str, str] | None - Headers for proxy connections\n- pool: queue.LifoQueue - The connection pool storage\n- num_connections: int - Count of total connections created\n- num_requests: int - Count of total requests made\n- conn_kw: typing.Any - Additional connection parameters\n\nMethods:\n- __init__(host: str, port: int | None = None, timeout: Timeout | float | None = None, maxsize: int = 1, block: bool = False, headers: typing.Mapping[str, str] | None = None, retries: Retry | bool | int | None = None, _proxy: Url | None = None, _proxy_headers: typing.Mapping[str, str] | None = None, **conn_kw: typing.Any) -> None - Initializes the connection pool with given parameters\n- _new_conn() -> HTTPConnection - Creates and returns a new connection\n- _get_conn(timeout: float | None = None) -> HTTPConnection - Retrieves a connection from the pool or creates a new one\n- _put_conn(conn: HTTPConnection | None) -> None - Returns a connection to the pool\n- _make_request(conn: HTTPConnection, method: str, url: str, timeout: Timeout = None, body: typing.Any = None, headers: typing.Mapping[str, str] | None = None, **response_kw: typing.Any) -> BaseHTTPResponse - Makes an HTTP request using the given connection\n- urlopen(method: str, url: str, body: typing.Any = None, headers: typing.Mapping[str, str] | None = None, retries: Retry | bool | int | None = None, timeout: Timeout | float | None = None, **response_kw: typing.Any) -> BaseHTTPResponse - Main method to make HTTP requests with connection pooling and retry logic\n- close() -> None - Closes all connections in the pool\n- _close_pool_connections(pool: queue.LifoQueue[typing.Any]) -> None - Static method to close all connections in a given pool",
    "Canonical_solution": "from socket import timeout as SocketTimeout\nfrom types import TracebackType\nfrom typing import Literal, TypeVar, Union, overload\nimport typing\nimport queue\nimport weakref\nfrom .connection import HTTPConnection, HTTPSConnection\nfrom .exceptions import (\n    ClosedPoolError,\n    EmptyPoolError,\n    FullPoolError,\n    HostChangedError,\n    MaxRetryError,\n    NewConnectionError,\n    ProtocolError,\n    ReadTimeoutError,\n)\nfrom .response import BaseHTTPResponse\nfrom .util.connection import is_connection_dropped\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.retry import Retry\nfrom .util.timeout import Timeout\nfrom .util.url import Url, parse_url\nfrom .util.util import to_str\n\n_SelfT = TypeVar(\"_SelfT\")\n\nclass HTTPConnectionPool:\n    scheme = \"http\"\n    ConnectionCls = HTTPConnection\n\n    def __init__(\n        self,\n        host: str,\n        port: int | None = None,\n        timeout: Timeout | float | None = None,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        _proxy: Url | None = None,\n        _proxy_headers: typing.Mapping[str, str] | None = None,\n        **conn_kw: typing.Any,\n    ):\n        self.host = host\n        self.port = port\n        self.timeout = Timeout.from_float(timeout) if not isinstance(timeout, Timeout) else timeout\n        self.retries = retries if retries is not None else Retry.DEFAULT\n        self.pool = queue.LifoQueue(maxsize)\n        self.block = block\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.num_connections = 0\n        self.num_requests = 0\n        self.conn_kw = conn_kw\n\n        for _ in range(maxsize):\n            self.pool.put(None)\n\n        weakref.finalize(self, self._close_pool_connections, self.pool)\n\n    def _new_conn(self) -> HTTPConnection:\n        self.num_connections += 1\n        return self.ConnectionCls(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout.connect_timeout,\n            **self.conn_kw,\n        )\n\n    def _get_conn(self, timeout: float | None = None) -> HTTPConnection:\n        conn = None\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n        except queue.Empty:\n            if self.block:\n                raise EmptyPoolError(self, \"Pool is empty and blocking mode is enabled.\")\n            pass\n\n        if conn and is_connection_dropped(conn):\n            conn.close()\n            conn = None\n\n        return conn or self._new_conn()\n\n    def _put_conn(self, conn: HTTPConnection | None) -> None:\n        try:\n            self.pool.put(conn, block=False)\n        except queue.Full:\n            if conn:\n                conn.close()\n\n    def _make_request(\n        self,\n        conn: HTTPConnection,\n        method: str,\n        url: str,\n        timeout: Timeout = None,\n        body: typing.Any = None,\n        headers: typing.Mapping[str, str] | None = None,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        try:\n            conn.request(method, url, body=body, headers=headers)\n            response = conn.getresponse()\n            response._pool = self\n            return response\n        except (SocketTimeout, BaseException) as e:\n            conn.close()\n            raise\n\n    def urlopen(\n        self,\n        method: str,\n        url: str,\n        body: typing.Any = None,\n        headers: typing.Mapping[str, str] | None = None,\n        retries: Retry | bool | int | None = None,\n        timeout: Timeout | float | None = None,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        if retries is None:\n            retries = self.retries\n        elif not isinstance(retries, Retry):\n            retries = Retry.from_int(retries)\n\n        conn = None\n        try:\n            conn = self._get_conn(timeout=timeout)\n            response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout,\n                body=body,\n                headers=headers,\n                **response_kw,\n            )\n            return response\n        except (EmptyPoolError, MaxRetryError, HostChangedError) as e:\n            raise\n        except (ProtocolError, OSError) as e:\n            retries = retries.increment(method, url, error=e, _pool=self)\n            retries.sleep()\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries,\n                timeout,\n                **response_kw,\n            )\n        finally:\n            if conn:\n                self._put_conn(conn)\n\n    def close(self) -> None:\n        if self.pool:\n            self._close_pool_connections(self.pool)\n            self.pool = None\n\n    @staticmethod\n    def _close_pool_connections(pool: queue.LifoQueue[typing.Any]) -> None:\n        try:\n            while True:\n                conn = pool.get(block=False)\n                if conn:\n                    conn.close()\n        except queue.Empty:\n            pass",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import queue\nimport typing\nfrom urllib3.connection import HTTPConnection\nfrom urllib3.response import BaseHTTPResponse\nfrom urllib3.util.retry import Retry\nfrom urllib3.util.timeout import Timeout\nfrom urllib3.util.url import Url\n\nclass HTTPConnectionPool:\n    def __init__(\n        self,\n        host: str,\n        port: typing.Optional[int] = None,\n        timeout: typing.Union[Timeout, float, None] = None,\n        maxsize: int = 1,\n        block: bool = False,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        retries: typing.Union[Retry, bool, int, None] = None,\n        _proxy: typing.Optional[Url] = None,\n        _proxy_headers: typing.Optional[typing.Mapping[str, str]] = None,\n        **conn_kw: typing.Any,\n    ) -> None:\n        self.host = host\n        self.port = port\n        self.timeout = timeout\n        self.maxsize = maxsize\n        self.block = block\n        self.headers = headers\n        self.retries = retries\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers\n        self.conn_kw = conn_kw\n        self.pool = queue.LifoQueue(maxsize)\n        self.num_connections = 0\n        self.num_requests = 0\n\n    def _new_conn(self) -> HTTPConnection:\n        self.num_connections += 1\n        return HTTPConnection(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout,\n            headers=self.headers,\n            proxy=self.proxy,\n            proxy_headers=self.proxy_headers,\n            **self.conn_kw,\n        )\n\n    def _get_conn(self, timeout: typing.Optional[float] = None) -> HTTPConnection:\n        try:\n            conn = self.pool.get(block=self.block, timeout=timeout)\n            if conn.is_closed:\n                conn = self._new_conn()\n            return conn\n        except queue.Empty:\n            if self.pool.qsize() < self.maxsize:\n                return self._new_conn()\n            raise\n\n    def _put_conn(self, conn: typing.Optional[HTTPConnection]) -> None:\n        if conn is None or conn.is_closed:\n            return\n        try:\n            self.pool.put(conn, block=False)\n        except queue.Full:\n            conn.close()\n\n    def _make_request(\n        self,\n        conn: HTTPConnection,\n        method: str,\n        url: str,\n        timeout: typing.Optional[Timeout] = None,\n        body: typing.Any = None,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        self.num_requests += 1\n        return conn.request(\n            method=method,\n            url=url,\n            body=body,\n            headers=headers,\n            timeout=timeout,\n            **response_kw,\n        )\n\n    def urlopen(\n        self,\n        method: str,\n        url: str,\n        body: typing.Any = None,\n        headers: typing.Optional[typing.Mapping[str, str]] = None,\n        retries: typing.Union[Retry, bool, int, None] = None,\n        timeout: typing.Union[Timeout, float, None] = None,\n        **response_kw: typing.Any,\n    ) -> BaseHTTPResponse:\n        retries = retries if retries is not None else self.retries\n        timeout = timeout if timeout is not None else self.timeout\n\n        conn = None\n        try:\n            conn = self._get_conn(timeout=timeout)\n            response = self._make_request(\n                conn=conn,\n                method=method,\n                url=url,\n                timeout=timeout,\n                body=body,\n                headers=headers,\n                **response_kw,\n            )\n            return response\n        except Exception:\n            if conn:\n                conn.close()\n                conn = None\n            raise\n        finally:\n            if conn:\n                self._put_conn(conn)\n\n    def close(self) -> None:\n        self._close_pool_connections(self.pool)\n\n    @staticmethod\n    def _close_pool_connections(pool: queue.LifoQueue[typing.Any]) -> None:\n        while True:\n            try:\n                conn = pool.get(block=False)\n                conn.close()\n            except queue.Empty:\n                break"
  },
  {
    "Task_id": 230,
    "Instruction": "Task Description: Create a mock HTTP server for testing purposes that can handle various types of requests (GET, POST) and responses (JSON, redirects, status codes).\n\nClass Description: MockServer is a context manager class that creates and manages a lightweight HTTP server in a separate thread for testing HTTP client functionality. It provides endpoints for testing different HTTP scenarios.\n\nAttributes:\n- httpd: [HTTPServer] - The HTTP server instance\n- address: [str] - The server's IP address (127.0.0.1)\n- port: [int] - The dynamically assigned port number\n- thread: [Thread] - The thread running the server\n\nMethods:\n- __enter__() -> [MockServer] - Starts the server in a new thread when entering the context\n- __exit__(exc_type, exc_value, traceback) -> [None] - Shuts down the server and cleans up when exiting the context\n- urljoin(url: Optional[str]) -> [str] - Constructs a full URL by joining the base server URL with an optional path\n\nNested Class: _RequestHandler (BaseHTTPRequestHandler) - Handles incoming HTTP requests\n\nMethods:\n- do_POST() -> [None] - Handles POST requests by echoing back the request body\n- do_GET() -> [None] - Handles GET requests with various endpoints:\n  * /headers - Returns request headers as JSON\n  * /status/204 - Returns empty 204 response\n  * /redirect - Chain of redirect responses\n  * Default - Returns 404 for unknown paths\n- _send_json(body: dict, status: int = 200) -> [None] - Helper method to send JSON responses",
    "Canonical_solution": "from http.server import HTTPServer, BaseHTTPRequestHandler\nfrom threading import Thread\nfrom urllib.parse import urljoin\nfrom typing import Optional\n\nclass _RequestHandler(BaseHTTPRequestHandler):\n    def do_POST(self) -> None:\n        content_length = int(self.headers.get(\"Content-Length\") or 0)\n        body_bytes = b\"Request body: \" + self.rfile.read(content_length)\n        self.send_response(200)\n        self.send_header(\"Content-Length\", str(len(body_bytes)))\n        self.end_headers()\n        self.wfile.write(body_bytes)\n\n    def do_GET(self) -> None:\n        if self.path == \"/headers\":\n            self._send_json(dict(self.headers))\n        elif self.path == \"/status/204\":\n            self.send_response(204)\n            self.end_headers()\n        elif self.path == \"/redirect2\":\n            self.send_response(302)\n            self.send_header(\"Content-Length\", \"0\")\n            self.send_header(\"Location\", \"/redirect\")\n            self.end_headers()\n        elif self.path == \"/redirect\":\n            self.send_response(301)\n            self.send_header(\"Content-Length\", \"0\")\n            self.send_header(\"Location\", \"/headers\")\n            self.end_headers()\n        else:\n            self._send_json({\"error\": \"unknown path\"}, status=404)\n\n    def _send_json(self, body: dict, status: int = 200) -> None:\n        body_bytes = json.dumps(body, indent=2).encode(\"utf8\")\n        self.send_response(status)\n        self.send_header(\"Content-Length\", str(len(body_bytes)))\n        self.send_header(\"Content-Type\", \"application/json\")\n        self.end_headers()\n        self.wfile.write(body_bytes)\n\nclass MockServer:\n    def __enter__(self):\n        self.httpd = HTTPServer((\"127.0.0.1\", 0), _RequestHandler)\n        self.address, self.port = self.httpd.server_address\n        self.thread = Thread(target=self.httpd.serve_forever)\n        self.thread.start()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        self.httpd.shutdown()\n        self.thread.join()\n\n    def urljoin(self, url: Optional[str] = None) -> str:\n        return urljoin(f\"http://{self.address}:{self.port}\", url)",
    "Code_level": "Class-level",
    "FSMID_for_test": 55,
    "wrong_result": "import json\nimport threading\nfrom http.server import HTTPServer, BaseHTTPRequestHandler\nfrom urllib.parse import urljoin as urllib_urljoin\n\nclass MockServer:\n    def __init__(self):\n        self.address = '127.0.0.1'\n        self.port = 0\n        self.httpd = None\n        self.thread = None\n\n    def __enter__(self):\n        self.httpd = HTTPServer((self.address, self.port), self._RequestHandler)\n        self.port = self.httpd.server_port\n        self.thread = threading.Thread(target=self.httpd.serve_forever)\n        self.thread.daemon = True\n        self.thread.start()\n        return self\n\n    def __exit__(self, exc_type, exc_value, traceback):\n        if self.httpd:\n            self.httpd.shutdown()\n            self.httpd.server_close()\n        if self.thread:\n            self.thread.join()\n\n    def urljoin(self, url=None):\n        base = f'http://{self.address}:{self.port}'\n        return urllib_urljoin(base, url) if url else base\n\n    class _RequestHandler(BaseHTTPRequestHandler):\n        def do_POST(self):\n            content_length = int(self.headers.get('Content-Length', 0))\n            body = self.rfile.read(content_length)\n            self._send_json({'received': body.decode('utf-8')})\n\n        def do_GET(self):\n            if self.path == '/headers':\n                self._send_json(dict(self.headers))\n            elif self.path == '/status/204':\n                self.send_response(204)\n                self.end_headers()\n            elif self.path == '/redirect':\n                self.send_response(302)\n                self.send_header('Location', '/headers')\n                self.end_headers()\n            else:\n                self.send_error(404, 'Not Found')\n\n        def _send_json(self, body, status=200):\n            self.send_response(status)\n            self.send_header('Content-Type', 'application/json')\n            self.end_headers()\n            self.wfile.write(json.dumps(body).encode('utf-8'))"
  },
  {
    "Task_id": 234,
    "Instruction": "Task Description: Implement a Tornado-based HTTP server for Bayesian optimization that can receive parameters and targets via POST requests, register them with a Bayesian optimization instance, and return suggested parameters for the next evaluation.\n\nClass Description: BayesianOptimizationHandler is a Tornado RequestHandler subclass that handles HTTP POST requests for Bayesian optimization. It maintains a BayesianOptimization instance and a UtilityFunction to suggest new parameters based on registered data points.\n\nAttributes:\n_bo: [BayesianOptimization] - Instance of BayesianOptimization configured with a black-box function and parameter bounds\n_uf: [UtilityFunction] - Utility function instance used for suggesting new parameters\n\nMethods:\npost: [post]() -> [None] - Handles HTTP POST requests. Expects JSON body with \"params\" and \"target\" fields to register with the optimizer. Returns suggested parameters as JSON. The method:\n1. Decodes the JSON request body\n2. Attempts to register the parameters and target with the optimizer\n3. Generates and returns new suggested parameters\n4. Writes the suggested parameters as JSON response\n\nThe class should be used as part of a Tornado web application that listens on a specified port for optimization requests.",
    "Canonical_solution": "import json\nimport tornado.ioloop\nimport tornado.httpserver\nfrom tornado.web import RequestHandler\nimport requests\n\nclass BayesianOptimizationHandler(RequestHandler):\n    \"\"\"Basic functionality for NLP handlers.\"\"\"\n    _bo = BayesianOptimization(\n        f=black_box_function,\n        pbounds={\"x\": (-4, 4), \"y\": (-3, 3)}\n    )\n    _uf = UtilityFunction(kind=\"ucb\", kappa=3, xi=1)\n\n    def post(self):\n        \"\"\"Deal with incoming requests.\"\"\"\n        body = tornado.escape.json_decode(self.request.body)\n\n        try:\n            self._bo.register(\n                params=body[\"params\"],\n                target=body[\"target\"],\n            )\n            print(\"BO has registered: {} points.\".format(len(self._bo.space)), end=\"\\n\\n\")\n        except KeyError:\n            pass\n        finally:\n            suggested_params = self._bo.suggest(self._uf)\n\n        self.write(json.dumps(suggested_params))\n\ndef run_optimization_app():\n    asyncio.set_event_loop(asyncio.new_event_loop())\n    handlers = [\n        (r\"/bayesian_optimization\", BayesianOptimizationHandler),\n    ]\n    server = tornado.httpserver.HTTPServer(\n        tornado.web.Application(handlers)\n    )\n    server.listen(9009)\n    tornado.ioloop.IOLoop.instance().start()",
    "Code_level": "Class-level",
    "FSMID_for_test": 64,
    "wrong_result": "import json\nimport tornado.ioloop\nimport tornado.web\nfrom bayes_opt import BayesianOptimization, UtilityFunction\n\nclass BayesianOptimizationHandler(tornado.web.RequestHandler):\n    def initialize(self, _bo, _uf):\n        self._bo = _bo\n        self._uf = _uf\n\n    def post(self):\n        try:\n            data = json.loads(self.request.body)\n            params = data.get('params')\n            target = data.get('target')\n            \n            if params is None or target is None:\n                raise ValueError(\"Missing 'params' or 'target' in request\")\n            \n            self._bo.register(params=params, target=target)\n            suggestion = self._bo.suggest(self._uf)\n            \n            self.write(json.dumps(suggestion))\n        except Exception as e:\n            self.set_status(400)\n            self.write(json.dumps({'error': str(e)}))"
  },
  {
    "Task_id": 24,
    "Instruction": "Task Description: Create a function to send metric data to the Librato metrics service using HTTP POST requests with basic authentication.\n\nFunction Description: The function `count` sends a counter metric to Librato's API. It establishes an HTTPS connection to the metrics API server, sends the metric data as a URL-encoded form, and includes basic authentication headers. The function handles the case where authentication credentials are not configured by doing nothing.\n\nInput:\n- `name` (str): The name of the counter metric to be sent\n- `value` (int, optional): The value to increment the counter by (defaults to 1)\n\nOutput: None (The function doesn't return any value, but sends data to the external metrics service)",
    "Canonical_solution": "from ConfigParser import NoOptionError, NoSectionError\nimport base64\nimport httplib\nimport urllib\nfrom blueprint import cfg\n\ntry:\n    token = cfg.get('librato', 'token')\n    username = cfg.get('librato', 'username')\n    auth = 'Basic {0}'.format(base64.b64encode('{0}:{1}'.format(username,\n                                                                token)))\nexcept (NoOptionError, NoSectionError):\n    auth = None\n\ndef count(name, value=1):\n    if auth is None:\n        return\n    conn = httplib.HTTPSConnection('metrics-api.librato.com')\n    conn.request('POST',\n                 '/v1/counters/{0}.json'.format(urllib.quote(name)),\n                 urllib.urlencode({'value': value}),\n                 {'Authorization': auth,\n                  'Content-Type': 'application/x-www-form-urlencoded'})\n    r = conn.getresponse()\n    conn.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 56,
    "wrong_result": "import os\nimport urllib.parse\nimport urllib.request\n\ndef count(name, value=1):\n    username = os.getenv('LIBRATO_USERNAME')\n    token = os.getenv('LIBRATO_TOKEN')\n    if not username or not token:\n        return\n\n    data = urllib.parse.urlencode({\n        'counters[0][name]': name,\n        'counters[0][value]': value\n    }).encode('utf-8')\n\n    req = urllib.request.Request(\n        'https://metrics-api.librato.com/v1/metrics',\n        data=data,\n        headers={\n            'Authorization': 'Basic ' + (username + ':' + token).encode('base64').strip(),\n            'Content-Type': 'application/x-www-form-urlencoded'\n        }\n    )\n\n    try:\n        urllib.request.urlopen(req)\n    except:\n        pass"
  },
  {
    "Task_id": 26,
    "Instruction": "Task Description: Implement an HTTP connection pool class that manages HTTP connections to a specified host, handling connection creation, request execution, and response processing with support for retries, timeouts, and redirects.\n\nClass Description: HTTPConnectionPool is a connection pool manager for HTTP connections that provides efficient reuse of connections to a specified host. It handles connection lifecycle, request execution, error handling, and retries.\n\nAttributes:\n- scheme: str - The URL scheme (default \"http\")\n- host: str - The normalized hostname for connections\n- port: int - The port number for connections\n- timeout: Timeout - Connection and read timeout configuration\n- retries: Retry - Retry configuration for failed requests\n- proxy: str - Proxy server URL if configured\n- proxy_headers: dict - Headers for proxy connections\n- conn_kw: dict - Additional connection parameters\n\nMethods:\n- __init__(host, port=None, strict=False, timeout=Timeout.DEFAULT_TIMEOUT, maxsize=1, block=False, headers=None, retries=None, _proxy=None, _proxy_headers=None, **conn_kw) -> None\n  - Initializes the connection pool with target host and configuration\n  - Input: \n    - host: str - Target hostname\n    - port: int - Target port (optional)\n    - timeout: Timeout - Timeout configuration\n    - retries: Retry - Retry configuration\n    - _proxy: str - Proxy URL (optional)\n    - _proxy_headers: dict - Proxy headers (optional)\n    - conn_kw: dict - Additional connection parameters\n\n- _new_conn() -> HTTPConnection\n  - Creates a new HTTP connection\n  - Output: HTTPConnection - A new connection instance\n\n- _make_request(conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw) -> HTTPResponse\n  - Executes an HTTP request through the given connection\n  - Input:\n    - conn: HTTPConnection - Connection to use\n    - method: str - HTTP method (GET, POST, etc.)\n    - url: str - Target URL\n    - timeout: Timeout - Request timeout\n    - chunked: bool - Whether to use chunked transfer encoding\n  - Output: HTTPResponse - The HTTP response\n\n- urlopen(method, url, body=None, headers=None, retries=None, redirect=True, assert_same_host=True, timeout=_Default, pool_timeout=None, release_conn=None, chunked=False, **response_kw) -> HTTPResponse\n  - Main method to execute HTTP requests with connection pooling\n  - Input:\n    - method: str - HTTP method\n    - url: str - Target URL\n    - body: str - Request body (optional)\n    - headers: dict - Request headers (optional)\n    - retries: Retry - Retry configuration (optional)\n    - redirect: bool - Whether to follow redirects\n    - timeout: Timeout - Request timeout\n    - chunked: bool - Use chunked transfer encoding\n  - Output: HTTPResponse - The HTTP response",
    "Canonical_solution": "import socket\nfrom socket import error as SocketError\nfrom socket import timeout as SocketTimeout\nfrom .connection import HTTPConnection, HTTPSConnection\nfrom .exceptions import (\n    EmptyPoolError,\n    HostChangedError,\n    MaxRetryError,\n    ProtocolError,\n    ProxyError,\n    ReadTimeoutError,\n    SSLError,\n    TimeoutError,\n)\nfrom .util.connection import is_connection_dropped\nfrom .util.proxy import connection_requires_http_tunnel\nfrom .util.retry import Retry\nfrom .util.timeout import Timeout\nfrom .util.url import Url, _encode_target\nfrom .util.url import _normalize_host as normalize_host\nfrom .util.url import get_host, parse_url\n\nclass HTTPConnectionPool:\n    scheme = \"http\"\n    ConnectionCls = HTTPConnection\n\n    def __init__(\n        self,\n        host,\n        port=None,\n        strict=False,\n        timeout=Timeout.DEFAULT_TIMEOUT,\n        maxsize=1,\n        block=False,\n        headers=None,\n        retries=None,\n        _proxy=None,\n        _proxy_headers=None,\n        **conn_kw\n    ):\n        self.host = _normalize_host(host, scheme=self.scheme)\n        self._proxy_host = host.lower()\n        self.port = port\n        self.timeout = timeout\n        self.retries = retries\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.conn_kw = conn_kw\n\n    def _new_conn(self):\n        return self.ConnectionCls(\n            host=self.host,\n            port=self.port,\n            timeout=self.timeout.connect_timeout,\n            strict=self.strict,\n            **self.conn_kw\n        )\n\n    def _make_request(\n        self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw\n    ):\n        try:\n            if chunked:\n                conn.request_chunked(method, url, **httplib_request_kw)\n            else:\n                conn.request(method, url, **httplib_request_kw)\n        except (SocketTimeout, BaseSSLError) as e:\n            raise\n\n        read_timeout = timeout_obj.read_timeout\n        if getattr(conn, \"sock\", None):\n            if read_timeout == 0:\n                raise ReadTimeoutError(\n                    self, url, \"Read timed out. (read timeout=%s)\" % read_timeout\n                )\n            conn.sock.settimeout(read_timeout)\n\n        try:\n            httplib_response = conn.getresponse()\n        except (SocketTimeout, BaseSSLError, SocketError) as e:\n            raise\n\n        return httplib_response\n\n    def urlopen(\n        self,\n        method,\n        url,\n        body=None,\n        headers=None,\n        retries=None,\n        redirect=True,\n        assert_same_host=True,\n        timeout=_Default,\n        pool_timeout=None,\n        release_conn=None,\n        chunked=False,\n        **response_kw\n    ):\n        parsed_url = parse_url(url)\n        destination_scheme = parsed_url.scheme\n\n        if not isinstance(retries, Retry):\n            retries = Retry.from_int(retries, redirect=redirect, default=self.retries)\n\n        if assert_same_host and not self.is_same_host(url):\n            raise HostChangedError(self, url, retries)\n\n        conn = None\n        release_this_conn = release_conn\n        http_tunnel_required = connection_requires_http_tunnel(\n            self.proxy, self.proxy_config, destination_scheme\n        )\n\n        try:\n            timeout_obj = self._get_timeout(timeout)\n            conn = self._get_conn(timeout=pool_timeout)\n            conn.timeout = timeout_obj.connect_timeout\n\n            if http_tunnel_required:\n                self._prepare_proxy(conn)\n\n            httplib_response = self._make_request(\n                conn,\n                method,\n                url,\n                timeout=timeout_obj,\n                body=body,\n                headers=headers,\n                chunked=chunked,\n            )\n\n            response = self.ResponseCls.from_httplib(\n                httplib_response,\n                pool=self,\n                connection=response_conn,\n                retries=retries,\n                **response_kw\n            )\n\n        except (\n            TimeoutError,\n            HTTPException,\n            SocketError,\n            ProtocolError,\n            BaseSSLError,\n            SSLError,\n            CertificateError,\n        ) as e:\n            retries = retries.increment(\n                method, url, error=e, _pool=self, _stacktrace=sys.exc_info()[2]\n            )\n            retries.sleep()\n            conn = conn and conn.close()\n\n        finally:\n            if release_this_conn:\n                self._put_conn(conn)\n\n        if not conn:\n            return self.urlopen(\n                method,\n                url,\n                body,\n                headers,\n                retries,\n                redirect,\n                assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                **response_kw\n            )\n\n        redirect_location = redirect and response.get_redirect_location()\n        if redirect_location:\n            response.drain_conn()\n            retries.sleep_for_retry(response)\n            return self.urlopen(\n                method,\n                redirect_location,\n                body,\n                headers,\n                retries=retries,\n                redirect=redirect,\n                assert_same_host=assert_same_host,\n                timeout=timeout,\n                pool_timeout=pool_timeout,\n                release_conn=release_conn,\n                chunked=chunked,\n                **response_kw\n            )\n\n        return response",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import socket\nimport sys\nimport time\nfrom http.client import HTTPConnection, HTTPResponse\nfrom queue import LifoQueue, Empty\nfrom urllib.parse import urlparse\n\nclass Timeout:\n    DEFAULT_TIMEOUT = 30.0\n\n    def __init__(self, connect=None, read=None):\n        self.connect = float(connect) if connect is not None else self.DEFAULT_TIMEOUT\n        self.read = float(read) if read is not None else self.DEFAULT_TIMEOUT\n\nclass Retry:\n    def __init__(self, total=3, connect=None, read=None, redirect=None, status=None, other=None):\n        self.total = total\n        self.connect = connect if connect is not None else total\n        self.read = read if read is not None else total\n        self.redirect = redirect if redirect is not None else total\n        self.status = status if status is not None else total\n        self.other = other if other is not None else total\n\nclass HTTPConnectionPool:\n    def __init__(self, host, port=None, strict=False, timeout=Timeout.DEFAULT_TIMEOUT, maxsize=1, block=False, headers=None, retries=None, _proxy=None, _proxy_headers=None, **conn_kw):\n        self.scheme = 'http'\n        self.host = host\n        self.port = port or 80\n        self.timeout = timeout if isinstance(timeout, Timeout) else Timeout(timeout)\n        self.retries = retries if retries is not None else Retry()\n        self.proxy = _proxy\n        self.proxy_headers = _proxy_headers or {}\n        self.conn_kw = conn_kw\n        self.maxsize = maxsize\n        self.block = block\n        self.headers = headers or {}\n        self.pool = LifoQueue(maxsize)\n        for _ in range(maxsize):\n            self.pool.put(None)\n\n    def _new_conn(self):\n        return HTTPConnection(host=self.host, port=self.port, timeout=self.timeout.connect, **self.conn_kw)\n\n    def _make_request(self, conn, method, url, timeout=_Default, chunked=False, **httplib_request_kw):\n        if timeout is _Default:\n            timeout = self.timeout.read\n        conn.timeout = timeout\n        conn.request(method, url, **httplib_request_kw)\n        response = conn.getresponse()\n        return response\n\n    def urlopen(self, method, url, body=None, headers=None, retries=None, redirect=True, assert_same_host=True, timeout=_Default, pool_timeout=None, release_conn=None, chunked=False, **response_kw):\n        if retries is None:\n            retries = self.retries\n        headers = headers or {}\n        headers.update(self.headers)\n        if pool_timeout is not None and self.block:\n            deadline = time.time() + pool_timeout\n        else:\n            deadline = None\n        conn = None\n        try:\n            while True:\n                try:\n                    conn = self.pool.get(block=self.block, timeout=pool_timeout)\n                    if conn is None:\n                        conn = self._new_conn()\n                    break\n                except Empty:\n                    if deadline is not None and time.time() > deadline:\n                        raise\n                    continue\n            try:\n                response = self._make_request(conn, method, url, timeout=timeout, chunked=chunked, body=body, headers=headers)\n            except (socket.timeout, socket.error) as e:\n                if retries.connect is not None and retries.connect > 0:\n                    retries.connect -= 1\n                    continue\n                raise\n            if redirect and response.status in (301, 302, 303, 307):\n                if retries.redirect is not None and retries.redirect > 0:\n                    retries.redirect -= 1\n                    location = response.getheader('Location')\n                    if location:\n                        parsed = urlparse(location)\n                        if assert_same_host and parsed.hostname != self.host:\n                            raise ValueError(\"Redirect host does not match\")\n                        return self.urlopen(method, location, body=body, headers=headers, retries=retries, redirect=redirect, assert_same_host=assert_same_host, timeout=timeout, pool_timeout=pool_timeout, release_conn=release_conn, chunked=chunked, **response_kw)\n            return response\n        finally:\n            if release_conn is not False and conn is not None:\n                self.pool.put(conn)"
  },
  {
    "Task_id": 268,
    "Instruction": "Task Description: Create a Python class that implements an MQTT client using the Paho MQTT library to subscribe to topics, handle incoming messages, and trigger appropriate actions based on the received messages.\n\nClass Description: The `MqttClient` class is a threaded MQTT client that connects to a broker, subscribes to specified topics, and processes incoming messages. It supports secure connections with TLS and authentication. Messages can be in JSON format and are routed to designated synapses (processing units) based on the topic.\n\nAttributes:\n\n- broker: [object] - Contains broker configuration details (client_id, protocol, username, password, ca_cert, certfile, keyfile, tls_insecure, broker_ip, port, keepalive, topics)\n- brain: [object] - Reference to the main processing unit that manages synapses\n- client: [paho.mqtt.client.Client] - The MQTT client instance\n\nMethods:\n\n- __init__(broker=None, brain=None) -> [None] - Initializes the MQTT client with broker configuration and brain reference. Sets up connection callbacks and security parameters.\n- run() -> [None] - Connects to the broker and starts the MQTT loop in a separate thread. Handles socket errors gracefully.\n- on_connect(client, userdata, flags, rc) -> [None] - Callback for connection events. Subscribes to all configured topics on successful connection (rc=0).\n- on_message(client, userdata, msg) -> [None] - Callback for incoming messages. Routes messages to concerned synapses based on topic.\n- on_subscribe(mqttc, obj, mid, granted_qos) -> [None] - Callback for subscription confirmation (currently no operation).\n- call_concerned_synapses(topic_name, message) -> [None] - Processes incoming messages, converts JSON if needed, and triggers appropriate synapses.\n- _get_protocol(protocol) -> [int] - Static method that converts protocol string to Paho MQTT protocol constant.",
    "Canonical_solution": "import json\nimport logging\nimport socket\nfrom threading import Thread\nimport paho.mqtt.client as mqtt\n\nclass MqttClient(Thread):\n    def __init__(self, broker=None, brain=None):\n        super(MqttClient, self).__init__()\n        self.broker = broker\n        self.brain = brain\n\n        self.client = mqtt.Client(client_id=self.broker.client_id, protocol=self._get_protocol(self.broker.protocol))\n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n        self.client.on_subscribe = self.on_subscribe\n\n        if self.broker.username is not None and self.broker.password is not None:\n            self.client.username_pw_set(self.broker.username, self.broker.password)\n\n        if self.broker.ca_cert is not None and self.broker.certfile is not None and self.broker.keyfile is not None:\n            self.client.tls_set(ca_certs=self.broker.ca_cert,\n                               certfile=self.broker.certfile,\n                               keyfile=self.broker.keyfile)\n            self.client.tls_insecure_set(self.broker.tls_insecure)\n        elif self.broker.ca_cert is not None:\n            self.client.tls_set(ca_certs=self.broker.ca_cert)\n            self.client.tls_insecure_set(self.broker.tls_insecure)\n\n    def run(self):\n        try:\n            self.client.connect(self.broker.broker_ip, self.broker.port, self.broker.keepalive)\n            self.client.loop_forever()\n        except socket.error:\n            pass\n\n    def on_connect(self, client, userdata, flags, rc):\n        if rc == 0:\n            for topic in self.broker.topics:\n                client.subscribe(topic.name)\n        else:\n            self.client.disconnect()\n\n    def on_message(self, client, userdata, msg):\n        self.call_concerned_synapses(msg.topic, msg.payload)\n\n    def on_subscribe(self, mqttc, obj, mid, granted_qos):\n        pass\n\n    def call_concerned_synapses(self, topic_name, message):\n        target_topic = next(topic for topic in self.broker.topics if topic.name == topic_name)\n        if target_topic.is_json:\n            message = json.loads(message)\n        \n        for synapse in target_topic.synapses:\n            overriding_parameter_dict = dict()\n            overriding_parameter_dict[\"mqtt_subscriber_message\"] = message\n            SynapseLauncher.start_synapse_by_list_name([synapse.name],\n                                                     brain=self.brain,\n                                                     overriding_parameter_dict=overriding_parameter_dict)\n\n    @staticmethod\n    def _get_protocol(protocol):\n        if protocol == \"MQTTv31\":\n            return paho.mqtt.client.MQTTv31\n        return paho.mqtt.client.MQTTv311",
    "Code_level": "Class-level",
    "FSMID_for_test": 84,
    "wrong_result": "import paho.mqtt.client as mqtt\nimport threading\nimport json\nimport ssl\n\nclass MqttClient:\n    def __init__(self, broker=None, brain=None):\n        self.broker = broker\n        self.brain = brain\n        self.client = mqtt.Client(client_id=self.broker['client_id'], protocol=self._get_protocol(self.broker['protocol']))\n        \n        if self.broker['username'] and self.broker['password']:\n            self.client.username_pw_set(self.broker['username'], self.broker['password'])\n        \n        if self.broker['ca_cert']:\n            self.client.tls_set(ca_certs=self.broker['ca_cert'], certfile=self.broker['certfile'], \n                               keyfile=self.broker['keyfile'], tls_version=ssl.PROTOCOL_TLSv1_2)\n            if self.broker['tls_insecure']:\n                self.client.tls_insecure_set(True)\n        \n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n        self.client.on_subscribe = self.on_subscribe\n\n    def run(self):\n        try:\n            self.client.connect(self.broker['broker_ip'], port=self.broker['port'], keepalive=self.broker['keepalive'])\n            self.client.loop_start()\n        except Exception as e:\n            pass\n\n    def on_connect(self, client, userdata, flags, rc):\n        if rc == 0:\n            for topic in self.broker['topics']:\n                self.client.subscribe(topic)\n\n    def on_message(self, client, userdata, msg):\n        self.call_concerned_synapses(msg.topic, msg.payload)\n\n    def on_subscribe(self, mqttc, obj, mid, granted_qos):\n        pass\n\n    def call_concerned_synapses(self, topic_name, message):\n        try:\n            payload = json.loads(message.decode('utf-8'))\n        except:\n            payload = message.decode('utf-8')\n        \n        if self.brain:\n            self.brain.on_message(topic_name, payload)\n\n    @staticmethod\n    def _get_protocol(protocol):\n        if protocol == 'MQTTv31':\n            return mqtt.MQTTv31\n        elif protocol == 'MQTTv311':\n            return mqtt.MQTTv311\n        elif protocol == 'MQTTv5':\n            return mqtt.MQTTv5\n        else:\n            return mqtt.MQTTv311"
  },
  {
    "Task_id": 269,
    "Instruction": "Task Description: Implement an MQTT subscriber function in Python that connects to a broker, subscribes to a topic, and processes incoming messages using the Paho MQTT client library.\n\nFunction Description: The function `mqtt_subscriber` establishes a connection to an MQTT broker over WebSockets, subscribes to a specified topic, and continuously listens for incoming messages. It utilizes callback functions to handle connection events, incoming messages, and subscription acknowledgments.\n\nInput: None (The broker address, port, and topic are hardcoded in the function for simplicity.)\n\nOutput: None (The function prints connection status, subscription acknowledgments, and received messages to the console.)\n\n---\n\nTask Description: Implement an MQTT subscriber class in Python that encapsulates the functionality of connecting to a broker, subscribing to topics, and processing incoming messages using the Paho MQTT client library.\n\nClass Description: The `MQTTSubscriber` class provides a reusable and configurable way to subscribe to MQTT topics. It initializes with broker details and topic configurations, and it uses callback methods to handle MQTT events such as connection, message reception, and subscription.\n\nAttributes:\n\n- `client`: [mqtt.Client] - The Paho MQTT client instance used for communication.\n- `broker_address`: [str] - The address of the MQTT broker.\n- `broker_port`: [int] - The port number of the MQTT broker.\n- `topic`: [str] - The MQTT topic to subscribe to.\n- `qos`: [int] - The Quality of Service level for the subscription.\n\nMethods:\n\n- `__init__(broker_address: str, broker_port: int, topic: str, qos: int = 0) -> None` - Initializes the MQTT subscriber with broker details and topic configuration.\n- `on_connect(client: mqtt.Client, obj: Any, flags: dict, reason_code: int, properties: Any) -> None` - Callback for when the client connects to the broker.\n- `on_message(client: mqtt.Client, obj: Any, msg: mqtt.MQTTMessage) -> None` - Callback for when a message is received on the subscribed topic.\n- `on_subscribe(client: mqtt.Client, obj: Any, mid: int, reason_code_list: list, properties: Any) -> None` - Callback for when the client subscribes to a topic.\n- `start() -> None` - Connects to the broker, subscribes to the topic, and starts the MQTT loop to process messages.\n- `stop() -> None` - Stops the MQTT loop and disconnects from the broker.",
    "Canonical_solution": "import paho.mqtt.client as mqtt\n\ndef on_connect(mqttc, obj, flags, reason_code, properties):\n    print(\"reason_code: \"+str(reason_code))\n\ndef on_message(mqttc, obj, msg):\n    print(msg.topic+\" \"+str(msg.qos)+\" \"+str(msg.payload))\n\ndef on_subscribe(mqttc, obj, mid, reason_code_list, properties):\n    print(\"Subscribed: \"+str(mid)+\" \"+str(reason_code_list))\n\ndef on_log(mqttc, obj, level, string):\n    print(string)\n\ndef mqtt_subscriber():\n    mqttc = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2, transport=\"websockets\")\n    mqttc.on_message = on_message\n    mqttc.on_connect = on_connect\n    mqttc.on_subscribe = on_subscribe\n    mqttc.connect(\"mqtt.eclipseprojects.io\", 80, 60)\n    mqttc.subscribe(\"$SYS/broker/version\", 0)\n    mqttc.loop_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 84,
    "wrong_result": "import paho.mqtt.client as mqtt\n\nclass MQTTSubscriber:\n    def __init__(self, broker_address, broker_port, topic, qos=0):\n        self.broker_address = broker_address\n        self.broker_port = broker_port\n        self.topic = topic\n        self.qos = qos\n        self.client = mqtt.Client(mqtt.CallbackAPIVersion.VERSION2)\n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n        self.client.on_subscribe = self.on_subscribe\n\n    def on_connect(self, client, obj, flags, reason_code, properties):\n        if reason_code == 0:\n            print(\"Connected to MQTT Broker!\")\n            self.client.subscribe(self.topic, self.qos)\n        else:\n            print(f\"Failed to connect, return code {reason_code}\")\n\n    def on_message(self, client, obj, msg):\n        print(f\"Received message on topic {msg.topic}: {msg.payload.decode()}\")\n\n    def on_subscribe(self, client, obj, mid, reason_code_list, properties):\n        print(f\"Subscribed to topic {self.topic} with QoS {self.qos}\")\n\n    def start(self):\n        self.client.connect(self.broker_address, self.broker_port)\n        self.client.loop_start()\n\n    def stop(self):\n        self.client.loop_stop()\n        self.client.disconnect()"
  },
  {
    "Task_id": 270,
    "Instruction": "Task Description: Create a Python class that implements an MQTT client for subscribing to topics and handling incoming messages, with support for custom command processing and authentication.\n\nClass Description: The MQTTClient class provides functionality to connect to an MQTT broker, subscribe to specified topics, and process incoming messages. It includes handlers for custom commands and button press events, with optional username/password authentication.\n\nAttributes:\n- host: str - The hostname or IP address of the MQTT broker\n- port: int - The port number of the MQTT broker\n- topic: str - The topic to subscribe to\n- username: str (optional) - Username for broker authentication\n- password: str (optional) - Password for broker authentication\n- client: mqtt.Client - The Paho MQTT client instance\n\nMethods:\n- __init__(host: str, port: int, topic: str, username: str=None, password: str=None) -> None - Initializes the MQTT client with connection parameters\n- on_connect(client: mqtt.Client, userdata: Any, flags: dict, rc: int) -> None - Callback for when the client connects to the broker\n- on_message(client: mqtt.Client, userdata: Any, msg: mqtt.MQTTMessage) -> None - Callback for handling incoming messages\n- start() -> None - Starts the MQTT client connection and message loop\n- custom_command(command: str) -> None - Placeholder for handling custom commands\n- buttonsinglepress() -> None - Placeholder for handling button press events",
    "Canonical_solution": "import paho.mqtt.client as mqtt\n\nclass MQTTClient:\n    def __init__(self, host, port, topic, username=None, password=None):\n        self.host = host\n        self.port = port\n        self.topic = topic\n        self.username = username\n        self.password = password\n        self.client = mqtt.Client()\n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n\n    def on_connect(self, client, userdata, flags, rc):\n        print(\"Connected with result code \"+str(rc))\n        client.subscribe(self.topic)\n\n    def on_message(self, client, userdata, msg):\n        print(\"Message from MQTT: \"+str(msg.payload.decode('utf-8')))\n        mqtt_query = str(msg.payload.decode('utf-8'))\n        if 'custom' in mqtt_query.lower():\n            mqtt_query = mqtt_query.lower()\n            mqtt_queryidx = mqtt_query.find('custom')\n            mqtt_query = mqtt_query[mqtt_queryidx:]\n            mqtt_query = mqtt_query.replace('custom',\"\",1)\n            mqtt_query = mqtt_query.strip()\n            self.custom_command(mqtt_query)\n        elif mqtt_query.lower() == 'mute':\n            self.buttonsinglepress()\n        else:\n            self.assistant.send_text_query(mqtt_query)\n\n    def start(self):\n        if self.username and self.password:\n            self.client.username_pw_set(self.username, self.password)\n        self.client.connect(self.host, self.port, 60)\n        self.client.loop_forever()\n\n    def custom_command(self, command):\n        # Placeholder for custom command handling\n        pass\n\n    def buttonsinglepress(self):\n        # Placeholder for button press handling\n        pass",
    "Code_level": "Class-level",
    "FSMID_for_test": 84,
    "wrong_result": "import paho.mqtt.client as mqtt\n\nclass MQTTClient:\n    def __init__(self, host, port, topic, username=None, password=None):\n        self.host = host\n        self.port = port\n        self.topic = topic\n        self.username = username\n        self.password = password\n        self.client = mqtt.Client()\n        self.client.on_connect = self.on_connect\n        self.client.on_message = self.on_message\n        if self.username and self.password:\n            self.client.username_pw_set(self.username, self.password)\n\n    def on_connect(self, client, userdata, flags, rc):\n        if rc == 0:\n            client.subscribe(self.topic)\n        else:\n            raise ConnectionError(f\"Connection failed with result code {rc}\")\n\n    def on_message(self, client, userdata, msg):\n        payload = msg.payload.decode()\n        if payload == \"buttonsinglepress\":\n            self.buttonsinglepress()\n        else:\n            self.custom_command(payload)\n\n    def start(self):\n        self.client.connect(self.host, self.port)\n        self.client.loop_start()\n\n    def custom_command(self, command):\n        pass\n\n    def buttonsinglepress(self):\n        pass"
  },
  {
    "Task_id": 271,
    "Instruction": "Task Description: Create a Python function that establishes an MQTT client connection, subscribes to specified topics, and continuously processes incoming messages using callback functions.\n\nFunction Description: The function `wait_for_messages` initializes an MQTT client, sets up a message callback handler, subscribes to predefined topics, and enters an infinite loop to process incoming MQTT messages. It handles different message types through topic-specific callback functions.\n\nInput: None (The function uses predefined configuration and topics internally)\n\nOutput: None (The function runs indefinitely, processing messages through callbacks)\n\n---\n\nTask Description: Create a helper function to initialize and configure an MQTT client with WebSocket transport.\n\nFunction Description: The function `get_client` creates and configures an MQTT client instance with WebSocket transport, sets authentication credentials, and establishes an asynchronous connection to the broker.\n\nInput: None (Uses hardcoded connection parameters)\n\nOutput: paho.mqtt.client.Client - Returns a configured MQTT client instance\n\n---\n\nTask Description: Create a callback function to handle incoming MQTT messages and route them to appropriate handlers based on topic.\n\nFunction Description: The function `on_message_callback` processes incoming MQTT messages, logs receipt, and routes them to specific handler functions based on topic patterns. It includes error handling for message processing.\n\nInput:\n- client: paho.mqtt.client.Client - The MQTT client instance\n- userdata: Any - User-defined data passed to the callback\n- message: paho.mqtt.client.MQTTMessage - The received message object\n\nOutput: None (Messages are processed through side effects in handler functions)",
    "Canonical_solution": "import json\nimport logging\nimport paho.mqtt.client as paho\n\ndef get_client():\n    mqtt_client = paho.Client(transport=\"websockets\", client_id=\"listener\")\n    mqtt_client.enable_logger()\n    mqtt_client.username_pw_set(username=\"tavern\", password=\"tavern\")\n    mqtt_client.connect_async(host=\"broker\", port=9001)\n    return mqtt_client\n\ndef on_message_callback(client, userdata, message):\n    try:\n        logging.info(\"Received message on %s\", message.topic)\n\n        if \"devices/status\" in message.topic:\n            handle_full_status_topic(client, message)\n        elif \"lights\" in message.topic:\n            handle_lights_topic(message)\n        elif \"echo\" in message.topic:\n            handle_echo_topic(client, message)\n        elif \"ping\" in message.topic:\n            handle_ping_topic(client, message)\n        elif \"status\" in message.topic:\n            handle_status_topic(client, message)\n        else:\n            logging.warning(\"Got unexpected MQTT topic '%s'\", message.topic)\n    except Exception as e:\n        logging.exception(\"error handling message: {}\".format(e))\n\ndef wait_for_messages():\n    mqtt_client = get_client()\n    mqtt_client.on_message = on_message_callback\n    mqtt_client.reconnect()\n\n    topics = [\"lights\", \"ping\", \"echo\", \"status\"]\n\n    for t in topics:\n        device_topic = \"/device/+/{}\".format(t)\n        logging.debug(\"Subscribing to '%s'\", device_topic)\n        mqtt_client.subscribe(device_topic)\n\n    mqtt_client.subscribe(\"/devices/status\")\n\n    mqtt_client.loop_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 84,
    "wrong_result": "import paho.mqtt.client as mqtt\nimport logging\nimport time\n\ndef get_client():\n    client = mqtt.Client(transport=\"websockets\")\n    client.ws_set_options(path=\"/mqtt\")\n    client.username_pw_set(\"username\", \"password\")\n    client.connect(\"broker.example.com\", 9001, 60)\n    return client\n\ndef on_message_callback(client, userdata, message):\n    try:\n        logging.info(f\"Received message on topic {message.topic}\")\n        if message.topic.startswith(\"sensors/\"):\n            handle_sensor_message(message)\n        elif message.topic.startswith(\"commands/\"):\n            handle_command_message(message)\n        elif message.topic.startswith(\"status/\"):\n            handle_status_message(message)\n        else:\n            logging.warning(f\"No handler for topic {message.topic}\")\n    except Exception as e:\n        logging.error(f\"Error processing message: {str(e)}\")\n\ndef wait_for_messages():\n    client = get_client()\n    client.on_message = on_message_callback\n    client.subscribe(\"sensors/#\")\n    client.subscribe(\"commands/#\")\n    client.subscribe(\"status/#\")\n    client.loop_start()\n    while True:\n        time.sleep(1)"
  },
  {
    "Task_id": 279,
    "Instruction": "Task Description: Create a Python class that utilizes subprocess communication to perform network scanning operations using masscan and nmap tools for both TCP and UDP ports.\n\nClass Description: The SubprocessScanner class provides functionality to scan a target host for open TCP/UDP ports using masscan and then perform service detection on those ports using nmap. It handles subprocess communication and output parsing.\n\nAttributes:\nhost: str - The target host IP address or hostname to be scanned\n\nMethods:\nuseMasscanTCP() -> list - Scans the host for open TCP ports using masscan and returns a list of open port numbers\nuseMasscanUDP() -> list - Scans the host for open UDP ports using masscan and returns a list of open port numbers\nuseNmapServTCP(port: str) -> str - Performs service detection on the specified TCP port using nmap and returns the service information\nuseNmapServUDP(port: str) -> str - Performs service detection on the specified UDP port using nmap and returns the service information",
    "Canonical_solution": "import subprocess\nimport re\n\nclass SubprocessScanner:\n    def __init__(self, host):\n        self.host = host\n\n    def useMasscanTCP(self):\n        try:\n            ports = list()\n            cmd = ['masscan', '-sS', '-Pn', '-p21-25,53,80-90,99,110,113,119,121-123,137-139,\\\n                    170,443-445,456,554,513-514,559,873,888,1080-1099,1200-1212,1234,1243-1255,\\\n                    1433-1434,1521,2000,2049,2181,2200-2300,2375,2535,3127-3128,3300-3310,3389,\\\n                    4443-4444,5000-5001,5432,5900-5901,5432,5984,6000,6370-6380,6984,7000-7010,\\\n                    8000-8200,8443-8449,8880-8900,9000-9001,9043,9080-9100,9200-9210,9300,9668,\\\n                    9876,9990-10000,10080,11211,12345,16379,18080,20000-20010,22220-23000,26379,\\\n                    27010-27020,33060,50070', self.host]\n            out1, err1 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            out2, err2 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            pattern = re.compile('[0-9]+/tcp')\n            list1 = re.findall(pattern, bytes.decode(out1))\n            list2 = re.findall(pattern, bytes.decode(out2))\n            listres = list(set(list1).union(set(list2)))\n            for item in listres:\n                ports.append(item.replace('/tcp', ''))\n            return ports\n        except Exception as e:\n            return {}\n\n    def useMasscanUDP(self):\n        try:\n            ports = list()\n            cmd = ['masscan', '-sS', '-Pn', '-pU:20-25,79,110,123,137-139,161,180,513-514,559,666,999,\\\n                    1011-1032,1042-1054,1200-1201,1342-1349,2000-2002,3333,6666,26274,26374,26444,26573,\\\n                    27184,27444,29589,29891,30103,31320-31340,34555,35555', self.host]\n            out1, err1 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            out2, err2 = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            pattern = re.compile('[0-9]+/udp')\n            list1 = re.findall(pattern, bytes.decode(out1))\n            list2 = re.findall(pattern, bytes.decode(out2))\n            listres = list(set(list1).union(set(list2)))\n            for item in listres:\n                ports.append(item.replace('/udp', ''))\n            return ports\n        except Exception as e:\n            return {}\n\n    def useNmapServTCP(self, port):\n        try:\n            cmd = ['nmap', '-sV', '-Pn', '--scan-delay', '2',\n                   '--host-timeout', '2m', '--version-intensity', '6', self.host, '-p', port]\n            out, err = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            service = bytes.decode(out)\n            return service\n        except Exception as e:\n            cmd = ['nmap', '--host-timeout', '1m', self.host, '-p', port]\n            out, err = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            service = bytes.decode(out)\n            return service\n\n    def useNmapServUDP(self, port):\n        try:\n            cmd = ['nmap', '-sV', '-Pn', '--scan-delay', '2',\n                   '--host-timeout', '2m', '--version-intensity', '6', '-sU', '-pU:{}'.format(port), self.host]\n            out, err = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            service = bytes.decode(out)\n            return service\n        except Exception as e:\n            cmd = ['nmap', '--host-timeout', '1m', '-sU', '-pU:{}'.format(port), self.host]\n            out, err = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE).communicate()\n            service = bytes.decode(out)\n            return service",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport re\n\nclass SubprocessScanner:\n    def __init__(self, host):\n        self.host = host\n\n    def useMasscanTCP(self):\n        try:\n            command = f\"masscan {self.host} -p1-65535 --rate=1000 --wait=0 -oG -\"\n            result = subprocess.run(command, shell=True, capture_output=True, text=True)\n            ports = []\n            for line in result.stdout.splitlines():\n                if \"Ports:\" in line:\n                    parts = line.split()\n                    for part in parts:\n                        if \"/tcp//\" in part:\n                            port = part.split(\"/\")[0]\n                            ports.append(port)\n            return ports\n        except Exception as e:\n            return []\n\n    def useMasscanUDP(self):\n        try:\n            command = f\"masscan {self.host} -pU:1-65535 --rate=1000 --wait=0 -oG -\"\n            result = subprocess.run(command, shell=True, capture_output=True, text=True)\n            ports = []\n            for line in result.stdout.splitlines():\n                if \"Ports:\" in line:\n                    parts = line.split()\n                    for part in parts:\n                        if \"/udp//\" in part:\n                            port = part.split(\"/\")[0]\n                            ports.append(port)\n            return ports\n        except Exception as e:\n            return []\n\n    def useNmapServTCP(self, port):\n        try:\n            command = f\"nmap -sV -p {port} {self.host}\"\n            result = subprocess.run(command, shell=True, capture_output=True, text=True)\n            service_info = \"\"\n            for line in result.stdout.splitlines():\n                if f\"{port}/tcp\" in line:\n                    service_info = line.strip()\n                    break\n            return service_info\n        except Exception as e:\n            return \"\"\n\n    def useNmapServUDP(self, port):\n        try:\n            command = f\"nmap -sU -sV -p {port} {self.host}\"\n            result = subprocess.run(command, shell=True, capture_output=True, text=True)\n            service_info = \"\"\n            for line in result.stdout.splitlines():\n                if f\"{port}/udp\" in line:\n                    service_info = line.strip()\n                    break\n            return service_info\n        except Exception as e:\n            return \"\""
  },
  {
    "Task_id": 28,
    "Instruction": "Task Description: Implement a class that interacts with Alibaba Cloud's Flash Recognizer service to convert audio files into text with timing information using HTTP client functionality.\n\nClass Description: AliRecognitionService is a client for Alibaba Cloud's speech recognition API that processes audio files and returns recognition results with timestamps.\n\nAttributes:\nALI_ACCESS_AKID: str - Alibaba Cloud access key ID for authentication\nALI_ACCESS_AKKEY: str - Alibaba Cloud access key secret for authentication\nALI_APP_KEY: str - Application key for the recognition service\ntoken: str - Authentication token obtained using access credentials\nformat: str - Audio file format (default: \"wav\")\nsampleRate: int - Audio sample rate in Hz (default: 16000)\nurl: str - API endpoint URL for the recognition service\n\nMethods:\n__init__(access_key_id: str, access_key_secret: str, app_key: str) -> None - Initializes the recognition service with authentication credentials and application key\nprocess(audioFile: str) -> List[AliRecognitionResult] - Processes an audio file and returns a list of recognition results containing text with begin/end timestamps\n\nHelper Class:\nAliRecognitionResult: A data class to store recognition results with text and timing information\n__init__(text: str, begin_time: int, end_time: int) -> None - Initializes a recognition result with text and its timing information",
    "Canonical_solution": "import http.client\nimport json\nfrom typing import List\n\nclass AliRecognitionResult:\n    def __init__(self, text, begin_time, end_time):\n        self.text = text\n        self.begin_time = begin_time\n        self.end_time = end_time\n\nclass AliRecognitionService:\n    def __init__(self, access_key_id: str, access_key_secret: str, app_key: str):\n        self.ALI_ACCESS_AKID = access_key_id\n        self.ALI_ACCESS_AKKEY = access_key_secret\n        self.ALI_APP_KEY = app_key\n        self.token = getToken(self.ALI_ACCESS_AKID, self.ALI_ACCESS_AKKEY)\n        self.format = \"wav\"\n        self.sampleRate = 16000\n        self.url = 'https://nls-gateway-cn-shanghai.aliyuncs.com/stream/v1/FlashRecognizer'\n\n    def process(self, audioFile: str) -> List[AliRecognitionResult]:\n        result_list = []\n        request = self.url + '?appkey=' + self.ALI_APP_KEY\n        request = request + '&token=' + self.token\n        request = request + '&format=' + self.format\n        request = request + '&sample_rate=' + str(self.sampleRate)\n\n        with open(audioFile, mode='rb') as f:\n            audioContent = f.read()\n        \n        host = 'nls-gateway-cn-shanghai.aliyuncs.com'\n        httpHeaders = {'Content-Length': len(audioContent)}\n        \n        conn = http.client.HTTPSConnection(host)\n        conn.request(method='POST', url=request, body=audioContent, headers=httpHeaders)\n        response = conn.getresponse()\n        \n        body = response.read()\n        try:\n            body = json.loads(body)\n            status = body['status']\n            if status == 20000000:\n                result = body['flash_result']\n                if 'sentences' in result:\n                    for sentence in result['sentences']:\n                        result_list.append(\n                            AliRecognitionResult(sentence['text'], sentence['begin_time'], sentence['end_time']))\n        except ValueError:\n            print('The response is not json format string')\n        finally:\n            conn.close()\n        return result_list",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nimport json\nimport time\nfrom dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass AliRecognitionResult:\n    text: str\n    begin_time: int\n    end_time: int\n\nclass AliRecognitionService:\n    ALI_ACCESS_AKID: str\n    ALI_ACCESS_AKKEY: str\n    ALI_APP_KEY: str\n    token: str\n    format: str = \"wav\"\n    sampleRate: int = 16000\n    url: str = \"https://nls-gateway.cn-shanghai.aliyuncs.com/stream/v1/asr\"\n\n    def __init__(self, access_key_id: str, access_key_secret: str, app_key: str):\n        self.ALI_ACCESS_AKID = access_key_id\n        self.ALI_ACCESS_AKKEY = access_key_secret\n        self.ALI_APP_KEY = app_key\n        self.token = self._get_token()\n\n    def _get_token(self) -> str:\n        token_url = \"https://nls-meta.cn-shanghai.aliyuncs.com\"\n        headers = {\"Content-Type\": \"application/json\"}\n        payload = {\n            \"AccessKeyId\": self.ALI_ACCESS_AKID,\n            \"AccessKeySecret\": self.ALI_ACCESS_AKKEY\n        }\n        response = requests.post(token_url, headers=headers, json=payload)\n        if response.status_code != 200:\n            raise Exception(f\"Failed to get token: {response.text}\")\n        return response.json().get(\"Token\", {}).get(\"Id\")\n\n    def process(self, audioFile: str) -> List[AliRecognitionResult]:\n        headers = {\n            \"X-NLS-Token\": self.token,\n            \"Content-Type\": \"application/octet-stream\",\n            \"appkey\": self.ALI_APP_KEY\n        }\n        params = {\n            \"format\": self.format,\n            \"sample_rate\": self.sampleRate,\n            \"enable_words\": \"true\"\n        }\n        with open(audioFile, 'rb') as audio_data:\n            response = requests.post(self.url, headers=headers, params=params, data=audio_data)\n        if response.status_code != 200:\n            raise Exception(f\"Recognition failed: {response.text}\")\n        result = response.json()\n        if result.get(\"status\") != 20000000:\n            raise Exception(f\"Recognition error: {result.get('status_text')}\")\n        sentences = result.get(\"result\", {}).get(\"sentences\", [])\n        return [\n            AliRecognitionResult(\n                text=s.get(\"text\", \"\"),\n                begin_time=s.get(\"begin_time\", 0),\n                end_time=s.get(\"end_time\", 0)\n            ) for s in sentences\n        ]"
  },
  {
    "Task_id": 280,
    "Instruction": "Task Description: Implement a function to execute system commands and handle input/output communication between processes using pipes.\n\nFunction Description: The function `runCmd` executes a system command in a subprocess and facilitates communication via pipes. It can handle both cases where input data is provided to the subprocess and where no input is required. The function captures the standard output, standard error, and return code of the executed command.\n\nInput:\n- `cmd`: [List or String] - The command to be executed, either as a list of arguments or a single string.\n- `data`: [Bytes, optional] - Input data to be sent to the subprocess's standard input. Defaults to None if no input is required.\n\nOutput:\n- A tuple containing:\n  - `stdout`: [String] - The decoded standard output of the command.\n  - `stderr`: [String] - The decoded standard error of the command.\n  - `returncode`: [Integer] - The return code of the executed command.",
    "Canonical_solution": "import sys\nimport subprocess\n\ndef runCmd(cmd, data=None):\n    if input is None:\n        p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate()\n    else:\n        p = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        stdout, stderr = p.communicate(input=data)\n\n    return stdout.decode('utf-8'), stderr.decode('utf-8'), p.returncode",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport sys\n\ndef runCmd(cmd, data=None):\n    if isinstance(cmd, str):\n        cmd = cmd.split()\n    \n    process = subprocess.Popen(\n        cmd,\n        stdin=subprocess.PIPE if data is not None else None,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE\n    )\n    \n    stdout, stderr = process.communicate(input=data)\n    \n    return (\n        stdout.decode('utf-8', errors='replace') if stdout else '',\n        stderr.decode('utf-8', errors='replace') if stderr else '',\n        process.returncode\n    )"
  },
  {
    "Task_id": 281,
    "Instruction": "Task Description: Create a function that runs a Python subprocess with pipe communication, compares the output with expected results from a JSON file, and asserts their equality.\n\nFunction Description: The function `run_subprocess_with_pipe` executes a Python subprocess with given arguments, captures its output through a pipe, and compares this output with expected results loaded from a JSON file. The comparison includes checking the length of the results and verifying that all elements match between the expected and actual outputs.\n\nInput:\n- `expected_file`: [str] - Path to the JSON file containing the expected output data.\n- `*args`: [variable arguments] - Additional command-line arguments to pass to the subprocess.\n\nOutput: None (The function performs assertions to validate the output against the expected results. If assertions fail, an AssertionError is raised.)",
    "Canonical_solution": "import sys\nimport subprocess\nimport json\n\ndef run_subprocess_with_pipe(expected_file, *args):\n    dg = subprocess.Popen([sys.executable] + list(args), stdout=subprocess.PIPE)\n    stdout, _ = dg.communicate()\n    expected = json.load(open(expected_file))\n    result = json.loads(stdout.decode())\n    \n    assert len(expected) == len(result)\n    assert all(r in result for r in expected)\n    assert all(r in expected for r in result)",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport json\n\ndef run_subprocess_with_pipe(expected_file, *args):\n    process = subprocess.Popen(\n        ['python', *args],\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        text=True\n    )\n    stdout, stderr = process.communicate()\n    \n    with open(expected_file, 'r') as f:\n        expected_output = json.load(f)\n    \n    actual_output = stdout.strip().split('\\n')\n    \n    assert len(actual_output) == len(expected_output), \"Output length mismatch\"\n    for actual, expected in zip(actual_output, expected_output):\n        assert actual == expected, f\"Output mismatch: expected '{expected}', got '{actual}'\""
  },
  {
    "Task_id": 286,
    "Instruction": "Task Description: Implement a class that interfaces with the Phonetisaurus G2P (Grapheme-to-Phoneme) tool to convert words into their phonetic pronunciations using a pre-trained FST (Finite State Transducer) model. The class should handle both single words and lists of words, and provide options for retrieving multiple pronunciation candidates.\n\nClass Description: The PhonetisaurusG2P class provides an interface to the Phonetisaurus G2P tool, allowing for the conversion of words to their phonetic pronunciations using a specified FST model. It supports both single-word and batch processing, with optional n-best results.\n\nAttributes:\n- fst_model: [str] - The file path to the pre-trained FST model used for G2P conversion.\n- nbest: [int/None] - The number of best pronunciation candidates to return for each word (optional).\n- _logger: [logging.Logger] - Logger instance for debugging and error reporting.\n\nMethods:\n- __init__(fst_model=None, nbest=None) -> None - Initializes the G2P converter with the path to the FST model and optional n-best setting.\n- execute(fst_model, input, is_file=False, nbest=None) -> dict - Executes the phonetisaurus-g2p command with the given input and returns the pronunciation results as a dictionary.\n- _translate_word(word) -> dict - Internal method to translate a single word to phonemes.\n- _translate_words(words) -> dict - Internal method to translate a list of words to phonemes using a temporary file.\n- translate(words) -> dict - Main interface method that translates either a single word or a list of words to their phonetic pronunciations.",
    "Canonical_solution": "import os\nimport re\nimport subprocess\nimport tempfile\nimport logging\nimport yaml\n\nclass PhonetisaurusG2P:\n    PATTERN = re.compile(r'^(?P<word>.+)\\t(?P<precision>\\d+\\.\\d+)\\t<s> ' +\n                         r'(?P<pronounciation>.*) </s>', re.MULTILINE)\n\n    def __init__(self, fst_model=None, nbest=None):\n        self._logger = logging.getLogger(__name__)\n        self.fst_model = os.path.abspath(fst_model)\n        self._logger.debug(\"Using FST model: '%s'\", self.fst_model)\n        self.nbest = nbest\n        if self.nbest is not None:\n            self._logger.debug(\"Will use the %d best results.\", self.nbest)\n\n    def execute(self, fst_model, input, is_file=False, nbest=None):\n        cmd = ['phonetisaurus-g2p',\n               '--model=%s' % fst_model,\n               '--input=%s' % input,\n               '--words']\n\n        if is_file:\n            cmd.append('--isfile')\n\n        if nbest is not None:\n            cmd.extend(['--nbest=%d' % nbest])\n\n        cmd = [str(x) for x in cmd]\n        try:\n            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            stdoutdata, stderrdata = proc.communicate()\n        except OSError:\n            self._logger.error(\"Error occured while executing command '%s'\",\n                             ' '.join(cmd), exc_info=True)\n            raise\n\n        if stderrdata:\n            for line in stderrdata.splitlines():\n                message = line.strip()\n                if message:\n                    self._logger.debug(message)\n\n        if proc.returncode != 0:\n            self._logger.error(\"Command '%s' return with exit status %d\",\n                             ' '.join(cmd), proc.returncode)\n            raise OSError(\"Command execution failed\")\n\n        result = {}\n        if stdoutdata is not None:\n            for word, precision, pronounc in self.PATTERN.findall(stdoutdata):\n                if word not in result:\n                    result[word] = []\n                result[word].append(pronounc)\n        return result\n\n    def _translate_word(self, word):\n        return self.execute(self.fst_model, word, nbest=self.nbest)\n\n    def _translate_words(self, words):\n        with tempfile.NamedTemporaryFile(suffix='.g2p', delete=False) as f:\n            for word in words:\n                f.write(\"%s\\n\" % word)\n            tmp_fname = f.name\n        output = self.execute(self.fst_model, tmp_fname, is_file=True,\n                              nbest=self.nbest)\n        os.remove(tmp_fname)\n        return output\n\n    def translate(self, words):\n        if type(words) is str or len(words) == 1:\n            self._logger.debug('Converting single word to phonemes')\n            output = self._translate_word(words if type(words) is str\n                                          else words[0])\n        else:\n            self._logger.debug('Converting %d words to phonemes', len(words))\n            output = self._translate_words(words)\n        self._logger.debug('G2P conversion returned phonemes for %d words',\n                           len(output))\n        return output",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import os\nimport tempfile\nimport logging\nimport subprocess\n\nclass PhonetisaurusG2P:\n    def __init__(self, fst_model=None, nbest=None):\n        self.fst_model = fst_model\n        self.nbest = nbest\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n    def execute(self, fst_model, input, is_file=False, nbest=None):\n        cmd = ['phonetisaurus-g2p', '--model=' + fst_model]\n        if nbest is not None:\n            cmd.extend(['--nbest=' + str(nbest)])\n        if is_file:\n            cmd.extend(['--input=' + input])\n        else:\n            cmd.extend(['--word=' + input])\n        try:\n            output = subprocess.check_output(cmd, stderr=subprocess.PIPE).decode('utf-8')\n            results = {}\n            for line in output.splitlines():\n                parts = line.strip().split('\\t')\n                if len(parts) >= 2:\n                    word = parts[0]\n                    pronunciation = parts[1]\n                    if word not in results:\n                        results[word] = []\n                    results[word].append(pronunciation)\n            return results\n        except subprocess.CalledProcessError as e:\n            self._logger.error(f\"Phonetisaurus execution failed: {e.stderr.decode('utf-8')}\")\n            raise\n\n    def _translate_word(self, word):\n        return self.execute(self.fst_model, word, is_file=False, nbest=self.nbest)\n\n    def _translate_words(self, words):\n        with tempfile.NamedTemporaryFile(mode='w+', delete=False) as f:\n            f.write('\\n'.join(words))\n            f.flush()\n            result = self.execute(self.fst_model, f.name, is_file=True, nbest=self.nbest)\n        os.unlink(f.name)\n        return result\n\n    def translate(self, words):\n        if isinstance(words, str):\n            return self._translate_word(words)\n        elif isinstance(words, list):\n            return self._translate_words(words)\n        else:\n            raise ValueError(\"Input must be a string or a list of strings\")"
  },
  {
    "Task_id": 287,
    "Instruction": "Task Description: Create a function that inspects the value of a Python object in a running process by its memory address using GDB debugger through subprocess communication.\n\nFunction Description: The function uses GDB to attach to a running Python process and inspect the value of a Python object at a specified memory address. It establishes inter-process communication using subprocess.PIPE to capture GDB's output and parse the result.\n\nInput:\n- pid: [int] - Process ID of the target Python process\n- address: [str/int] - Memory address of the Python object to inspect (can be hex string or integer)\n\nOutput:\n- [str] - The string representation of the inspected Python object as returned by GDB, or None if the address couldn't be resolved",
    "Canonical_solution": "import sys\nimport subprocess\n\nencoding = sys.getdefaultencoding()\n\ndef inspect(pid, address):\n    \"Return the value of an object in a given process at the specified address\"\n    cmd = ' '.join([\n        'gdb --quiet -p %s -batch' % pid,\n        '-eval-command=\"print (PyObject *)%s\"' % address,\n    ])\n    p = subprocess.Popen(cmd, shell=True, stdout=subprocess.PIPE)\n    for line in p.communicate()[0].decode(encoding).split('\\n'):\n        if line.startswith('$1 = '):\n            return line[5:]",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport re\n\ndef inspect_python_object(pid, address):\n    try:\n        if isinstance(address, str):\n            if address.startswith('0x'):\n                addr = address\n            else:\n                addr = f'0x{address}'\n        else:\n            addr = f'0x{address:x}'\n\n        gdb_cmds = [\n            f'attach {pid}',\n            f'python import sys; import gdb; val = gdb.parse_and_eval(\"*(PyObject*){addr}\"); print(sys._debugmallocstats() if {addr} == 0 else val)',\n            'detach',\n            'quit'\n        ]\n\n        process = subprocess.Popen(['gdb', '--batch', '--nh', '-q'],\n                                 stdin=subprocess.PIPE,\n                                 stdout=subprocess.PIPE,\n                                 stderr=subprocess.PIPE,\n                                 universal_newlines=True)\n        stdout, _ = process.communicate('\\n'.join(gdb_cmds))\n\n        if 'Cannot access memory at address' in stdout:\n            return None\n\n        match = re.search(r'\\$1 = (.+)', stdout)\n        if match:\n            return match.group(1).strip()\n        return None\n    except:\n        return None"
  },
  {
    "Task_id": 289,
    "Instruction": "Task Description: Implement functions to retrieve clipboard text content on different operating systems using subprocess pipes for inter-process communication.\n\nFunction Description: The functions use system clipboard utilities through subprocess pipes to get text content from the clipboard, handling platform-specific requirements and text encoding conversions.\n\nInput: None (both functions operate on system clipboard content)\n\nOutput: \n- osx_clipboard_get: Returns str - The decoded text content from macOS clipboard with line endings normalized\n- wayland_clipboard_get: Returns str - The decoded text content from Wayland clipboard, or raises exceptions for non-Wayland environments or missing dependencies\n\nMethods:\n1. osx_clipboard_get() -> str - Retrieves clipboard text on macOS using pbpaste command, converts line endings, and decodes the output\n2. wayland_clipboard_get() -> str - Retrieves clipboard text on Wayland using wl-paste command, with environment checks and error handling",
    "Canonical_solution": "import os\nimport subprocess\nimport IPython.utils.py3compat as py3compat\n\ndef osx_clipboard_get() -> str:\n    \"\"\" Get the clipboard's text on OS X.\n    \"\"\"\n    p = subprocess.Popen(['pbpaste', '-Prefer', 'ascii'],\n        stdout=subprocess.PIPE)\n    bytes_, stderr = p.communicate()\n    # Text comes in with old Mac \\r line endings. Change them to \\n.\n    bytes_ = bytes_.replace(b'\\r', b'\\n')\n    text = py3compat.decode(bytes_)\n    return text\n\ndef wayland_clipboard_get():\n    \"\"\"Get the clipboard's text under Wayland using wl-paste command.\n\n    This requires Wayland and wl-clipboard installed and running.\n    \"\"\"\n    if os.environ.get(\"XDG_SESSION_TYPE\") != \"wayland\":\n        raise TryNext(\"wayland is not detected\")\n\n    try:\n        with subprocess.Popen([\"wl-paste\"], stdout=subprocess.PIPE) as p:\n            raw, err = p.communicate()\n            if p.wait():\n                raise TryNext(err)\n    except FileNotFoundError as e:\n        raise TryNext(\n            \"Getting text from the clipboard under Wayland requires the wl-clipboard \"\n            \"extension: https://github.com/bugaevc/wl-clipboard\"\n        ) from e\n\n    if not raw:\n        raise ClipboardEmpty\n\n    try:\n        text = py3compat.decode(raw)\n    except UnicodeDecodeError as e:\n        raise ClipboardEmpty from e\n\n    return text",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport sys\n\ndef osx_clipboard_get() -> str:\n    try:\n        result = subprocess.run(['pbpaste'], capture_output=True, check=True)\n        text = result.stdout.decode('utf-8').replace('\\r\\n', '\\n').replace('\\r', '\\n')\n        return text\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(\"Failed to access macOS clipboard\") from e\n    except FileNotFoundError as e:\n        raise RuntimeError(\"pbpaste command not found - not running on macOS?\") from e\n\ndef wayland_clipboard_get() -> str:\n    if 'WAYLAND_DISPLAY' not in os.environ and 'XDG_SESSION_TYPE' not in os.environ:\n        raise RuntimeError(\"Not running in a Wayland session\")\n    try:\n        result = subprocess.run(['wl-paste'], capture_output=True, check=True)\n        return result.stdout.decode('utf-8')\n    except subprocess.CalledProcessError as e:\n        raise RuntimeError(\"Failed to access Wayland clipboard\") from e\n    except FileNotFoundError as e:\n        raise RuntimeError(\"wl-paste command not found - install wl-clipboard package\") from e"
  },
  {
    "Task_id": 290,
    "Instruction": "Task Description: Implement a class that interfaces with the Phonetisaurus G2P (Grapheme-to-Phoneme) tool to convert words to their phonetic pronunciations using a pre-trained FST (Finite State Transducer) model.\n\nClass Description: The PhonetisaurusG2P class provides an interface to the Phonetisaurus G2P tool, allowing for the conversion of words or files containing words to their phonetic pronunciations. It handles process execution, error logging, and output parsing.\n\nAttributes:\n- fst_model: [str] - The file path to the pre-trained FST model used for grapheme-to-phoneme conversion.\n- nbest: [int/None] - The number of best pronunciations to return for each word. If None, only the top pronunciation is returned.\n- _logger: [logging.Logger] - A logger instance for debugging and error logging.\n\nMethods:\n- __init__(fst_model=None, nbest=None) -> [None] - Initializes the PhonetisaurusG2P instance with the path to the FST model and the number of best pronunciations to return.\n  - Input Parameters:\n    - fst_model: [str] - Path to the FST model file.\n    - nbest: [int/None] - Number of best pronunciations to return (default: None).\n  - Return Type: None\n\n- execute(input, is_file=False) -> [dict] - Executes the Phonetisaurus G2P tool to convert the input word or file to phonetic pronunciations.\n  - Input Parameters:\n    - input: [str] - The input word or file path containing words to be converted.\n    - is_file: [bool] - Flag indicating whether the input is a file (default: False).\n  - Return Type: dict - A dictionary where keys are words and values are lists of pronunciations. Each pronunciation is a string.\n  - Description: This method uses subprocess.Popen to run the Phonetisaurus G2P tool, captures stdout and stderr, parses the output, and returns the results in a structured format. Errors are logged, and exceptions are raised for execution failures.",
    "Canonical_solution": "import os\nimport re\nimport subprocess\nimport logging\n\nclass PhonetisaurusG2P:\n    PATTERN = re.compile(r'^(?P<word>.+)\\t(?P<precision>\\d+\\.\\d+)\\t<s> ' +\n                         r'(?P<pronounciation>.*) </s>', re.MULTILINE)\n\n    def __init__(self, fst_model=None, nbest=None):\n        self._logger = logging.getLogger(__name__)\n        self.fst_model = os.path.abspath(fst_model)\n        self.nbest = nbest\n\n    def execute(self, input, is_file=False):\n        cmd = ['phonetisaurus-g2p',\n               '--model=%s' % self.fst_model,\n               '--input=%s' % input,\n               '--words']\n\n        if is_file:\n            cmd.append('--isfile')\n\n        if self.nbest is not None:\n            cmd.extend(['--nbest=%d' % self.nbest])\n\n        cmd = [str(x) for x in cmd]\n        try:\n            proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE)\n            stdoutdata, stderrdata = proc.communicate()\n        except OSError:\n            self._logger.error(\"Error occured while executing command '%s'\",\n                             ' '.join(cmd), exc_info=True)\n            raise\n\n        if stderrdata:\n            for line in stderrdata.splitlines():\n                message = line.strip()\n                if message:\n                    self._logger.debug(message)\n\n        if proc.returncode != 0:\n            self._logger.error(\"Command '%s' return with exit status %d\",\n                             ' '.join(cmd), proc.returncode)\n            raise OSError(\"Command execution failed\")\n\n        result = {}\n        if stdoutdata is not None:\n            for word, precision, pronounc in self.PATTERN.findall(stdoutdata):\n                if word not in result:\n                    result[word] = []\n                result[word].append(pronounc)\n        return result",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import logging\nimport subprocess\nimport os\n\nclass PhonetisaurusG2P:\n    def __init__(self, fst_model=None, nbest=None):\n        self.fst_model = fst_model\n        self.nbest = nbest\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n    def execute(self, input, is_file=False):\n        if not os.path.exists(self.fst_model):\n            raise FileNotFoundError(f\"FST model file not found: {self.fst_model}\")\n\n        cmd = ['phonetisaurus-g2p']\n        if self.nbest is not None:\n            cmd.extend(['--nbest', str(self.nbest)])\n        cmd.extend(['--model', self.fst_model])\n        if is_file:\n            if not os.path.exists(input):\n                raise FileNotFoundError(f\"Input file not found: {input}\")\n            cmd.extend(['--input', input])\n        else:\n            cmd.extend(['--word', input])\n\n        try:\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True\n            )\n            stdout, stderr = process.communicate()\n\n            if process.returncode != 0:\n                self._logger.error(f\"Phonetisaurus G2P failed: {stderr}\")\n                raise RuntimeError(f\"Phonetisaurus G2P execution failed: {stderr}\")\n\n            return self._parse_output(stdout)\n        except Exception as e:\n            self._logger.error(f\"Error executing Phonetisaurus G2P: {str(e)}\")\n            raise\n\n    def _parse_output(self, output):\n        results = {}\n        for line in output.splitlines():\n            if not line.strip():\n                continue\n            parts = line.split('\\t')\n            if len(parts) < 2:\n                continue\n            word = parts[0].strip()\n            pronunciation = parts[1].strip()\n            if word not in results:\n                results[word] = []\n            results[word].append(pronunciation)\n        return results"
  },
  {
    "Task_id": 297,
    "Instruction": "Task Description: Create a function that executes a system command in a subprocess, captures its output and error streams, and handles potential errors appropriately.\n\nFunction Description: The function `run_cmd` executes a given system command as a subprocess, with options for verbose output. It captures both stdout and stderr streams, provides real-time output when in verbose mode, and raises an exception if the command fails (non-zero return code). The function ensures proper process cleanup and returns the command's output and error streams.\n\nInput:\n- command: List[str] - A list of strings representing the command and its arguments to be executed\n- verbose: bool (optional, default=False) - If True, prints real-time output of the command execution\n\nOutput:\n- Returns: Tuple[bytes, bytes] - A tuple containing the stdout and stderr output from the command\n- Raises: Exception - If the command returns a non-zero exit code, includes the return code and output streams in the exception message",
    "Canonical_solution": "import subprocess\nimport sys\n\ndef run_cmd(command, verbose=False):\n    if verbose:\n        print(\"Running command:\", \" \".join(command))\n    proc = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n    if verbose:\n        while proc.poll() is None:\n            string = b\"\"\n            for c in iter(lambda: proc.stdout.read(1), string):\n                c = c.decode(\"ascii\")\n                sys.stdout.write(c)\n\n        sys.stdout.flush()\n\n    proc.wait()\n    if proc.returncode != 0:\n        out, err = proc.communicate()\n        raise Exception(\n            \"The training could not be completed (returncode=%i): %s %s\"\n            % (proc.returncode, out, err)\n        )\n\n    out, err = proc.communicate()",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport sys\n\ndef run_cmd(command, verbose=False):\n    process = subprocess.Popen(\n        command,\n        stdout=subprocess.PIPE,\n        stderr=subprocess.PIPE,\n        bufsize=1,\n        universal_newlines=False\n    )\n\n    stdout = []\n    stderr = []\n\n    while True:\n        output = process.stdout.readline()\n        error = process.stderr.readline()\n\n        if output == b'' and error == b'' and process.poll() is not None:\n            break\n\n        if output:\n            stdout.append(output)\n            if verbose:\n                sys.stdout.buffer.write(output)\n                sys.stdout.flush()\n\n        if error:\n            stderr.append(error)\n            if verbose:\n                sys.stderr.buffer.write(error)\n                sys.stderr.flush()\n\n    stdout_bytes = b''.join(stdout)\n    stderr_bytes = b''.join(stderr)\n    return_code = process.poll()\n\n    if return_code != 0:\n        raise Exception(\n            f\"Command failed with return code {return_code}\\n\"\n            f\"stdout: {stdout_bytes.decode('utf-8', errors='replace')}\\n\"\n            f\"stderr: {stderr_bytes.decode('utf-8', errors='replace')}\"\n        )\n\n    return (stdout_bytes, stderr_bytes)"
  },
  {
    "Task_id": 298,
    "Instruction": "Task Description: Implement a class to collect and analyze OSSEC (Open Source Host-based Intrusion Detection System) agent states using subprocess communication.\n\nClass Description: The OssecCollector class interacts with the OSSEC agent control utility to retrieve agent status information and parse the results into a structured format. It handles command execution with optional sudo privileges and processes the output to count agent states.\n\nAttributes:\n- bin_path: [str] - Path to the OSSEC agent_control binary (default: '/var/ossec/bin/agent_control')\n- use_sudo: [bool] - Flag indicating whether to use sudo for command execution (default: True)\n- sudo_cmd: [str] - Path to the sudo binary (default: '/usr/bin/sudo')\n\nMethods:\n- __init__(bin_path: str = '/var/ossec/bin/agent_control', use_sudo: bool = True, sudo_cmd: str = '/usr/bin/sudo') -> None - Initializes the collector with configuration paths\n- collect() -> dict - Executes the agent control command, processes the output, and returns a dictionary counting agent states by status\n\nThe implementation should use subprocess.PIPE for inter-process communication and handle potential execution errors appropriately.",
    "Canonical_solution": "import subprocess\nimport re\nfrom diamond.collector import str_to_bool\n\nclass OssecCollector:\n    def __init__(self, bin_path='/var/ossec/bin/agent_control', use_sudo=True, sudo_cmd='/usr/bin/sudo'):\n        self.bin_path = bin_path\n        self.use_sudo = use_sudo\n        self.sudo_cmd = sudo_cmd\n\n    def collect(self):\n        command = [self.bin_path, '-l']\n\n        if str_to_bool(self.use_sudo):\n            command.insert(0, self.sudo_cmd)\n\n        try:\n            p = subprocess.Popen(command, stdout=subprocess.PIPE)\n            res = p.communicate()[0]\n        except Exception as e:\n            raise Exception('Unable to exec cmd: %s, because %s' % (' '.join(command), str(e)))\n\n        if res == '':\n            raise Exception('Empty result from exec cmd: %s' % (' '.join(command)))\n\n        states = {}\n        for line in res.split(\"\\n\"):\n            if not line.startswith('   ID: '):\n                continue\n            fragments = line.split(',')\n            state = fragments[-1].lstrip()\n            if state not in states:\n                states[state] = 1\n            else:\n                states[state] += 1\n\n        return states",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport re\n\nclass OssecCollector:\n    def __init__(self, bin_path='/var/ossec/bin/agent_control', use_sudo=True, sudo_cmd='/usr/bin/sudo'):\n        self.bin_path = bin_path\n        self.use_sudo = use_sudo\n        self.sudo_cmd = sudo_cmd\n\n    def collect(self):\n        cmd = [self.bin_path, '-l']\n        if self.use_sudo:\n            cmd = [self.sudo_cmd] + cmd\n\n        try:\n            result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n            if result.returncode != 0:\n                return {'error': result.stderr.strip()}\n\n            output = result.stdout\n            state_counts = {\n                'Active': 0,\n                'Disconnected': 0,\n                'Never connected': 0,\n                'Pending': 0,\n                'Other': 0\n            }\n\n            for line in output.splitlines():\n                if not line.strip():\n                    continue\n                match = re.search(r'Status: (.+?)(?:,|$)', line)\n                if match:\n                    status = match.group(1).strip()\n                    if status == 'Active':\n                        state_counts['Active'] += 1\n                    elif status == 'Disconnected':\n                        state_counts['Disconnected'] += 1\n                    elif status == 'Never connected':\n                        state_counts['Never connected'] += 1\n                    elif status == 'Pending':\n                        state_counts['Pending'] += 1\n                    else:\n                        state_counts['Other'] += 1\n\n            return state_counts\n\n        except Exception as e:\n            return {'error': str(e)}"
  },
  {
    "Task_id": 30,
    "Instruction": "Task Description: Implement a Python class to interact with the DNSPod API for managing DNS records, including querying domain information, listing records, and updating DNS records.\n\nClass Description: DNSPodAPI is a class that provides methods to interact with DNSPod's API for DNS management. It handles authentication, request construction, and response processing for various DNS operations.\n\nAttributes:\n\nSITE: str - The DNSPod API endpoint (dnsapi.cn)\nMETHOD: str - Default HTTP method for requests (POST)\nTOKEN_PARAM: str - Parameter name for authentication token (login_token)\nDEFAULT: str - Default value for record line (\"\u9ed8\u8ba4\")\nLENGTH: str - Parameter name for response length (\"length\")\nID: str - DNSPod API token ID\nTOKEN: str - DNSPod API token key\nPROXY: str - Optional proxy server address\nTTL: int - Time-to-live value for DNS records\ndomain_list: dict - Cache of domain names and their IDs\nrecords: dict - Cache of DNS records organized by domain ID\nrecord_keys: tuple - Valid fields for DNS records\n\nMethods:\n\n__init__(token_id=None, token_key=None, proxy=None, ttl=None) -> None - Initializes the DNSPodAPI instance with authentication tokens and optional proxy/ttl settings.\n\nrequest(action, param=None, **params) -> dict - Makes an authenticated API request to the specified action with given parameters. Returns parsed JSON response or raises an exception on error.\n\nget_domain_info(domain) -> tuple - Splits a domain into its main and subdomain parts, returning the domain ID and subdomain.\n\nget_domain_id(domain) -> str - Retrieves and caches the ID for a given domain name.\n\nget_records(did, **conditions) -> dict - Retrieves DNS records for a domain ID that match given conditions (e.g., name, type).\n\nupdate_record(domain, value, record_type=\"A\") -> dict - Updates or creates a DNS record for the given domain with the specified value and record type.",
    "Canonical_solution": "from http.client import HTTPSConnection\nfrom urllib.parse import urlencode\nfrom json import loads as jsondecode\nfrom os import environ\n\nclass DNSPodAPI:\n    def __init__(self, token_id=None, token_key=None, proxy=None, ttl=None):\n        self.SITE = \"dnsapi.cn\"\n        self.METHOD = \"POST\"\n        self.TOKEN_PARAM = \"login_token\"\n        self.DEFAULT = \"\u9ed8\u8ba4\"\n        self.LENGTH = \"length\"\n        \n        self.ID = token_id\n        self.TOKEN = token_key\n        self.PROXY = proxy\n        self.TTL = ttl\n        self.domain_list = {}\n        self.records = {}\n        self.record_keys = (\"id\", \"name\", \"type\", \"line\", \"line_id\", \"enabled\", \"mx\", \"value\")\n\n    def request(self, action, param=None, **params):\n        if param:\n            params.update(param)\n        params = {k: v for k, v in params.items() if v is not None}\n        params.update({self.TOKEN_PARAM: \"%s,%s\" % (self.ID, self.TOKEN), 'format': 'json', self.LENGTH: \"3000\"})\n        \n        if self.PROXY:\n            conn = HTTPSConnection(self.PROXY)\n            conn.set_tunnel(self.SITE, 443)\n        else:\n            conn = HTTPSConnection(self.SITE)\n\n        conn.request(self.METHOD, '/' + action, urlencode(params), {\n            \"Content-type\": \"application/x-www-form-urlencoded\",\n            \"User-Agent\": \"DDNS/%s (ddns@newfuture.cc)\" % environ.get(\"DDNS_VERSION\", \"1.0.0\")\n        })\n        response = conn.getresponse()\n        res = response.read().decode('utf8')\n        conn.close()\n\n        if response.status < 200 or response.status >= 300:\n            raise Exception(res)\n        else:\n            data = jsondecode(res)\n            if not data:\n                raise Exception(\"empty response\")\n            elif data.get(\"status\", {}).get(\"code\") == \"1\":\n                return data\n            else:\n                raise Exception(data.get('status', {}))\n\n    def get_domain_info(self, domain):\n        domain_split = domain.split('.')\n        sub, did = None, None\n        main = domain_split.pop()\n        while domain_split:\n            main = domain_split.pop() + '.' + main\n            did = self.get_domain_id(main)\n            if did:\n                sub = \".\".join(domain_split) or '@'\n                break\n        return did, sub\n\n    def get_domain_id(self, domain):\n        if domain in self.domain_list:\n            return self.domain_list[domain]\n        else:\n            try:\n                info = self.request('Domain.Info', domain=domain)\n                did = info.get(\"domain\", {}).get(\"id\")\n                if did:\n                    self.domain_list[domain] = did\n                    return did\n            except Exception:\n                return None\n\n    def get_records(self, did, **conditions):\n        if did not in self.records:\n            self.records[did] = {}\n            data = self.request('Record.List', domain_id=did)\n            if data:\n                for record in data.get('records'):\n                    self.records[did][record[\"id\"]] = {\n                        k: v for (k, v) in record.items() if k in self.record_keys}\n\n        records = {}\n        for (rid, record) in self.records[did].items():\n            for (k, value) in conditions.items():\n                if record.get(k) != value:\n                    break\n            else:\n                records[rid] = record\n        return records\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        domainid, sub = self.get_domain_info(domain)\n        if not domainid:\n            raise Exception(\"invalid domain: [ %s ] \" % domain)\n\n        records = self.get_records(domainid, name=sub, type=record_type)\n        result = {}\n        if records:\n            for (did, record) in records.items():\n                if record[\"value\"] != value:\n                    res = self.request('Record.Modify', \n                                     record_id=did, \n                                     record_line=record[\"line\"].replace(\"Default\", \"default\").encode(\"utf-8\"), \n                                     value=value, \n                                     sub_domain=sub, \n                                     domain_id=domainid, \n                                     record_type=record_type, \n                                     ttl=self.TTL)\n                    if res:\n                        self.records[domainid][did][\"value\"] = value\n                        result[did] = res.get(\"record\")\n                    else:\n                        result[did] = \"update fail!\\n\" + str(res)\n                else:\n                    result[did] = domain\n        else:\n            res = self.request(\"Record.Create\", \n                             domain_id=domainid, \n                             value=value,\n                             sub_domain=sub, \n                             record_type=record_type, \n                             record_line=self.DEFAULT, \n                             ttl=self.TTL)\n            if res:\n                did = res.get(\"record\")[\"id\"]\n                self.records[domainid][did] = res.get(\"record\")\n                self.records[domainid][did].update(\n                    value=value, sub_domain=sub, record_type=record_type)\n                result = res.get(\"record\")\n            else:\n                result = domain + \" created fail!\"\n        return result",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nimport json\n\nclass DNSPodAPI:\n    SITE = \"dnsapi.cn\"\n    METHOD = \"POST\"\n    TOKEN_PARAM = \"login_token\"\n    DEFAULT = \"\u9ed8\u8ba4\"\n    LENGTH = \"length\"\n    ID = \"id\"\n    TOKEN = \"token\"\n    PROXY = \"proxy\"\n    TTL = \"ttl\"\n    \n    def __init__(self, token_id=None, token_key=None, proxy=None, ttl=None):\n        self.ID = token_id\n        self.TOKEN = token_key\n        self.PROXY = proxy\n        self.TTL = ttl\n        self.domain_list = {}\n        self.records = {}\n        self.record_keys = ('id', 'name', 'type', 'value', 'line', 'ttl', 'enabled', 'mx', 'weight')\n\n    def request(self, action, param=None, **params):\n        if param is None:\n            param = {}\n        param.update(params)\n        param[self.TOKEN_PARAM] = f\"{self.ID},{self.TOKEN}\"\n        param['format'] = 'json'\n        param['lang'] = 'en'\n        headers = {'Content-Type': 'application/x-www-form-urlencoded', 'Accept': 'application/json'}\n        proxies = {'http': self.PROXY, 'https': self.PROXY} if self.PROXY else None\n        url = f\"https://{self.SITE}/{action}\"\n        response = requests.request(self.METHOD, url, data=param, headers=headers, proxies=proxies)\n        if response.status_code != 200:\n            raise Exception(f\"API request failed with status {response.status_code}\")\n        result = response.json()\n        if result.get('status', {}).get('code') != '1':\n            raise Exception(f\"API error: {result.get('status', {}).get('message')}\")\n        return result\n\n    def get_domain_info(self, domain):\n        parts = domain.split('.')\n        if len(parts) < 2:\n            raise ValueError(\"Invalid domain format\")\n        main_domain = '.'.join(parts[-2:])\n        sub_domain = '.'.join(parts[:-2]) if len(parts) > 2 else '@'\n        did = self.get_domain_id(main_domain)\n        return did, sub_domain\n\n    def get_domain_id(self, domain):\n        if domain in self.domain_list:\n            return self.domain_list[domain]\n        result = self.request('Domain.List')\n        for item in result.get('domains', []):\n            self.domain_list[item['name']] = item['id']\n        if domain not in self.domain_list:\n            raise Exception(f\"Domain not found: {domain}\")\n        return self.domain_list[domain]\n\n    def get_records(self, did, **conditions):\n        if did not in self.records:\n            result = self.request('Record.List', domain_id=did)\n            self.records[did] = result.get('records', [])\n        matched = []\n        for record in self.records[did]:\n            match = True\n            for key, value in conditions.items():\n                if key not in self.record_keys:\n                    continue\n                if str(record.get(key, '')) != str(value):\n                    match = False\n                    break\n            if match:\n                matched.append(record)\n        return matched\n\n    def update_record(self, domain, value, record_type=\"A\"):\n        did, sub_domain = self.get_domain_info(domain)\n        records = self.get_records(did, name=sub_domain, type=record_type)\n        param = {\n            'domain_id': did,\n            'sub_domain': sub_domain,\n            'record_type': record_type,\n            'record_line': self.DEFAULT,\n            'value': value\n        }\n        if self.TTL:\n            param['ttl'] = self.TTL\n        if records:\n            param['record_id'] = records[0]['id']\n            action = 'Record.Modify'\n        else:\n            action = 'Record.Create'\n        result = self.request(action, **param)\n        if did in self.records:\n            del self.records[did]\n        return result"
  },
  {
    "Task_id": 301,
    "Instruction": "Task Description: Implement a class to monitor and collect statistics about Passenger application server processes, including memory usage, CPU usage, and queue statistics.\n\nClass Description: The PassengerProcessCommunicator class is designed to interact with Passenger application server processes to gather various performance metrics. It uses subprocess communication to execute system commands and parse their outputs.\n\nAttributes:\n- config: [dict] - A configuration dictionary containing settings like command paths and sudo usage flags.\n\nMethods:\n- get_passenger_memory_stats() -> [dict] - Collects memory statistics for Apache, Nginx, and Passenger processes. Returns a dictionary with process lists and total memory usage.\n- get_passenger_cpu_usage(dict_stats: dict) -> [float] - Calculates total CPU usage percentage for all Passenger-related processes. Takes a dictionary of process stats and returns the total CPU usage.\n- get_passenger_queue_stats() -> [dict] - Gathers queue statistics from Passenger status, including top-level and application queue sizes. Returns a dictionary with queue metrics.",
    "Canonical_solution": "import subprocess\nimport re\nimport os\nfrom diamond.collector import str_to_bool\n\nclass PassengerProcessCommunicator:\n    def __init__(self, config):\n        self.config = config\n\n    def get_passenger_memory_stats(self):\n        command = [self.config[\"passenger_memory_stats_bin\"]]\n        if str_to_bool(self.config[\"use_sudo\"]):\n            command.insert(0, self.config[\"sudo_cmd\"])\n\n        try:\n            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n        except OSError:\n            return {}\n\n        if std_out is None:\n            return {}\n\n        dict_stats = {\n            \"apache_procs\": [],\n            \"nginx_procs\": [],\n            \"passenger_procs\": [],\n            \"apache_mem_total\": 0.0,\n            \"nginx_mem_total\": 0.0,\n            \"passenger_mem_total\": 0.0,\n        }\n        \n        re_colour = re.compile(\"\\x1B\\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]\")\n        re_digit = re.compile(\"^\\d\")\n        \n        apache_flag = 0\n        nginx_flag = 0\n        passenger_flag = 0\n        for raw_line in std_out.splitlines():\n            line = re_colour.sub(\"\", raw_line)\n            if \"Apache processes\" in line:\n                apache_flag = 1\n            elif \"Nginx processes\" in line:\n                nginx_flag = 1\n            elif \"Passenger processes\" in line:\n                passenger_flag = 1\n            elif re_digit.match(line):\n                line_splitted = line.split()\n                if apache_flag == 1:\n                    dict_stats[\"apache_procs\"].append(line_splitted[0])\n                    dict_stats[\"apache_mem_total\"] += float(line_splitted[4])\n                elif nginx_flag == 1:\n                    dict_stats[\"nginx_procs\"].append(line_splitted[0])\n                    dict_stats[\"nginx_mem_total\"] += float(line_splitted[4])\n                elif passenger_flag == 1:\n                    dict_stats[\"passenger_procs\"].append(line_splitted[0])\n                    dict_stats[\"passenger_mem_total\"] += float(line_splitted[3])\n            elif \"Processes:\" in line:\n                passenger_flag = 0\n                apache_flag = 0\n                nginx_flag = 0\n\n        return dict_stats\n\n    def get_passenger_cpu_usage(self, dict_stats):\n        try:\n            proc1 = subprocess.Popen(\n                [\"top\", \"-b\", \"-n\", \"2\"],\n                stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n        except OSError:\n            return -1\n\n        re_lspaces = re.compile(\"^\\s*\")\n        re_digit = re.compile(\"^\\d\")\n        overall_cpu = 0\n        for raw_line in std_out.splitlines():\n            line = re_lspaces.sub(\"\", raw_line)\n            if not re_digit.match(line):\n                continue\n\n            line_splitted = line.split()\n            if line_splitted[0] in dict_stats[\"apache_procs\"]:\n                overall_cpu += float(line_splitted[8])\n            elif line_splitted[0] in dict_stats[\"nginx_procs\"]:\n                overall_cpu += float(line_splitted[8])\n            elif line_splitted[0] in dict_stats[\"passenger_procs\"]:\n                overall_cpu += float(line_splitted[8])\n\n        return overall_cpu\n\n    def get_passenger_queue_stats(self):\n        queue_stats = {\n            \"top_level_queue_size\": 0.0,\n            \"passenger_queue_size\": 0.0,\n        }\n\n        command = [self.config[\"passenger_status_bin\"]]\n        if str_to_bool(self.config[\"use_sudo\"]):\n            command.insert(0, self.config[\"sudo_cmd\"])\n\n        try:\n            proc1 = subprocess.Popen(command, stdout=subprocess.PIPE)\n            (std_out, std_err) = proc1.communicate()\n        except OSError:\n            return {}\n\n        if std_out is None:\n            return {}\n\n        re_colour = re.compile(\"\\x1B\\[([0-9]{1,3}((;[0-9]{1,3})*)?)?[m|K]\")\n        re_requests = re.compile(r\"Requests\")\n        re_topqueue = re.compile(r\"^top-level\")\n\n        gen_info_flag = 0\n        app_groups_flag = 0\n        for raw_line in std_out.splitlines():\n            line = re_colour.sub(\"\", raw_line)\n            if \"General information\" in line:\n                gen_info_flag = 1\n            if \"Application groups\" in line:\n                app_groups_flag = 1\n            elif re_requests.match(line) and re_topqueue.search(line):\n                line_splitted = line.split()\n                if gen_info_flag == 1 and line_splitted:\n                    queue_stats[\"top_level_queue_size\"] = float(line_splitted[5])\n            elif re_requests.search(line) and not re_topqueue.search(line):\n                line_splitted = line.split()\n                if app_groups_flag == 1 and line_splitted:\n                    queue_stats[\"passenger_queue_size\"] = float(line_splitted[3])\n\n        return queue_stats",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport re\n\nclass PassengerProcessCommunicator:\n    def __init__(self, config):\n        self.config = config\n\n    def get_passenger_memory_stats(self):\n        stats = {\n            'apache_processes': [],\n            'nginx_processes': [],\n            'passenger_processes': [],\n            'total_memory': 0\n        }\n\n        try:\n            sudo = ['sudo'] if self.config.get('use_sudo', False) else []\n            ps_cmd = sudo + ['ps', 'aux']\n            output = subprocess.check_output(ps_cmd).decode('utf-8')\n\n            for line in output.splitlines():\n                if 'apache' in line.lower() or 'httpd' in line.lower():\n                    stats['apache_processes'].append(line)\n                    mem = float(line.split()[5])\n                    stats['total_memory'] += mem\n                elif 'nginx' in line.lower():\n                    stats['nginx_processes'].append(line)\n                    mem = float(line.split()[5])\n                    stats['total_memory'] += mem\n                elif 'passenger' in line.lower():\n                    stats['passenger_processes'].append(line)\n                    mem = float(line.split()[5])\n                    stats['total_memory'] += mem\n\n        except subprocess.CalledProcessError:\n            pass\n\n        return stats\n\n    def get_passenger_cpu_usage(self, dict_stats):\n        total_cpu = 0.0\n\n        for process in dict_stats.get('passenger_processes', []):\n            cpu_usage = float(process.split()[2])\n            total_cpu += cpu_usage\n\n        return total_cpu\n\n    def get_passenger_queue_stats(self):\n        stats = {\n            'top_level_queue': 0,\n            'app_queues': {}\n        }\n\n        try:\n            sudo = ['sudo'] if self.config.get('use_sudo', False) else []\n            passenger_status_cmd = sudo + ['passenger-status']\n            output = subprocess.check_output(passenger_status_cmd).decode('utf-8')\n\n            top_level_match = re.search(r'Requests in top-level queue\\s*:\\s*(\\d+)', output)\n            if top_level_match:\n                stats['top_level_queue'] = int(top_level_match.group(1))\n\n            app_sections = re.finditer(r'App \\S+ \\((.+?)\\):\\s*Requests in queue:\\s*(\\d+)', output)\n            for match in app_sections:\n                app_name = match.group(1)\n                queue_size = int(match.group(2))\n                stats['app_queues'][app_name] = queue_size\n\n        except subprocess.CalledProcessError:\n            pass\n\n        return stats"
  },
  {
    "Task_id": 302,
    "Instruction": "Task Description: Create a function that uses subprocess.Popen with pipes to execute a curl command, handle its output, and store the results in a database.\n\nFunction Description: The function executes a curl command as a subprocess to fetch HTTP headers from a URL using a SOCKS proxy. It handles timeouts, processes the output to extract HTTP status codes, and records all results (including errors) in a global database. The function includes a small delay between calls to avoid overwhelming the system.\n\nInput:\n- self: The object instance containing URL and attempt count (implicit in method context)\n- url: [str] - The target URL to fetch (accessed via self.url)\n- USER_AGENT: [str] - The user agent string for the HTTP request\n- SOCKS_PROXY: [str] - The SOCKS proxy address to use\n- CURL_TIMEOUT: [int] - Maximum time allowed for the curl command to complete\n\nOutput:\n- None (direct return value), but the function:\n  - Updates self.last_code with the HTTP status code\n  - Increments self.attempt counter\n  - Inserts a record into GLOBAL_DB containing:\n    - url: [str] - The requested URL\n    - attempt: [int] - The attempt number\n    - out: [bytes/str] - The curl command's stdout output\n    - err: [bytes/str] - The curl command's stderr output or error message\n    - http_code: [int] - The extracted HTTP status code\n    - curl_exit: [int] - The curl process's exit code",
    "Canonical_solution": "import subprocess\nimport time\n\ndef fetch1(self):\n    args = [ 'curl', '--head', '--user-agent', USER_AGENT, '--proxy', SOCKS_PROXY, self.url ]\n    time.sleep(1) # slight breathing space because MP\n    try:\n        p = subprocess.Popen(args, stdin=subprocess.DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.PIPE) # todo: text=True\n        (out, err) = p.communicate(timeout=CURL_TIMEOUT)\n        hcode = extract_hcode(str(out)) # str() not needed if text=True\n        if hcode == 200: err = PLACEHOLDER\n        ecode = p.returncode\n    except subprocess.TimeoutExpired as e:\n        (out, err) = (PLACEHOLDER, str(e))\n        hcode = BADNESS + 10\n        ecode = BADNESS + 10\n    self.last_code = hcode\n    self.attempt += 1\n    GLOBAL_DB.insert(dict(\n        url=self.url,\n        attempt=self.attempt,\n        out=out,\n        err=err,\n        http_code=hcode,\n        curl_exit=ecode,\n    ))",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport time\nimport re\n\ndef fetch_url_with_curl(self, USER_AGENT, SOCKS_PROXY, CURL_TIMEOUT):\n    time.sleep(0.5)\n    self.attempt += 1\n    cmd = [\n        'curl',\n        '-s',\n        '-I',\n        '-L',\n        '-m', str(CURL_TIMEOUT),\n        '-x', SOCKS_PROXY,\n        '-A', USER_AGENT,\n        self.url\n    ]\n    try:\n        process = subprocess.Popen(\n            cmd,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        out, err = process.communicate()\n        exit_code = process.returncode\n        http_code = None\n        if exit_code == 0:\n            match = re.search(r'HTTP/\\d\\.\\d (\\d+)', out.decode('utf-8', errors='ignore'))\n            if match:\n                http_code = int(match.group(1))\n                self.last_code = http_code\n        GLOBAL_DB.insert({\n            'url': self.url,\n            'attempt': self.attempt,\n            'out': out,\n            'err': err,\n            'http_code': http_code,\n            'curl_exit': exit_code\n        })\n    except Exception as e:\n        GLOBAL_DB.insert({\n            'url': self.url,\n            'attempt': self.attempt,\n            'out': None,\n            'err': str(e),\n            'http_code': None,\n            'curl_exit': None\n        })"
  },
  {
    "Task_id": 303,
    "Instruction": "Task Description: Create a Python class that collects and parses statistics from the amavisd-agent program using subprocess communication.\n\nClass Description: The AmavisCollector class is designed to interface with the amavisd-agent program to collect mail processing statistics. It uses subprocess.PIPE to communicate with the agent process and regular expressions to parse the output. The class supports optional sudo execution for privilege escalation.\n\nAttributes:\n\nmatchers: [List[re.Pattern]] - A list of compiled regular expressions used to parse different formats of statistics from amavisd-agent output.\nconfig: [Dict[str, Any]] - Configuration dictionary containing paths to executables and sudo settings.\n\nMethods:\n\n__init__: [Name](amavisd_exe: str = '/usr/sbin/amavisd-agent', use_sudo: bool = False, sudo_exe: str = '/usr/bin/sudo', sudo_user: str = 'amavis') -> None - Initializes the collector with configuration parameters for the amavisd-agent executable and optional sudo settings.\ncollect: [Name]() -> Generator[Tuple[str, str, str], None, None] - Executes the amavisd-agent process, captures its output, and yields parsed statistics as tuples of (metric_name, metric_type, value). Raises an exception if the process cannot be executed.",
    "Canonical_solution": "import os\nimport subprocess\nimport re\n\nclass AmavisCollector:\n    matchers = [\n        re.compile(r'^\\s*(?P<name>[\\w]+)\\s+(?P<time>[\\d]+) s\\s+'\n                   r'(?P<frequency>[\\d.]+) s/msg\\s+\\([\\w]+\\)\\s*$'),\n        re.compile(r'^\\s*(?P<name>[\\w.-]+)\\s+(?P<count>[\\d]+)\\s+'\n                   r'(?P<frequency>[\\d.]+)/h\\s+(?P<percentage>[\\d.]+) %'\n                   r'\\s\\([\\w]+\\)\\s*$'),\n        re.compile(r'^\\s*(?P<name>[\\w.-]+)\\s+(?P<size>[\\d]+)MB\\s+'\n                   r'(?P<frequency>[\\d.]+)MB/h\\s+(?P<percentage>[\\d.]+) %'\n                   r'\\s\\([\\w]+\\)\\s*$'),\n    ]\n\n    def __init__(self, amavisd_exe='/usr/sbin/amavisd-agent', use_sudo=False, sudo_exe='/usr/bin/sudo', sudo_user='amavis'):\n        self.config = {\n            'amavisd_exe': amavisd_exe,\n            'use_sudo': use_sudo,\n            'sudo_exe': sudo_exe,\n            'sudo_user': sudo_user\n        }\n\n    def collect(self):\n        try:\n            if self.config['use_sudo']:\n                cmdline = [\n                    self.config['sudo_exe'], '-u', self.config['sudo_user'],\n                    '--', self.config['amavisd_exe'], '-c', '1'\n                ]\n            else:\n                cmdline = [self.config['amavisd_exe'], '-c', '1']\n                \n            agent = subprocess.Popen(cmdline, stdout=subprocess.PIPE)\n            agent_out = agent.communicate()[0]\n            lines = agent_out.strip().split(os.linesep)\n            \n            for line in lines:\n                for rex in self.matchers:\n                    res = rex.match(line)\n                    if res:\n                        groups = res.groupdict()\n                        name = groups['name']\n                        for metric, value in groups.items():\n                            if metric == 'name':\n                                continue\n                            yield (name, metric, value)\n\n        except OSError as err:\n            raise Exception(f\"Could not run {self.config['amavisd_exe']}: {err}\")",
    "Code_level": "Class-level",
    "FSMID_for_test": 76,
    "wrong_result": "import re\nimport subprocess\nfrom typing import Dict, Any, Generator, Tuple, List\n\nclass AmavisCollector:\n    def __init__(\n        self,\n        amavisd_exe: str = '/usr/sbin/amavisd-agent',\n        use_sudo: bool = False,\n        sudo_exe: str = '/usr/bin/sudo',\n        sudo_user: str = 'amavis'\n    ) -> None:\n        self.matchers = [\n            re.compile(r'^(\\w+)\\s+(\\w+)\\s+(\\d+)$'),\n            re.compile(r'^(\\w+)\\s+(\\w+)\\s+(\\d+\\.\\d+)$')\n        ]\n        self.config = {\n            'amavisd_exe': amavisd_exe,\n            'use_sudo': use_sudo,\n            'sudo_exe': sudo_exe,\n            'sudo_user': sudo_user\n        }\n\n    def collect(self) -> Generator[Tuple[str, str, str], None, None]:\n        cmd = [self.config['amavisd_exe']]\n        if self.config['use_sudo']:\n            cmd = [self.config['sudo_exe'], '-u', self.config['sudo_user']] + cmd\n\n        try:\n            proc = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.PIPE,\n                universal_newlines=True\n            )\n            stdout, stderr = proc.communicate()\n            if proc.returncode != 0:\n                raise RuntimeError(f'amavisd-agent failed: {stderr}')\n\n            for line in stdout.splitlines():\n                line = line.strip()\n                for matcher in self.matchers:\n                    match = matcher.match(line)\n                    if match:\n                        yield match.groups()\n                        break\n\n        except OSError as e:\n            raise RuntimeError(f'Failed to execute amavisd-agent: {e}')"
  },
  {
    "Task_id": 304,
    "Instruction": "Task Description: Create a function that executes a system command using Python's subprocess module and returns the output.\n\nFunction Description: The function should take a system command as input, execute it in a subprocess, capture the standard output, and return it as a string. The function should handle the process creation, communication, and cleanup automatically.\n\nInput: \n- com: str - A string representing the system command to be executed\n\nOutput:\n- str - The decoded standard output from the executed command",
    "Canonical_solution": "import urllib.request\nimport subprocess\n\ndef excuteCommand(com):\n    ex = subprocess.Popen(com, stdout=subprocess.PIPE, shell=True)\n    out, err = ex.communicate()\n    statusofssh = ex.wait()\n    return out.decode()",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\n\ndef execute_command(com):\n    result = subprocess.run(com, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n    return result.stdout"
  },
  {
    "Task_id": 305,
    "Instruction": "Task Description: The task involves creating Python functions for executing system commands and processing their output through inter-process communication (IPC) using pipes. The functions should handle command execution, output decoding, and temporary file management for command processing.\n\nFunction Description: The provided functions enable executing system commands through subprocesses, capturing and decoding their output, and processing text data through temporary files when needed. The functions handle Windows-specific configurations and character encoding.\n\nInput:\n- For run_and_read(view, cmd):\n  - view: [Any] - A placeholder parameter (unused in function)\n  - cmd: [str] - The command string to execute via cmd.exe\n\n- For filter_region(view, txt, command):\n  - view: [Any] - A placeholder parameter (unused in function)\n  - txt: [str] - The text content to process\n  - command: [str] - The command to process the text with\n\n- For get_startup_info() and get_oem_cp(): No input parameters\n\nOutput:\n- For run_and_read(): [str] - The decoded output from the command execution\n- For filter_region(): [str] - The processed text output from the command\n- For get_startup_info(): [subprocess.STARTUPINFO] - Windows process startup configuration\n- For get_oem_cp(): [str] - The OEM code page identifier as a string",
    "Canonical_solution": "import subprocess\nfrom subprocess import PIPE\nimport os\nimport tempfile\nimport ctypes\n\ndef get_startup_info():\n    startupinfo = subprocess.STARTUPINFO()\n    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    return startupinfo\n\ndef run_and_read(view, cmd):\n    out, err = subprocess.Popen(['cmd.exe', '/c', cmd],\n                                stdout=PIPE,\n                                stderr=PIPE,\n                                shell=True,\n                                startupinfo=get_startup_info()).communicate()\n    try:\n        return (out or err).decode(get_oem_cp()).replace('\\r\\n', '\\n')\n    except AttributeError:\n        return ''\n\ndef filter_region(view, txt, command):\n    try:\n        contents = tempfile.NamedTemporaryFile(suffix='.txt', delete=False)\n        contents.write(txt.encode('utf-8'))\n        contents.close()\n\n        script = tempfile.NamedTemporaryFile(suffix='.bat', delete=False)\n        script.write(('@echo off\\ntype %s | %s' % (contents.name, command)).encode('utf-8'))\n        script.close()\n\n        p = subprocess.Popen([script.name],\n                             stdout=PIPE,\n                             stderr=PIPE,\n                             startupinfo=get_startup_info())\n\n        out, err = p.communicate()\n        return (out or err).decode(get_oem_cp()).replace('\\r\\n', '\\n')[:-1].strip()\n    finally:\n        os.remove(script.name)\n        os.remove(contents.name)\n\ndef get_oem_cp():\n    codepage = ctypes.windll.kernel32.GetOEMCP()\n    return str(codepage)",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import os\nimport subprocess\nimport tempfile\n\ndef get_startup_info():\n    startupinfo = subprocess.STARTUPINFO()\n    startupinfo.dwFlags |= subprocess.STARTF_USESHOWWINDOW\n    startupinfo.wShowWindow = subprocess.SW_HIDE\n    return startupinfo\n\ndef get_oem_cp():\n    import ctypes\n    return str(ctypes.windll.kernel32.GetOEMCP())\n\ndef run_and_read(view, cmd):\n    startupinfo = get_startup_info()\n    proc = subprocess.Popen(\n        cmd, \n        stdin=subprocess.PIPE, \n        stdout=subprocess.PIPE, \n        stderr=subprocess.PIPE, \n        shell=True, \n        startupinfo=startupinfo\n    )\n    out, err = proc.communicate()\n    return out.decode(get_oem_cp())\n\ndef filter_region(view, txt, command):\n    tmp = tempfile.NamedTemporaryFile(delete=False)\n    try:\n        tmp.write(txt.encode('utf-8'))\n        tmp.close()\n        cmd = command + ' \"' + tmp.name + '\"'\n        result = run_and_read(view, cmd)\n        return result\n    finally:\n        os.unlink(tmp.name)"
  },
  {
    "Task_id": 307,
    "Instruction": "Task Description: The task involves creating a function that executes a command in a subprocess, captures its output, and extracts specific metrics from the output for analysis.\n\nFunction Description: The function `run_gpt` executes a given command as a subprocess, captures its standard output and error streams, and parses the output to extract metrics such as runtime, number of parameters, and success status. It then returns these metrics along with any error messages.\n\nInput: \n- `cmd` (str): A string representing the command to be executed in the subprocess.\n\nOutput: \n- A tuple containing:\n  - `runtime` (float): The average iteration time extracted from the output.\n  - `num_params` (float): The number of parameters in billions, rounded to 3 decimal places.\n  - `success` (bool): A boolean indicating whether the test was successful based on a predefined success message.\n  - `errs` (bytes): The error output from the subprocess.",
    "Canonical_solution": "import subprocess\nfrom apex.transformer.testing.commons import TEST_SUCCESS_MESSAGE\n\ndef run_gpt(cmd):\n    args = list(cmd.split(\" \"))\n    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    outs, errs = p.communicate()\n    outs = list(str((outs).decode(\"utf-8\")).splitlines())\n    success = False\n    runtime = 0\n    num_params = 0\n    for out in outs:\n        out = str(out)\n        if \"Average Iteration Time:\" in str(out):\n            slicey = out[out.find(\":\") + 2 :]\n            try:\n                runtime = float(slicey)\n            except:\n                print(slicey)\n                quit()\n        if \"Number of Parameters:\" in str(out):\n            slicey = out[out.find(\":\") + 2 :]\n            try:\n                num_params = int(slicey)\n            except:\n                print(slicey)\n                quit()\n        if str(out) == str(TEST_SUCCESS_MESSAGE):\n            success = True\n    return runtime, round(float(int(num_params)) / 10.0 ** 9, 3), success, errs",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "```python\nimport subprocess\nimport re\n\ndef run_gpt(cmd):\n    runtime = 0.0\n    num_params = 0.0\n    success = False\n    errs = b''\n    \n    try:\n        process = subprocess.Popen(\n            cmd,\n            shell=True,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        stdout, errs = process.communicate()\n        \n        output = stdout.decode('utf-8')\n        \n        runtime_match = re.search(r'avg_iter_time\\s*:\\s*([\\d.]+)', output)\n        if runtime_match:\n            runtime = float(runtime_match.group(1))\n        \n        params_match = re.search(r'num_params\\s*:\\s*([\\d.]+)', output)\n        if params_match:\n            num_params = round(float(params_match.group(1)) / 1e9, 3)\n        \n        success = 'test passed' in output.lower()\n        \n    except Exception:\n        pass\n    \n    return (runtime, num_params, success, errs)\n```"
  },
  {
    "Task_id": 308,
    "Instruction": "Task Description: Write a Python function that queries DNS information for a given domain using the `dig` command, with options for TCP protocol and IPv6.\n\nFunction Description: The function `query_domain` executes a DNS query using the `dig` command-line tool. It retrieves the CNAME and IP address for the specified domain, with configurable query type and DNS server. The function supports TCP protocol and IPv6 queries. It parses the output to extract the first valid CNAME and IP address.\n\nInput:\n- `domain`: [str] - The domain name to query\n- `tcp`: [bool] - Whether to use TCP protocol for the DNS query (default: False)\n- `config`: [dict] - Configuration dictionary containing:\n  - `querytype`: [str] - DNS query type (e.g., 'A', 'AAAA')\n  - `dns`: [str] - DNS server address to query\n\nOutput:\n- [tuple] - A tuple containing:\n  - First valid CNAME found (str), empty string if none\n  - First valid IP address found (str), empty string if none",
    "Canonical_solution": "import subprocess\nimport shlex\n\ndef query_domain(domain, tcp):\n    cmd = \"dig +short +time=2 -6 %s @'%s' '%s'\"\\\n        % (config['querytype'], config['dns'], domain)\n\n    if tcp:\n        cmd = cmd + ' +tcp'\n\n    proc = subprocess.Popen(shlex.split(cmd), stdout=subprocess.PIPE)\n    out, _ = proc.communicate()\n    outarr = out.decode('utf-8').splitlines()\n\n    cname = ip = ''\n    for v in outarr:\n        if cname == '' and validate_domain(v[:-1]):\n            cname = v[:-1]\n        if ip == '' and validate_ip_addr(v):\n            ip = v\n            break\n\n    return (cname, ip)",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport re\n\ndef query_domain(domain, tcp=False, config=None):\n    if config is None:\n        config = {}\n    querytype = config.get('querytype', 'A')\n    dns = config.get('dns', '')\n    \n    cmd = ['dig']\n    if tcp:\n        cmd.append('+tcp')\n    if dns:\n        cmd.extend(['@' + dns])\n    cmd.extend(['+short', domain, querytype])\n    \n    try:\n        result = subprocess.run(cmd, capture_output=True, text=True, check=True)\n        output = result.stdout.strip()\n    except subprocess.CalledProcessError:\n        return ('', '')\n    \n    cname = ''\n    ip = ''\n    \n    for line in output.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        if re.match(r'^[a-zA-Z0-9.-]+$', line) and '.' in line and not re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', line) and not ':' in line:\n            cname = line\n            break\n    \n    for line in output.splitlines():\n        line = line.strip()\n        if not line:\n            continue\n        if querytype == 'A' and re.match(r'^\\d+\\.\\d+\\.\\d+\\.\\d+$', line):\n            ip = line\n            break\n        elif querytype == 'AAAA' and ':' in line:\n            ip = line\n            break\n    \n    return (cname, ip)"
  },
  {
    "Task_id": 309,
    "Instruction": "Task Description: Convert HTML content to plain text using external command-line tools through subprocess communication.\n\nFunction Description: The function converts HTML data to plain text using either 'lynx' or 'html2text' command-line tools. It handles process creation, input/output communication via pipes, and result processing. The function supports additional options for the conversion tools and ensures proper character encoding.\n\nInput:\n- data: [str] - The HTML content to be converted\n- baseurl: [str] - Base URL for relative links (unused in current implementation)\n- method: [str] - Conversion method ('lynx' or 'html2text')\n- options: [dict] - Additional options to pass to the conversion tool (key-value pairs where value can be None for flag options)\n\nOutput:\n- [str] - The converted plain text content with formatting cleaned up",
    "Canonical_solution": "import re\nimport os\nimport subprocess\nimport logging\n\nlogger = logging.getLogger(__name__)\n\ndef html2text(data, baseurl, method, options):\n    if method == 'lynx':\n        cmd = ['lynx', '-nonumbers', '-dump', '-stdin', '-assume_charset UTF-8', '-display_charset UTF-8']\n    elif method == 'html2text':\n        if '-utf8' in subprocess.check_output(['html2text', '-help'], encoding='utf-8'):\n            cmd = ['html2text', '-nobs', '-utf8']\n        else:\n            cmd = ['html2text', '-nobs']\n    else:\n        raise ValueError('Unknown html2text method: %r' % (method,))\n\n    stdout_encoding = 'utf-8'\n\n    for k, v in options.items():\n        if v is not None:\n            cmd.extend(['-{}'.format(k), '{}'.format(v)])\n        else:\n            cmd.append('-{}'.format(k))\n\n    logger.debug('Command: %r, stdout encoding: %s', cmd, stdout_encoding)\n\n    env = {}\n    env.update(os.environ)\n    env['LANG'] = 'en_US.utf-8'\n    env['LC_ALL'] = 'en_US.utf-8'\n\n    html2text = subprocess.Popen(cmd, stdin=subprocess.PIPE, stdout=subprocess.PIPE, env=env)\n    stdout, stderr = html2text.communicate(data.encode('utf-8'))\n    stdout = stdout.decode(stdout_encoding)\n\n    if method == 'lynx':\n        stdout = re.sub(r'file://%s/[^/]*/' % (os.environ.get('TMPDIR', '/tmp'),), '', stdout)\n        stdout = re.sub(r'file://localhost%s/[^/]*/' % (os.environ.get('TMPDIR', '/tmp'),), '', stdout)\n        stdout = re.sub(r'L\\d+-\\d+TMP.html', '', stdout)\n\n    return stdout.strip()",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport shlex\nimport sys\n\ndef html_to_text(data, baseurl=None, method='lynx', options=None):\n    if options is None:\n        options = {}\n\n    if method == 'lynx':\n        cmd = ['lynx', '-stdin', '-dump', '-nomargins', '-width=1024']\n        for opt, val in options.items():\n            if val is None:\n                cmd.append(f'-{opt}')\n            else:\n                cmd.append(f'-{opt}={val}')\n    elif method == 'html2text':\n        cmd = ['html2text']\n        for opt, val in options.items():\n            if val is None:\n                cmd.append(f'--{opt}')\n            else:\n                cmd.append(f'--{opt}={val}')\n    else:\n        raise ValueError(f\"Unsupported method: {method}\")\n\n    try:\n        proc = subprocess.Popen(\n            cmd,\n            stdin=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE\n        )\n        stdout, stderr = proc.communicate(input=data.encode('utf-8'))\n        \n        if proc.returncode != 0:\n            raise RuntimeError(f\"Command failed with {proc.returncode}: {stderr.decode('utf-8')}\")\n        \n        text = stdout.decode('utf-8')\n        return text.strip()\n    \n    except FileNotFoundError as e:\n        raise RuntimeError(f\"Command not found: {cmd[0]}\") from e\n    except Exception as e:\n        raise RuntimeError(f\"Error converting HTML to text: {str(e)}\") from e"
  },
  {
    "Task_id": 310,
    "Instruction": "Task Description: Create a function that uses subprocess.Popen with pipes to execute ffprobe (a multimedia stream analyzer) on a specified file and return the output in JSON format.\n\nFunction Description: The function _probe executes ffprobe command-line tool to analyze a multimedia file, capturing its format and stream information. It uses subprocess.Popen with pipes to handle inter-process communication, converts the output to JSON, and handles errors if the process fails.\n\nInput:\n- filename: [str] - Path to the multimedia file to analyze\n- cmd: [str] (optional, default=\"ffprobe\") - Name/path of the ffprobe executable\n- timeout: [int/float] (optional) - Timeout in seconds for the subprocess\n- **kwargs: [dict] (optional) - Additional ffprobe command-line arguments\n\nOutput:\n- [dict] - Parsed JSON output containing multimedia file information\n- Raises ffmpeg.Error if the process returns a non-zero exit code (includes stderr in exception)",
    "Canonical_solution": "import json\nimport subprocess\nimport ffmpeg\n\ndef _probe(filename, cmd=\"ffprobe\", timeout=None, **kwargs):\n    \"\"\"Run ffprobe on the specified file and return a JSON representation of the output.\n\n    Raises:\n        :class:`ffmpeg.Error`: if ffprobe returns a non-zero exit code,\n            an :class:`Error` is returned with a generic error message.\n            The stderr output can be retrieved by accessing the\n            ``stderr`` property of the exception.\n    \"\"\"\n    args = [cmd, \"-show_format\", \"-show_streams\", \"-of\", \"json\"]\n    args += ffmpeg._utils.convert_kwargs_to_cmd_line_args(kwargs)\n    args += [filename]\n\n    p = subprocess.Popen(args, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    communicate_kwargs = {}\n    if timeout is not None:\n        communicate_kwargs[\"timeout\"] = timeout\n    out, err = p.communicate(**communicate_kwargs)\n    if p.returncode != 0:\n        raise ffmpeg.Error(\"ffprobe\", out, err)\n    return json.loads(out.decode(\"utf-8\"))",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport json\n\ndef _probe(filename, cmd=\"ffprobe\", timeout=None, **kwargs):\n    args = [cmd, '-v', 'quiet', '-print_format', 'json', '-show_format', '-show_streams', filename]\n    for key, value in kwargs.items():\n        args.extend([f'-{key}', str(value)])\n\n    try:\n        process = subprocess.Popen(\n            args,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            universal_newlines=True\n        )\n        stdout, stderr = process.communicate(timeout=timeout)\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, args, stdout, stderr)\n        return json.loads(stdout)\n    except subprocess.TimeoutExpired:\n        process.kill()\n        stdout, stderr = process.communicate()\n        raise subprocess.TimeoutExpired(args, timeout, stdout, stderr)\n    except subprocess.CalledProcessError as e:\n        raise Exception(f\"ffprobe error: {e.stderr}\") from e\n    except json.JSONDecodeError as e:\n        raise Exception(f\"Failed to parse ffprobe output: {e}\") from e"
  },
  {
    "Task_id": 313,
    "Instruction": "Task Description: Create a function that executes a system command in a specified working directory and handles the command's output and errors.\n\nFunction Description: The function `run_command` executes a given system command in a specified working directory using Python's `subprocess` module. It captures the command's standard output and error streams, decodes them, and prints them. If the command returns a non-zero exit code, it raises a `CalledProcessError`.\n\nInput:  \n- `command`: [str] - The system command to be executed.  \n- `working_dir`: [str] - The working directory where the command will be executed.  \n\nOutput:  \n- None (prints output/error messages directly). Raises `subprocess.CalledProcessError` if the command fails (non-zero exit code).",
    "Canonical_solution": "import os\nimport subprocess\nimport sys\n\ndef run_command(command, working_dir):\n    command_array = command.split(\" \")\n    process = subprocess.Popen(command_array, cwd=working_dir, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    stdout, stderr = process.communicate()\n    if stdout:\n        print(stdout.decode('utf-8'))\n    if stderr:\n        print(stderr.decode('utf-8'), level=\"error\")\n    if process.returncode != 0:\n        raise subprocess.CalledProcessError(process.returncode, command)",
    "Code_level": "Function-level",
    "FSMID_for_test": 76,
    "wrong_result": "import subprocess\nimport os\n\ndef run_command(command, working_dir):\n    try:\n        process = subprocess.Popen(\n            command,\n            cwd=working_dir,\n            stdout=subprocess.PIPE,\n            stderr=subprocess.PIPE,\n            shell=True,\n            universal_newlines=True\n        )\n        stdout, stderr = process.communicate()\n        if stdout:\n            print(stdout)\n        if stderr:\n            print(stderr, file=sys.stderr)\n        if process.returncode != 0:\n            raise subprocess.CalledProcessError(process.returncode, command)\n    except Exception as e:\n        raise e"
  },
  {
    "Task_id": 318,
    "Instruction": "Task Description: Implement a TCP relay handler class that manages communication between a local client and a remote server, handling socket connections, data encryption, and protocol obfuscation.\n\nClass Description: The TCPRelayHandler class is responsible for managing the relay of TCP traffic between a local client and a remote server. It handles socket creation, connection management, data encryption/decryption, and protocol obfuscation. The class integrates with an event loop for asynchronous I/O operations.\n\nAttributes:\n- _server: [object] - Reference to the parent server instance\n- _fd_to_handlers: [dict] - Mapping of file descriptors to handler instances\n- _loop: [object] - Event loop instance for I/O multiplexing\n- _local_sock: [socket.socket] - Local client socket\n- _remote_sock: [socket.socket] - Remote server socket (IPv4)\n- _remote_sock_v6: [socket.socket] - Remote server socket (IPv6)\n- _remote_udp: [bool] - Flag indicating UDP mode\n- _config: [dict] - Configuration parameters\n- _dns_resolver: [object] - DNS resolver instance\n- _is_local: [bool] - Flag indicating local mode\n- _stage: [int] - Current connection stage\n- _encryptor: [object] - Encryption handler\n- _encrypt_correct: [bool] - Encryption status flag\n- _obfs: [object] - Obfuscation handler\n- _protocol: [object] - Protocol handler\n- _fastopen_connected: [bool] - TCP Fast Open status\n- _data_to_write_to_local: [list] - Buffer for data to local client\n- _data_to_write_to_remote: [list] - Buffer for data to remote server\n- _udp_data_send_buffer: [bytes] - UDP data buffer\n- _upstream_status: [int] - Upstream connection status\n- _downstream_status: [int] - Downstream connection status\n- _client_address: [tuple] - Local client address\n- _remote_address: [tuple] - Remote server address\n- _forbidden_iplist: [list] - List of blocked IP addresses\n- _chosen_server: [object] - Selected server instance\n- last_activity: [int] - Timestamp of last activity\n\nMethods:\n- __init__(server, fd_to_handlers, loop, local_sock, config, dns_resolver, is_local) -> None - Initializes the handler with server configuration and sockets\n- _create_remote_socket(ip, port) -> socket.socket - Creates and configures a remote socket connection\n- _write_to_sock(data, sock) -> bool - Writes data to a socket with encryption/obfuscation\n- destroy() -> None - Cleans up resources and closes all connections\n- _update_activity() -> None - Updates the last activity timestamp\n- _get_a_server() -> object - Selects an appropriate server from configuration\n- _handle_dns_resolved(result, error) -> None - Callback for DNS resolution\n- _update_stream(stream, status) -> None - Updates stream status in event loop\n- _on_local_read() -> None - Handles local socket read events\n- _on_remote_read() -> None - Handles remote socket read events\n- _on_local_write() -> None - Handles local socket write events\n- _on_remote_write() -> None - Handles remote socket write events",
    "Canonical_solution": "import socket\nimport errno\nimport struct\nimport logging\nimport binascii\nimport traceback\nimport random\nfrom shadowsocks import encrypt, obfs, eventloop, shell, common\nfrom shadowsocks.common import pre_parse_header, parse_header\n\nclass TCPRelayHandler:\n    def __init__(self, server, fd_to_handlers, loop, local_sock, config, dns_resolver, is_local):\n        self._server = server\n        self._fd_to_handlers = fd_to_handlers\n        self._loop = loop\n        self._local_sock = local_sock\n        self._remote_sock = None\n        self._remote_sock_v6 = None\n        self._remote_udp = False\n        self._config = config\n        self._dns_resolver = dns_resolver\n        self._is_local = is_local\n        self._stage = STAGE_INIT\n        self._encryptor = encrypt.Encryptor(config['password'], config['method'])\n        self._encrypt_correct = True\n        self._obfs = obfs.obfs(config['obfs'])\n        server_info = obfs.server_info(server.obfs_data)\n        server_info.host = config['server']\n        server_info.port = server._listen_port\n        server_info.tcp_mss = 1440\n        server_info.param = config['obfs_param']\n        self._obfs.set_server_info(server_info)\n        self._protocol = obfs.obfs(config['protocol'])\n        server_info = obfs.server_info(server.protocol_data)\n        server_info.host = config['server']\n        server_info.port = server._listen_port\n        server_info.tcp_mss = 1440\n        server_info.param = ''\n        self._protocol.set_server_info(server_info)\n        self._fastopen_connected = False\n        self._data_to_write_to_local = []\n        self._data_to_write_to_remote = []\n        self._udp_data_send_buffer = b''\n        self._upstream_status = WAIT_STATUS_READING\n        self._downstream_status = WAIT_STATUS_INIT\n        self._client_address = local_sock.getpeername()[:2]\n        self._remote_address = None\n        if 'forbidden_ip' in config:\n            self._forbidden_iplist = config['forbidden_ip']\n        else:\n            self._forbidden_iplist = None\n        if is_local:\n            self._chosen_server = self._get_a_server()\n        fd_to_handlers[local_sock.fileno()] = self\n        local_sock.setblocking(False)\n        local_sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)\n        loop.add(local_sock, eventloop.POLL_IN | eventloop.POLL_ERR, self._server)\n        self.last_activity = 0\n        self._update_activity()\n        self._server.add_connection(1)\n\n    def _create_remote_socket(self, ip, port):\n        if self._remote_udp:\n            addrs_v6 = socket.getaddrinfo(\"::\", 0, 0, socket.SOCK_DGRAM, socket.SOL_UDP)\n            addrs = socket.getaddrinfo(\"0.0.0.0\", 0, 0, socket.SOCK_DGRAM, socket.SOL_UDP)\n        else:\n            addrs = socket.getaddrinfo(ip, port, 0, socket.SOCK_STREAM, socket.SOL_TCP)\n        if len(addrs) == 0:\n            raise Exception(\"getaddrinfo failed for %s:%d\" % (ip, port))\n        af, socktype, proto, canonname, sa = addrs[0]\n        if self._forbidden_iplist:\n            if common.to_str(sa[0]) in self._forbidden_iplist:\n                raise Exception('IP %s is in forbidden list, reject' % common.to_str(sa[0]))\n        remote_sock = socket.socket(af, socktype, proto)\n        self._remote_sock = remote_sock\n        self._fd_to_handlers[remote_sock.fileno()] = self\n\n        if self._remote_udp:\n            af, socktype, proto, canonname, sa = addrs_v6[0]\n            remote_sock_v6 = socket.socket(af, socktype, proto)\n            self._remote_sock_v6 = remote_sock_v6\n            self._fd_to_handlers[remote_sock_v6.fileno()] = self\n            remote_sock.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1024 * 32)\n            remote_sock.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 1024 * 32)\n            remote_sock_v6.setsockopt(socket.SOL_SOCKET, socket.SO_RCVBUF, 1024 * 32)\n            remote_sock_v6.setsockopt(socket.SOL_SOCKET, socket.SO_SNDBUF, 1024 * 32)\n\n        remote_sock.setblocking(False)\n        if self._remote_udp:\n            remote_sock_v6.setblocking(False)\n        else:\n            remote_sock.setsockopt(socket.SOL_TCP, socket.TCP_NODELAY, 1)\n        return remote_sock\n\n    def _write_to_sock(self, data, sock):\n        if not sock:\n            return False\n        uncomplete = False\n        if self._remote_udp and sock == self._remote_sock:\n            try:\n                self._udp_data_send_buffer += data\n                while len(self._udp_data_send_buffer) > 6:\n                    length = struct.unpack('>H', self._udp_data_send_buffer[:2])[0]\n                    if length > len(self._udp_data_send_buffer):\n                        break\n                    data = self._udp_data_send_buffer[:length]\n                    self._udp_data_send_buffer = self._udp_data_send_buffer[length:]\n                    frag = common.ord(data[2])\n                    if frag != 0:\n                        logging.warn('drop a message since frag is %d' % (frag,))\n                        continue\n                    else:\n                        data = data[3:]\n                    header_result = parse_header(data)\n                    if header_result is None:\n                        continue\n                    connecttype, dest_addr, dest_port, header_length = header_result\n                    addrs = socket.getaddrinfo(dest_addr, dest_port, 0, socket.SOCK_DGRAM, socket.SOL_UDP)\n                    if addrs:\n                        af, socktype, proto, canonname, server_addr = addrs[0]\n                        data = data[header_length:]\n                        if af == socket.AF_INET6:\n                            self._remote_sock_v6.sendto(data, (server_addr[0], dest_port))\n                        else:\n                            sock.sendto(data, (server_addr[0], dest_port))\n            except Exception as e:\n                error_no = eventloop.errno_from_exception(e)\n                if error_no in (errno.EAGAIN, errno.EINPROGRESS, errno.EWOULDBLOCK):\n                    uncomplete = True\n                else:\n                    shell.print_exception(e)\n                    self.destroy()\n                    return False\n            return True\n        else:\n            try:\n                if self._is_local:\n                    pass\n                else:\n                    if sock == self._local_sock and self._encrypt_correct:\n                        obfs_encode = self._obfs.server_encode(data)\n                        data = obfs_encode\n                if data:\n                    l = len(data)\n                    s = sock.send(data)\n                    if s < l:\n                        data = data[s:]\n                        uncomplete = True\n                else:\n                    return\n            except (OSError, IOError) as e:\n                error_no = eventloop.errno_from_exception(e)\n                if error_no in (errno.EAGAIN, errno.EINPROGRESS, errno.EWOULDBLOCK):\n                    uncomplete = True\n                else:\n                    shell.print_exception(e)\n                    self.destroy()\n                    return False\n            except Exception as e:\n                shell.print_exception(e)\n                self.destroy()\n                return False\n        if uncomplete:\n            if sock == self._local_sock:\n                self._data_to_write_to_local.append(data)\n                self._update_stream(STREAM_DOWN, WAIT_STATUS_WRITING)\n            elif sock == self._remote_sock:\n                self._data_to_write_to_remote.append(data)\n                self._update_stream(STREAM_UP, WAIT_STATUS_WRITING)\n            else:\n                logging.error('write_all_to_sock:unknown socket')\n        else:\n            if sock == self._local_sock:\n                self._update_stream(STREAM_DOWN, WAIT_STATUS_READING)\n            elif sock == self._remote_sock:\n                self._update_stream(STREAM_UP, WAIT_STATUS_READING)\n            else:\n                logging.error('write_all_to_sock:unknown socket')\n        return True\n\n    def destroy(self):\n        if self._stage == STAGE_DESTROYED:\n            return\n        self._stage = STAGE_DESTROYED\n        if self._remote_sock:\n            try:\n                self._loop.remove(self._remote_sock)\n            except Exception as e:\n                pass\n            del self._fd_to_handlers[self._remote_sock.fileno()]\n            self._remote_sock.close()\n            self._remote_sock = None\n        if self._remote_sock_v6:\n            try:\n                self._loop.remove(self._remote_sock_v6)\n            except Exception as e:\n                pass\n            del self._fd_to_handlers[self._remote_sock_v6.fileno()]\n            self._remote_sock_v6.close()\n            self._remote_sock_v6 = None\n        if self._local_sock:\n            self._loop.remove(self._local_sock)\n            del self._fd_to_handlers[self._local_sock.fileno()]\n            self._local_sock.close()\n            self._local_sock = None\n        if self._obfs:\n            self._obfs.dispose()\n            self._obfs = None\n        if self._protocol:\n            self._protocol.dispose()\n            self._protocol = None\n        self._dns_resolver.remove_callback(self._handle_dns_resolved)\n        self._server.remove_handler(self)\n        self._server.add_connection(-1)",
    "Code_level": "Class-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport time\nimport errno\n\nclass TCPRelayHandler:\n    def __init__(self, server, fd_to_handlers, loop, local_sock, config, dns_resolver, is_local):\n        self._server = server\n        self._fd_to_handlers = fd_to_handlers\n        self._loop = loop\n        self._local_sock = local_sock\n        self._remote_sock = None\n        self._remote_sock_v6 = None\n        self._remote_udp = False\n        self._config = config\n        self._dns_resolver = dns_resolver\n        self._is_local = is_local\n        self._stage = 0\n        self._encryptor = None\n        self._encrypt_correct = False\n        self._obfs = None\n        self._protocol = None\n        self._fastopen_connected = False\n        self._data_to_write_to_local = []\n        self._data_to_write_to_remote = []\n        self._udp_data_send_buffer = b''\n        self._upstream_status = 0\n        self._downstream_status = 0\n        self._client_address = local_sock.getpeername()\n        self._remote_address = None\n        self._forbidden_iplist = []\n        self._chosen_server = None\n        self.last_activity = time.time()\n\n    def _create_remote_socket(self, ip, port):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.setblocking(False)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_KEEPALIVE, 1)\n        if hasattr(socket, 'TCP_FASTOPEN'):\n            sock.setsockopt(socket.SOL_TCP, socket.TCP_FASTOPEN, 5)\n        return sock\n\n    def _write_to_sock(self, data, sock):\n        if not data:\n            return True\n        try:\n            if sock == self._local_sock:\n                data = self._protocol.client_pre_encrypt(data)\n                data = self._encryptor.encrypt(data)\n                data = self._obfs.client_encode(data)\n            elif sock == self._remote_sock or sock == self._remote_sock_v6:\n                data = self._obfs.server_decode(data)\n                if data:\n                    data = self._encryptor.decrypt(data)\n                    if data:\n                        data = self._protocol.server_post_decrypt(data)\n        except Exception:\n            return False\n        try:\n            l = len(data)\n            s = sock.send(data)\n            if s == l:\n                return True\n            if sock == self._local_sock:\n                self._data_to_write_to_local.append(data[s:])\n            else:\n                self._data_to_write_to_remote.append(data[s:])\n            self._update_stream(sock, 2)\n            return False\n        except (socket.error, OSError) as e:\n            if e.errno not in (errno.EAGAIN, errno.EINPROGRESS, errno.EWOULDBLOCK):\n                return False\n            if sock == self._local_sock:\n                self._data_to_write_to_local.append(data)\n            else:\n                self._data_to_write_to_remote.append(data)\n            self._update_stream(sock, 2)\n            return False\n\n    def destroy(self):\n        if self._local_sock:\n            self._loop.remove(self._local_sock)\n            self._local_sock.close()\n            self._local_sock = None\n        if self._remote_sock:\n            self._loop.remove(self._remote_sock)\n            self._remote_sock.close()\n            self._remote_sock = None\n        if self._remote_sock_v6:\n            self._loop.remove(self._remote_sock_v6)\n            self._remote_sock_v6.close()\n            self._remote_sock_v6 = None\n        if self in self._fd_to_handlers:\n            del self._fd_to_handlers[self._local_sock.fileno()]\n\n    def _update_activity(self):\n        self.last_activity = time.time()\n\n    def _get_a_server(self):\n        return self._config['server']\n\n    def _handle_dns_resolved(self, result, error):\n        if error:\n            self.destroy()\n            return\n        self._remote_address = result\n        self._connect_to_remote()\n\n    def _update_stream(self, stream, status):\n        if stream == self._local_sock:\n            self._downstream_status = status\n        else:\n            self._upstream_status = status\n        self._loop.modify(stream, status)\n\n    def _on_local_read(self):\n        try:\n            data = self._local_sock.recv(4096)\n            if not data:\n                self.destroy()\n                return\n            self._update_activity()\n            self._data_to_write_to_remote.append(data)\n            if self._upstream_status == 0:\n                self._update_stream(self._remote_sock, 1)\n        except (socket.error, OSError) as e:\n            if e.errno not in (errno.EAGAIN, errno.EINPROGRESS, errno.EWOULDBLOCK):\n                self.destroy()\n\n    def _on_remote_read(self):\n        try:\n            data = self._remote_sock.recv(4096)\n            if not data:\n                self.destroy()\n                return\n            self._update_activity()\n            self._data_to_write_to_local.append(data)\n            if self._downstream_status == 0:\n                self._update_stream(self._local_sock, 1)\n        except (socket.error, OSError) as e:\n            if e.errno not in (errno.EAGAIN, errno.EINPROGRESS, errno.EWOULDBLOCK):\n                self.destroy()\n\n    def _on_local_write(self):\n        if not self._data_to_write_to_local:\n            self._update_stream(self._local_sock, 0)\n            return\n        data = b''.join(self._data_to_write_to_local)\n        self._data_to_write_to_local = []\n        if not self._write_to_sock(data, self._local_sock):\n            self.destroy()\n\n    def _on_remote_write(self):\n        if not self._data_to_write_to_remote:\n            self._update_stream(self._remote_sock, 0)\n            return\n        data = b''.join(self._data_to_write_to_remote)\n        self._data_to_write_to_remote = []\n        if not self._write_to_sock(data, self._remote_sock):\n            self.destroy()"
  },
  {
    "Task_id": 320,
    "Instruction": "Task Description: Create a TCP client function that establishes a connection to a server, sends a message, and receives a response.\n\nFunction Description: The function creates a TCP socket, connects to a specified server, sends a UTF-8 encoded message, receives a response, and then closes the connection. It handles the basic TCP client-server interaction.\n\nInput:\n- host: str - The IP address or hostname of the target server\n- port: int - The port number on which the server is listening\n- message: str - The message to be sent to the server\n\nOutput:\n- str - The decoded response received from the server",
    "Canonical_solution": "import socket\n\ndef tcp_client_interaction(host: str, port: int, message: str):\n    \"\"\"\n    TCP client interaction function that sends a message to server and receives response\n    \n    Args:\n        host: Server host address\n        port: Server port number\n        message: Message to send to server\n        \n    Returns:\n        The response received from server\n    \"\"\"\n    # Create TCP socket\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    \n    # Connect to server\n    s.connect((host, port))\n    \n    # Send message\n    len_sent = s.send(message.encode('utf-8'))\n    \n    # Receive response\n    response = s.recv(1024)\n    \n    # Clean up\n    s.close()\n    \n    return response.decode('utf-8')",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client(host, port, message):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((host, port))\n        s.sendall(message.encode('utf-8'))\n        response = s.recv(1024)\n        return response.decode('utf-8')"
  },
  {
    "Task_id": 321,
    "Instruction": "Task Description: Create a Python function that implements a TCP client to interact with a server by sending a command with an offset and receiving the server's response.\n\nFunction Description: The function `tcp_client_interaction` establishes a TCP connection to a specified server, sends a command concatenated with an offset, receives the server's initial response, and then closes the connection. This is useful for basic client-server communication over TCP/IPv4.\n\nInput:\n- `host` (str): The IP address or hostname of the target server.\n- `port` (int): The port number on which the server is listening.\n- `cmd` (str): The command to be sent to the server.\n- `offset` (str): An offset value to be appended to the command.\n\nOutput:\n- The function prints the server's initial response to the console.\n- The function prints a confirmation message (\"Done Sending GTER with buffer\") upon successful completion.",
    "Canonical_solution": "import socket\n\ndef tcp_client_interaction(host, port, cmd, offset):\n    buffer = cmd + offset + '\\r\\n'\n    \n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((host, port))\n    data = s.recv(1024)\n    print \"\\n\" + data\n    s.send(buffer)\n    s.close()\n    print \"Done Sending GTER with buffer\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client_interaction(host, port, cmd, offset):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect((host, port))\n            message = cmd + offset\n            s.sendall(message.encode())\n            response = s.recv(1024)\n            print(response.decode())\n            print(\"Done Sending GTER with buffer\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")"
  },
  {
    "Task_id": 322,
    "Instruction": "Task Description: Create a SOCKS-enabled socket class that extends Python's built-in socket functionality to support proxy connections through SOCKS4, SOCKS5, and HTTP proxies.\n\nClass Description: The `socksocket` class extends Python's `socket.socket` class to provide proxy support. It handles connection establishment through various proxy types while maintaining standard socket functionality. The class manages proxy configuration, connection negotiation, and error handling for proxy-related operations.\n\nAttributes:\n- default_proxy: [Class Variable] - Default proxy configuration used when no specific proxy is set\n- _proxyconn: [socket.socket] - TCP connection used to maintain UDP relay (for SOCKS5 UDP)\n- proxy: [tuple] - Current proxy configuration (type, addr, port, rdns, username, password)\n- proxy_sockname: [tuple] - The bound address (IP, port) of the proxy connection\n- proxy_peername: [tuple] - The destination address (IP, port) through the proxy\n- _timeout: [float] - Socket timeout value\n\nMethods:\n- __init__(family, type, proto, *args, **kwargs) -> None - Initializes the socket with optional proxy support. Validates socket type and sets up proxy attributes.\n- set_proxy(proxy_type, addr, port, rdns, username, password) -> None - Configures proxy settings for the socket.\n- connect(dest_pair) -> None - Establishes connection to destination through configured proxy. Handles both direct and proxied connections.\n- _readall(file, count) -> bytes - Internal method to read exact number of bytes from a file object.\n- _proxy_addr() -> tuple - Internal method to resolve proxy address and port.\n- _negotiate_SOCKS4(dest_addr, dest_port) -> None - Internal method to handle SOCKS4 proxy negotiation.\n- _negotiate_SOCKS5(dest_addr, dest_port) -> None - Internal method to handle SOCKS5 proxy negotiation.\n- _negotiate_HTTP(dest_addr, dest_port) -> None - Internal method to handle HTTP proxy negotiation.",
    "Canonical_solution": "import socket\nfrom errno import EOPNOTSUPP, EINVAL, EAGAIN\nimport struct\nfrom io import BytesIO\nfrom base64 import b64encode\n\nclass socksocket(socket.socket):\n    \"\"\"socksocket([family[, type[, proto]]]) -> socket object\n\n    Open a SOCKS enabled socket. The parameters are the same as\n    those of the standard socket init. In order for SOCKS to work,\n    you must specify family=AF_INET and proto=0.\n    The \"type\" argument must be either SOCK_STREAM or SOCK_DGRAM.\n    \"\"\"\n\n    default_proxy = None\n\n    def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM,\n                 proto=0, *args, **kwargs):\n        if type not in (socket.SOCK_STREAM, socket.SOCK_DGRAM):\n            msg = \"Socket type must be stream or datagram, not {!r}\"\n            raise ValueError(msg.format(type))\n\n        super(socksocket, self).__init__(family, type, proto, *args, **kwargs)\n        self._proxyconn = None  # TCP connection to keep UDP relay alive\n\n        if self.default_proxy:\n            self.proxy = self.default_proxy\n        else:\n            self.proxy = (None, None, None, None, None, None)\n        self.proxy_sockname = None\n        self.proxy_peername = None\n\n        self._timeout = None\n\n    def _readall(self, file, count):\n        \"\"\"Receive EXACTLY the number of bytes requested from the file object.\"\"\"\n        data = b\"\"\n        while len(data) < count:\n            d = file.read(count - len(data))\n            if not d:\n                raise GeneralProxyError(\"Connection closed unexpectedly\")\n            data += d\n        return data\n\n    def set_proxy(self, proxy_type=None, addr=None, port=None, rdns=True,\n                  username=None, password=None):\n        \"\"\"Sets the proxy to be used.\"\"\"\n        self.proxy = (proxy_type, addr, port, rdns,\n                      username.encode() if username else None,\n                      password.encode() if password else None)\n\n    def connect(self, dest_pair):\n        \"\"\"\n        Connects to the specified destination through a proxy.\n        Uses the same API as socket's connect().\n\n        dest_pair - 2-tuple of (IP/hostname, port).\n        \"\"\"\n        if len(dest_pair) != 2 or dest_pair[0].startswith(\"[\"):\n            raise socket.error(\"PySocks doesn't support IPv6: %s\" % str(dest_pair))\n\n        dest_addr, dest_port = dest_pair\n\n        if self.type == socket.SOCK_DGRAM:\n            if not self._proxyconn:\n                self.bind((\"\", 0))\n            dest_addr = socket.gethostbyname(dest_addr)\n\n            if dest_addr == \"0.0.0.0\" and not dest_port:\n                self.proxy_peername = None\n            else:\n                self.proxy_peername = (dest_addr, dest_port)\n            return\n\n        (proxy_type, proxy_addr, proxy_port, rdns, username,\n         password) = self.proxy\n\n        if (not isinstance(dest_pair, (list, tuple))\n                or len(dest_pair) != 2\n                or not dest_addr\n                or not isinstance(dest_port, int)):\n            raise GeneralProxyError(\n                \"Invalid destination-connection (host, port) pair\")\n\n        super(socksocket, self).settimeout(self._timeout)\n\n        if proxy_type is None:\n            self.proxy_peername = dest_pair\n            super(socksocket, self).settimeout(self._timeout)\n            super(socksocket, self).connect((dest_addr, dest_port))\n            return\n\n        proxy_addr = self._proxy_addr()\n\n        try:\n            super(socksocket, self).connect(proxy_addr)\n        except socket.error as error:\n            self.close()\n            proxy_addr, proxy_port = proxy_addr\n            proxy_server = \"{0}:{1}\".format(proxy_addr, proxy_port)\n            printable_type = PRINTABLE_PROXY_TYPES[proxy_type]\n\n            msg = \"Error connecting to {0} proxy {1}\".format(printable_type,\n                                                             proxy_server)\n            raise ProxyConnectionError(msg, error)\n        else:\n            try:\n                negotiate = self._proxy_negotiators[proxy_type]\n                negotiate(self, dest_addr, dest_port)\n            except socket.error as error:\n                self.close()\n                raise GeneralProxyError(\"Socket error\", error)\n            except ProxyError:\n                self.close()\n                raise\n\n    def _proxy_addr(self):\n        \"\"\"Return proxy address to connect to as tuple object\"\"\"\n        (proxy_type, proxy_addr, proxy_port, rdns, username,\n         password) = self.proxy\n        proxy_port = proxy_port or DEFAULT_PORTS.get(proxy_type)\n        if not proxy_port:\n            raise GeneralProxyError(\"Invalid proxy type\")\n        return proxy_addr, proxy_port\n\n    def _negotiate_SOCKS5(self, dest_addr, dest_port):\n        \"\"\"Negotiates a connection through a SOCKS5 server.\"\"\"\n        self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(\n            self, b\"\\x01\", (dest_addr, dest_port))\n\n    def _negotiate_SOCKS4(self, dest_addr, dest_port):\n        \"\"\"Negotiates a connection through a SOCKS4 server.\"\"\"\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        writer = self.makefile(\"wb\")\n        reader = self.makefile(\"rb\", 0)\n        try:\n            remote_resolve = False\n            try:\n                addr_bytes = socket.inet_aton(dest_addr)\n            except socket.error:\n                if rdns:\n                    addr_bytes = b\"\\x00\\x00\\x00\\x01\"\n                    remote_resolve = True\n                else:\n                    addr_bytes = socket.inet_aton(\n                        socket.gethostbyname(dest_addr))\n\n            writer.write(struct.pack(\">BBH\", 0x04, 0x01, dest_port))\n            writer.write(addr_bytes)\n\n            if username:\n                writer.write(username)\n            writer.write(b\"\\x00\")\n\n            if remote_resolve:\n                writer.write(dest_addr.encode(\"idna\") + b\"\\x00\")\n            writer.flush()\n\n            resp = self._readall(reader, 8)\n            if resp[0:1] != b\"\\x00\":\n                raise GeneralProxyError(\n                    \"SOCKS4 proxy server sent invalid data\")\n\n            status = ord(resp[1:2])\n            if status != 0x5A:\n                error = SOCKS4_ERRORS.get(status, \"Unknown error\")\n                raise SOCKS4Error(\"{0:#04x}: {1}\".format(status, error))\n\n            self.proxy_sockname = (socket.inet_ntoa(resp[4:]),\n                                   struct.unpack(\">H\", resp[2:4])[0])\n            if remote_resolve:\n                self.proxy_peername = socket.inet_ntoa(addr_bytes), dest_port\n            else:\n                self.proxy_peername = dest_addr, dest_port\n        finally:\n            reader.close()\n            writer.close()\n\n    def _negotiate_HTTP(self, dest_addr, dest_port):\n        \"\"\"Negotiates a connection through an HTTP server.\"\"\"\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        addr = dest_addr if rdns else socket.gethostbyname(dest_addr)\n\n        http_headers = [\n            (b\"CONNECT \" + addr.encode(\"idna\") + b\":\" +\n             str(dest_port).encode() + b\" HTTP/1.1\"),\n            b\"Host: \" + dest_addr.encode(\"idna\")\n        ]\n\n        if username and password:\n            http_headers.append(b\"Proxy-Authorization: basic \" +\n                                b64encode(username + b\":\" + password))\n\n        http_headers.append(b\"\\r\\n\")\n\n        self.sendall(b\"\\r\\n\".join(http_headers))\n\n        fobj = self.makefile()\n        status_line = fobj.readline()\n        fobj.close()\n\n        if not status_line:\n            raise GeneralProxyError(\"Connection closed unexpectedly\")\n\n        try:\n            proto, status_code, status_msg = status_line.split(\" \", 2)\n        except ValueError:\n            raise GeneralProxyError(\"HTTP proxy server sent invalid response\")\n\n        if not proto.startswith(\"HTTP/\"):\n            raise GeneralProxyError(\n                \"Proxy server does not appear to be an HTTP proxy\")\n\n        try:\n            status_code = int(status_code)\n        except ValueError:\n            raise HTTPError(\n                \"HTTP proxy server did not return a valid HTTP status\")\n\n        if status_code != 200:\n            error = \"{0}: {1}\".format(status_code, status_msg)\n            raise HTTPError(error)\n\n        self.proxy_sockname = (b\"0.0.0.0\", 0)\n        self.proxy_peername = addr, dest_port\n\n    _proxy_negotiators = {\n        SOCKS4: _negotiate_SOCKS4,\n        SOCKS5: _negotiate_SOCKS5,\n        HTTP: _negotiate_HTTP\n    }",
    "Code_level": "Class-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport struct\nimport errno\nimport base64\n\nclass socksocket(socket.socket):\n    default_proxy = None\n\n    def __init__(self, family=socket.AF_INET, type=socket.SOCK_STREAM, proto=0, *args, **kwargs):\n        super().__init__(family, type, proto, *args, **kwargs)\n        self._proxyconn = None\n        self.proxy = None\n        self.proxy_sockname = None\n        self.proxy_peername = None\n        self._timeout = None\n        if self.default_proxy:\n            self.set_proxy(*self.default_proxy)\n\n    def set_proxy(self, proxy_type=None, addr=None, port=None, rdns=True, username=None, password=None):\n        self.proxy = (proxy_type, addr, port, rdns, username, password)\n\n    def connect(self, dest_pair):\n        if not self.proxy:\n            return super().connect(dest_pair)\n        \n        dest_addr, dest_port = dest_pair\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        super().connect(self._proxy_addr())\n        \n        if proxy_type == 1:\n            self._negotiate_SOCKS4(dest_addr, dest_port)\n        elif proxy_type == 2:\n            self._negotiate_SOCKS5(dest_addr, dest_port)\n        elif proxy_type == 3:\n            self._negotiate_HTTP(dest_addr, dest_port)\n        else:\n            raise ValueError(\"Invalid proxy type\")\n\n    def _readall(self, file, count):\n        data = bytearray()\n        while len(data) < count:\n            packet = file.read(count - len(data))\n            if not packet:\n                raise socket.error(\"Connection closed unexpectedly\")\n            data.extend(packet)\n        return bytes(data)\n\n    def _proxy_addr(self):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n        return (addr, port)\n\n    def _negotiate_SOCKS4(self, dest_addr, dest_port):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n        try:\n            ipaddr = socket.inet_aton(dest_addr)\n        except socket.error:\n            if not rdns:\n                raise socket.error(\"SOCKS4 does not support remote DNS\")\n            ipaddr = struct.pack(\"!I\", 0x00000001)\n        \n        username = (username or \"\").encode(\"ascii\")\n        packet = struct.pack(\"!BBH\", 0x04, 0x01, dest_port) + ipaddr + username + b\"\\x00\"\n        self.sendall(packet)\n        \n        data = self._readall(self.makefile(\"rb\"), 8)\n        if data[0] != 0x00 or data[1] != 0x5A:\n            raise socket.error(\"SOCKS4 connection failed\")\n        self.proxy_sockname = (socket.inet_ntoa(data[4:8]), struct.unpack(\"!H\", data[2:4])[0])\n\n    def _negotiate_SOCKS5(self, dest_addr, dest_port):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n        auth_methods = [0x00]\n        if username and password:\n            auth_methods.append(0x02)\n        \n        self.sendall(struct.pack(\"!BB\", 0x05, len(auth_methods))\n        self.sendall(struct.pack(\"!B\" * len(auth_methods), *auth_methods))\n        \n        data = self._readall(self.makefile(\"rb\"), 2)\n        if data[0] != 0x05:\n            raise socket.error(\"SOCKS5 negotiation failed\")\n        if data[1] == 0x02:\n            if not username or not password:\n                raise socket.error(\"SOCKS5 server requires authentication\")\n            username = username.encode(\"utf-8\")\n            password = password.encode(\"utf-8\")\n            self.sendall(struct.pack(\"!BB\", 0x01, len(username)) + username + struct.pack(\"!B\", len(password)) + password)\n            data = self._readall(self.makefile(\"rb\"), 2)\n            if data[0] != 0x01 or data[1] != 0x00:\n                raise socket.error(\"SOCKS5 authentication failed\")\n        elif data[1] != 0x00:\n            raise socket.error(\"SOCKS5 negotiation failed\")\n        \n        try:\n            ipaddr = socket.inet_aton(dest_addr)\n            addr_type = 0x01\n        except socket.error:\n            if not rdns:\n                raise socket.error(\"SOCKS5 does not support remote DNS\")\n            addr_type = 0x03\n            ipaddr = dest_addr.encode(\"idna\")\n        \n        packet = struct.pack(\"!BBB\", 0x05, 0x01, 0x00) + struct.pack(\"!B\", addr_type)\n        if addr_type == 0x01:\n            packet += ipaddr\n        elif addr_type == 0x03:\n            packet += struct.pack(\"!B\", len(ipaddr)) + ipaddr\n        packet += struct.pack(\"!H\", dest_port)\n        self.sendall(packet)\n        \n        data = self._readall(self.makefile(\"rb\"), 4)\n        if data[0] != 0x05 or data[1] != 0x00:\n            raise socket.error(\"SOCKS5 connection failed\")\n        \n        if data[3] == 0x01:\n            data += self._readall(self.makefile(\"rb\"), 4)\n            self.proxy_sockname = (socket.inet_ntoa(data[4:8]), struct.unpack(\"!H\", data[2:4])[0])\n        elif data[3] == 0x03:\n            length = ord(self._readall(self.makefile(\"rb\"), 1))\n            data += self._readall(self.makefile(\"rb\"), length + 2)\n            self.proxy_sockname = (data[4:4+length].decode(\"ascii\"), struct.unpack(\"!H\", data[4+length:6+length])[0])\n        else:\n            raise socket.error(\"SOCKS5 returned invalid address type\")\n\n    def _negotiate_HTTP(self, dest_addr, dest_port):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n        headers = [\"CONNECT {}:{} HTTP/1.1\".format(dest_addr, dest_port)]\n        if username and password:\n            auth = base64.b64encode(\"{}:{}\".format(username, password).encode(\"utf-8\")).decode(\"ascii\")\n            headers.append(\"Proxy-Authorization: Basic {}\".format(auth))\n        headers.append(\"\\r\\n\")\n        self.sendall(\"\\r\\n\".join(headers).encode(\"utf-8\"))\n        \n        response = []\n        while True:\n            line = self._readall(self.makefile(\"rb\"), 1)\n            while line[-2:] != b\"\\r\\n\":\n                line += self._readall(self.makefile(\"rb\"), 1)\n            response.append(line[:-2])\n            if line.startswith(b\"HTTP/\") and b\" \" in line:\n                status = int(line.split(b\" \")[1])\n            if line == b\"\\r\\n\":\n                break\n        \n        if status != 200:\n            raise socket.error(\"HTTP proxy connection failed\")"
  },
  {
    "Task_id": 323,
    "Instruction": "Task Description: Implement a TCP client class for managing connections to a server, sending and receiving commands, and handling various server messages.\n\nClass Description: The Client class provides functionality to connect to a server, send and receive commands, and maintain client state including client ID, room information, and attributes. It handles connection management, message processing, and error handling.\n\nAttributes:\n- host: str - The server host address to connect to\n- port: int - The server port to connect to\n- pending_commands: List[common.Command] - Commands waiting to be sent to the server\n- socket: Socket - The socket connection to the server\n- client_id: Optional[str] - The unique ID assigned by the server\n- current_custom_attributes: Dict[str, Any] - Attributes of the current client\n- clients_attributes: Dict[str, Dict[str, Any]] - Attributes of all connected clients\n- rooms_attributes: Dict[str, Dict[str, Any]] - Attributes of all available rooms\n- current_room: Optional[str] - The current room the client is in\n\nMethods:\n- __init__(host: str = common.DEFAULT_HOST, port: int = common.DEFAULT_PORT) -> None - Initializes the client with server connection details\n- __del__() -> None - Destructor that ensures proper disconnection\n- connect() -> None - Establishes connection to the server and requests initial data\n- disconnect() -> None - Closes the connection to the server\n- is_connected() -> bool - Returns True if connected to the server\n- send_command(command: common.Command) -> bool - Sends a command to the server\n- handle_connection_lost() -> None - Handles cleanup when connection is lost\n- fetch_incoming_commands() -> List[common.Command] - Receives and processes incoming commands from server\n- fetch_outgoing_commands(commands_send_interval: int = 0) -> None - Sends pending commands to server\n- fetch_commands(commands_send_interval: int = 0) -> List[common.Command] - Handles both incoming and outgoing commands",
    "Canonical_solution": "import socket\nimport logging\nimport time\nfrom typing import Dict, Any, Mapping, Optional, List, Callable\n\nimport mixer.broadcaster.common as common\nfrom mixer.broadcaster.socket import Socket\nfrom mixer.broadcaster.common import MessageType\nfrom mixer.broadcaster.common import update_attributes_and_get_diff, update_named_attributes\n\nclass Client:\n    def __init__(self, host: str = common.DEFAULT_HOST, port: int = common.DEFAULT_PORT):\n        self.host = host\n        self.port = port\n        self.pending_commands: List[common.Command] = []\n        self.socket: Socket = None\n        self.client_id: Optional[str] = None\n        self.current_custom_attributes: Dict[str, Any] = {}\n        self.clients_attributes: Dict[str, Dict[str, Any]] = {}\n        self.rooms_attributes: Dict[str, Dict[str, Any]] = {}\n        self.current_room: Optional[str] = None\n\n    def __del__(self):\n        if self.socket is not None:\n            self.disconnect()\n\n    def connect(self):\n        if self.is_connected():\n            raise RuntimeError(\"Client.connect : already connected\")\n\n        try:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            self.socket = Socket(sock)\n            self.socket.connect((self.host, self.port))\n            self.send_command(common.Command(common.MessageType.CLIENT_ID))\n            self.send_command(common.Command(common.MessageType.LIST_CLIENTS))\n            self.send_command(common.Command(common.MessageType.LIST_ROOMS))\n        except ConnectionRefusedError:\n            self.socket = None\n        except common.ClientDisconnectedException:\n            self.handle_connection_lost()\n        except Exception as e:\n            self.socket = None\n            raise\n\n    def disconnect(self):\n        if self.socket:\n            self.socket.shutdown(socket.SHUT_RDWR)\n            self.socket.close()\n            self.socket = None\n\n    def is_connected(self):\n        return self.socket is not None\n\n    def send_command(self, command: common.Command):\n        try:\n            common.write_message(self.socket, command)\n            return True\n        except common.ClientDisconnectedException:\n            self.handle_connection_lost()\n            return False\n\n    def handle_connection_lost(self):\n        self.socket = None\n\n    def fetch_incoming_commands(self) -> List[common.Command]:\n        try:\n            received_commands = common.read_all_messages(self.socket)\n        except common.ClientDisconnectedException:\n            self.handle_connection_lost()\n            raise\n\n        for command in received_commands:\n            if command.type in self._default_command_handlers:\n                self._default_command_handlers[command.type](self, command)\n\n        return received_commands\n\n    def fetch_outgoing_commands(self, commands_send_interval=0):\n        for command in self.pending_commands:\n            if not self.send_command(command):\n                break\n            if commands_send_interval > 0:\n                time.sleep(commands_send_interval)\n        self.pending_commands = []\n\n    def fetch_commands(self, commands_send_interval=0) -> List[common.Command]:\n        self.fetch_outgoing_commands(commands_send_interval)\n        return self.fetch_incoming_commands()\n\n    _default_command_handlers: Mapping[MessageType, Callable[[common.Command], None]] = {\n        MessageType.LIST_CLIENTS: lambda self, cmd: update_named_attributes(\n            self.clients_attributes, common.decode_json(cmd.data, 0)[0]\n        ),\n        MessageType.LIST_ROOMS: lambda self, cmd: update_named_attributes(\n            self.rooms_attributes, common.decode_json(cmd.data, 0)[0]\n        ),\n        MessageType.CLIENT_ID: lambda self, cmd: setattr(self, \"client_id\", cmd.data.decode()),\n        MessageType.ROOM_UPDATE: lambda self, cmd: update_named_attributes(\n            self.rooms_attributes, common.decode_json(cmd.data, 0)[0]\n        ),\n        MessageType.ROOM_DELETED: lambda self, cmd: self.rooms_attributes.pop(common.decode_string(cmd.data, 0)[0], None),\n        MessageType.CLIENT_UPDATE: lambda self, cmd: update_named_attributes(\n            self.clients_attributes, common.decode_json(cmd.data, 0)[0]\n        ),\n        MessageType.CLIENT_DISCONNECTED: lambda self, cmd: self.clients_attributes.pop(common.decode_string(cmd.data, 0)[0], None),\n        MessageType.JOIN_ROOM: lambda self, cmd: setattr(self, \"current_room\", common.decode_string(cmd.data, 0)[0]),\n        MessageType.SEND_ERROR: lambda self, cmd: logging.error(\"Received error message : %s\", common.decode_string(cmd.data, 0)[0]),\n    }",
    "Code_level": "Class-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport json\nfrom typing import Optional, Dict, Any, List\nimport common\n\nclass Client:\n    def __init__(self, host: str = common.DEFAULT_HOST, port: int = common.DEFAULT_PORT) -> None:\n        self.host = host\n        self.port = port\n        self.pending_commands: List[common.Command] = []\n        self.socket: Optional[socket.socket] = None\n        self.client_id: Optional[str] = None\n        self.current_custom_attributes: Dict[str, Any] = {}\n        self.clients_attributes: Dict[str, Dict[str, Any]] = {}\n        self.rooms_attributes: Dict[str, Dict[str, Any]] = {}\n        self.current_room: Optional[str] = None\n\n    def __del__(self) -> None:\n        self.disconnect()\n\n    def connect(self) -> None:\n        if self.is_connected():\n            return\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.connect((self.host, self.port))\n        self.socket.settimeout(0.0)\n        self.pending_commands.append(common.Command(common.CommandType.REQUEST_INITIAL_DATA))\n\n    def disconnect(self) -> None:\n        if self.socket:\n            try:\n                self.socket.close()\n            except:\n                pass\n            self.socket = None\n        self.handle_connection_lost()\n\n    def is_connected(self) -> bool:\n        return self.socket is not None\n\n    def send_command(self, command: common.Command) -> bool:\n        if not self.is_connected():\n            return False\n        self.pending_commands.append(command)\n        return True\n\n    def handle_connection_lost(self) -> None:\n        self.client_id = None\n        self.current_custom_attributes = {}\n        self.clients_attributes = {}\n        self.rooms_attributes = {}\n        self.current_room = None\n        self.pending_commands = []\n\n    def fetch_incoming_commands(self) -> List[common.Command]:\n        if not self.is_connected():\n            return []\n        commands = []\n        try:\n            while True:\n                data = self.socket.recv(4096)\n                if not data:\n                    self.disconnect()\n                    break\n                try:\n                    command_data = json.loads(data.decode('utf-8'))\n                    command = common.Command.from_dict(command_data)\n                    commands.append(command)\n                    if command.type == common.CommandType.SET_INITIAL_DATA:\n                        self.client_id = command.data.get('client_id')\n                        self.current_custom_attributes = command.data.get('custom_attributes', {})\n                        self.clients_attributes = command.data.get('clients_attributes', {})\n                        self.rooms_attributes = command.data.get('rooms_attributes', {})\n                        self.current_room = command.data.get('current_room')\n                    elif command.type == common.CommandType.UPDATE_CLIENT_ATTRIBUTES:\n                        client_id = command.data.get('client_id')\n                        if client_id == self.client_id:\n                            self.current_custom_attributes.update(command.data.get('attributes', {}))\n                        if client_id in self.clients_attributes:\n                            self.clients_attributes[client_id].update(command.data.get('attributes', {}))\n                    elif command.type == common.CommandType.UPDATE_ROOM_ATTRIBUTES:\n                        room_id = command.data.get('room_id')\n                        if room_id in self.rooms_attributes:\n                            self.rooms_attributes[room_id].update(command.data.get('attributes', {}))\n                    elif command.type == common.CommandType.CLIENT_LEFT:\n                        client_id = command.data.get('client_id')\n                        if client_id in self.clients_attributes:\n                            del self.clients_attributes[client_id]\n                    elif command.type == common.CommandType.CLIENT_JOINED:\n                        client_id = command.data.get('client_id')\n                        self.clients_attributes[client_id] = command.data.get('attributes', {})\n                    elif command.type == common.CommandType.ROOM_CREATED:\n                        room_id = command.data.get('room_id')\n                        self.rooms_attributes[room_id] = command.data.get('attributes', {})\n                    elif command.type == common.CommandType.ROOM_DELETED:\n                        room_id = command.data.get('room_id')\n                        if room_id in self.rooms_attributes:\n                            del self.rooms_attributes[room_id]\n                except (json.JSONDecodeError, KeyError, ValueError):\n                    continue\n        except BlockingIOError:\n            pass\n        except ConnectionError:\n            self.disconnect()\n        return commands\n\n    def fetch_outgoing_commands(self, commands_send_interval: int = 0) -> None:\n        if not self.is_connected() or not self.pending_commands:\n            return\n        try:\n            for command in self.pending_commands:\n                self.socket.sendall(json.dumps(command.to_dict()).encode('utf-8'))\n            self.pending_commands = []\n        except ConnectionError:\n            self.disconnect()\n\n    def fetch_commands(self, commands_send_interval: int = 0) -> List[common.Command]:\n        self.fetch_outgoing_commands(commands_send_interval)\n        return self.fetch_incoming_commands()"
  },
  {
    "Task_id": 325,
    "Instruction": "Task Description: Create a function to probe an HTTP server's connectivity and response behavior, checking for potential network interference or server availability issues.\n\nFunction Description: The function attempts to connect to an HTTP server, send a GET request with a specific keyword, and analyze the response. It also checks Redis for potential firewall interference patterns. The function returns different status codes based on the server's response and network conditions.\n\nInput:\n- domain: [str] - The domain name of the target server\n- ip: [str] - The IP address of the target server\n- keyword: [str] - The search keyword to include in the GET request\n\nOutput:\n- [int] - Returns one of the following status codes:\n  * RET_SUCCESS (1) if server responds normally\n  * RET_SVR_NO_RESP (2) if server doesn't respond or times out\n  * RET_GFW_RST (3) if firewall interference is detected\n  * RET_OTHER (99) for other unexpected errors",
    "Canonical_solution": "import os\nimport socket\nimport errno\nimport time\nimport redis\nimport struct\n\nRET_SUCCESS = 1\nRET_SVR_NO_RESP = 2\nRET_GFW_RST = 3\nRET_OTHER = 99\n\ndef probe_http_server(domain, ip, keyword):\n    redis_conn = redis.StrictRedis(host='localhost', port=6389, db=0)\n\n    ret = 0\n    try:\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.settimeout(5)\n        s.connect((ip, 80))\n        s.sendall(\"GET /?keyword=%s HTTP/1.1\\r\\nHost: %s\\r\\nConnection: close\\r\\nUser-Agent: connectivity measurement\\r\\n\\r\\n\" % (keyword, domain))\n        data = s.recv(1024)\n        s.shutdown(socket.SHUT_RDWR)\n        s.close()\n        if data and len(data) > 1:\n            ret = 1\n    except socket.timeout:\n        ret = 0\n    except socket.error, serr:\n        if serr[0] == errno.ECONNRESET:\n            ret = -1\n        elif serr[0] == errno.ETIMEDOUT:\n            ret = 0\n        else:\n            ret = -99\n    except Exception, err:\n        ret = -99\n\n    time.sleep(2)\n    type1rst = redis_conn.keys(\"rst:attack1:*_%d\" % ip2int(ip))\n    type2rst = redis_conn.keys(\"rst:attack2:*_%d\" % ip2int(ip))\n    if type1rst or type2rst:\n        return RET_GFW_RST\n    else:\n        if ret == -1:\n            return RET_SVR_NO_RESP\n        elif ret == 0:\n            return RET_SVR_NO_RESP\n        elif ret == 1:\n            return RET_SUCCESS\n        else:\n            return RET_OTHER\n\ndef ip2int(addr):                                                               \n    return struct.unpack(\"I\", socket.inet_aton(addr))[0]",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport redis\nimport requests\nfrom urllib.parse import quote\n\nRET_SUCCESS = 1\nRET_SVR_NO_RESP = 2\nRET_GFW_RST = 3\nRET_OTHER = 99\n\ndef probe_http_server(domain, ip, keyword):\n    try:\n        headers = {'Host': domain}\n        url = f\"http://{ip}/search?q={quote(keyword)}\"\n        response = requests.get(url, headers=headers, timeout=10)\n        if response.status_code == 200:\n            return RET_SUCCESS\n        return RET_OTHER\n    except requests.exceptions.ConnectionError as e:\n        if isinstance(e.args[0], requests.packages.urllib3.exceptions.ProtocolError):\n            if 'Connection reset by peer' in str(e):\n                return RET_GFW_RST\n        return RET_SVR_NO_RESP\n    except requests.exceptions.Timeout:\n        return RET_SVR_NO_RESP\n    except Exception:\n        return RET_OTHER"
  },
  {
    "Task_id": 329,
    "Instruction": "Task Description: Implement a TCP client function in Python that connects to a server, receives initial data, sends a formatted message, and handles connection errors gracefully.\n\nFunction Description: The function establishes a TCP connection to a specified server, receives initial data from the server, sends a formatted message (prefixed with \"USV \" and suffixed with '\\r\\n\\r\\n'), and ensures proper connection closure. Error handling is included to manage connection issues.\n\nInput:\n- host: str - The IP address or hostname of the target server\n- port: int - The port number on which the server is listening\n- buffer: str - The message content to be sent to the server\n\nOutput:\n- None (prints received data and status messages to console)\n- Success: Prints \"Buffer Sent Successfully!\" upon completion\n- Failure: Prints \"Check your debugger\" if connection fails",
    "Canonical_solution": "import socket\n\ndef tcp_client_interaction(host, port, buffer):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        data = s.recv(1024)\n        print(\"\\n\" + data)\n        s.send(\"USV \" + buffer + '\\r\\n\\r\\n')\n    except:\n        print(\"Check your debugger\")\n    finally:\n        s.close()\n        print(\"Buffer Sent Successfully!\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client(host, port, buffer):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            s.connect((host, port))\n            initial_data = s.recv(1024)\n            print(f\"Received initial data: {initial_data.decode()}\")\n            formatted_msg = f\"USV {buffer}\\r\\n\\r\\n\"\n            s.sendall(formatted_msg.encode())\n            print(\"Buffer Sent Successfully!\")\n    except socket.timeout:\n        print(\"Connection timed out. Check your debugger\")\n    except ConnectionRefusedError:\n        print(\"Connection refused. Check your debugger\")\n    except socket.gaierror:\n        print(\"Host resolution failed. Check your debugger\")\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check your debugger\")"
  },
  {
    "Task_id": 33,
    "Instruction": "Task Description: Create a Python class that implements an HTTP client for interacting with the SambaVerse API, handling request construction, response processing, and error management.\n\nClass Description: The SambaVerseClient class provides an interface to communicate with the SambaVerse API, sending chat messages and receiving completions. It handles API authentication, request formatting, and response conversion.\n\nAttributes:\n_api_key: str - The API key for authentication with the SambaVerse service\nmodel_type: str - The model type/name to be used for completions\n_url: str - The endpoint URL for the SambaVerse API\nmodel_config_dict: Dict[str, Any] - Configuration parameters for the model (temperature, max_tokens, etc.)\n\nMethods:\n__init__(api_key: str, model_type: str, url: str, model_config_dict: Dict[str, Any]) -> None - Initializes the client with API credentials and configuration\nsend_request(messages: List[OpenAIMessage]) -> ChatCompletion - Sends a chat completion request to the API and returns the processed response\n_convert_response(samba_response: Dict[str, Any]) -> ChatCompletion - Converts the raw API response into a standardized ChatCompletion object",
    "Canonical_solution": "import json\nimport uuid\nfrom typing import Any, Dict, List, Optional, Union\n\nimport httpx\nfrom camel.messages import OpenAIMessage\nfrom camel.types import ChatCompletion, CompletionUsage\n\nclass SambaVerseClient:\n    def __init__(self, api_key: str, model_type: str, url: str, model_config_dict: Dict[str, Any]):\n        self._api_key = api_key\n        self.model_type = model_type\n        self._url = url\n        self.model_config_dict = model_config_dict\n\n    def send_request(self, messages: List[OpenAIMessage]) -> ChatCompletion:\n        headers = {\n            \"Content-Type\": \"application/json\",\n            \"key\": str(self._api_key),\n            \"modelName\": self.model_type,\n        }\n\n        data = {\n            \"instance\": json.dumps({\n                \"conversation_id\": str(uuid.uuid4()),\n                \"messages\": messages,\n            }),\n            \"params\": {\n                \"do_sample\": {\"type\": \"bool\", \"value\": \"true\"},\n                \"max_tokens_to_generate\": {\n                    \"type\": \"int\",\n                    \"value\": str(self.model_config_dict.get(\"max_tokens\")),\n                },\n                \"process_prompt\": {\"type\": \"bool\", \"value\": \"true\"},\n                \"repetition_penalty\": {\n                    \"type\": \"float\",\n                    \"value\": str(self.model_config_dict.get(\"repetition_penalty\")),\n                },\n                \"return_token_count_only\": {\"type\": \"bool\", \"value\": \"false\"},\n                \"select_expert\": {\n                    \"type\": \"str\",\n                    \"value\": self.model_type.split('/')[1],\n                },\n                \"stop_sequences\": {\n                    \"type\": \"str\",\n                    \"value\": self.model_config_dict.get(\"stop_sequences\"),\n                },\n                \"temperature\": {\n                    \"type\": \"float\",\n                    \"value\": str(self.model_config_dict.get(\"temperature\")),\n                },\n                \"top_k\": {\n                    \"type\": \"int\",\n                    \"value\": str(self.model_config_dict.get(\"top_k\")),\n                },\n                \"top_p\": {\n                    \"type\": \"float\",\n                    \"value\": str(self.model_config_dict.get(\"top_p\")),\n                },\n            },\n        }\n\n        with httpx.Client() as client:\n            response = client.post(\n                self._url,\n                headers=headers,\n                json=data,\n            )\n\n        raw_text = response.text\n        dicts = raw_text.split('}\\n{')\n        last_dict = '{' + dicts[-1]\n        last_dict = json.loads(last_dict)\n        return self._convert_response(last_dict)\n\n    def _convert_response(self, samba_response: Dict[str, Any]) -> ChatCompletion:\n        choices = [\n            dict(\n                index=0,\n                message={\n                    \"role\": 'assistant',\n                    \"content\": samba_response['result']['responses'][0]['completion'],\n                },\n                finish_reason=samba_response['result']['responses'][0]['stop_reason'],\n            )\n        ]\n\n        return ChatCompletion.construct(\n            id=None,\n            choices=choices,\n            created=int(time.time()),\n            model=self.model_type,\n            object=\"chat.completion\",\n            usage=CompletionUsage(\n                completion_tokens=0,\n                prompt_tokens=0,\n                total_tokens=int(samba_response['result']['responses'][0]['total_tokens_count']),\n            ),\n        )",
    "Code_level": "Class-level",
    "FSMID_for_test": 58,
    "wrong_result": "import requests\nfrom typing import List, Dict, Any\n\nclass SambaVerseClient:\n    def __init__(self, api_key: str, model_type: str, url: str, model_config_dict: Dict[str, Any]) -> None:\n        self._api_key = api_key\n        self.model_type = model_type\n        self._url = url\n        self.model_config_dict = model_config_dict\n\n    def send_request(self, messages: List[Dict[str, Any]]) -> Dict[str, Any]:\n        headers = {\n            'Authorization': f'Bearer {self._api_key}',\n            'Content-Type': 'application/json'\n        }\n        payload = {\n            'model': self.model_type,\n            'messages': messages,\n            **self.model_config_dict\n        }\n        response = requests.post(self._url, headers=headers, json=payload)\n        response.raise_for_status()\n        return self._convert_response(response.json())\n\n    def _convert_response(self, samba_response: Dict[str, Any]) -> Dict[str, Any]:\n        return {\n            'id': samba_response.get('id', ''),\n            'object': samba_response.get('object', 'chat.completion'),\n            'created': samba_response.get('created', 0),\n            'model': samba_response.get('model', self.model_type),\n            'choices': [{\n                'message': {\n                    'role': 'assistant',\n                    'content': choice.get('message', {}).get('content', '')\n                },\n                'finish_reason': choice.get('finish_reason', 'stop'),\n                'index': idx\n            } for idx, choice in enumerate(samba_response.get('choices', []))],\n            'usage': samba_response.get('usage', {})\n        }"
  },
  {
    "Task_id": 34,
    "Instruction": "Task Description: Create a function to crawl content from Weixin Official Account Platform (mp.weixin.qq.com) URLs using HTTP requests with proper error handling and logging.\n\nFunction Description: The function mp_crawler takes a URL and a logger object as input, validates the URL, makes an HTTPS request to the Weixin Official Account Platform, and returns a status code along with response data (empty in case of errors). It includes URL validation, HTTP request handling with timeout, and proper error logging.\n\nInput:\n- url: str - The target URL to crawl (must be from mp.weixin.qq.com domain)\n- logger: logging.Logger - Logger object for recording warnings and errors\n\nOutput:\n- Tuple[int, dict] - Returns a tuple containing:\n  - First element: int status code (-5 for invalid URL, -7 for request failure, or HTTP status code for success)\n  - Second element: dict containing response data (empty dict in case of errors)",
    "Canonical_solution": "import httpx\nfrom bs4 import BeautifulSoup\nfrom datetime import datetime\nimport re\n\nheader = {\n    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/605.1.15 (KHTML, like Gecko) Chrome/112.0.0.0 Safari/604.1 Edg/112.0.100.0'}\n\ndef mp_crawler(url: str, logger) -> (int, dict):\n    if not url.startswith('https://mp.weixin.qq.com') and not url.startswith('http://mp.weixin.qq.com'):\n        logger.warning(f'{url} is not a mp url, you should not use this function')\n        return -5, {}\n\n    url = url.replace(\"http://\", \"https://\", 1)\n\n    try:\n        with httpx.Client() as client:\n            response = client.get(url, headers=header, timeout=30)\n    except Exception as e:\n        logger.warning(f\"cannot get content from {url}\\n{e}\")\n        return -7, {}",
    "Code_level": "Function-level",
    "FSMID_for_test": 58,
    "wrong_result": "import logging\nimport requests\nfrom urllib.parse import urlparse\nfrom typing import Tuple, Dict\n\ndef mp_crawler(url: str, logger: logging.Logger) -> Tuple[int, Dict]:\n    if not url.startswith('https://mp.weixin.qq.com/'):\n        logger.error(f\"Invalid URL domain: {url}\")\n        return (-5, {})\n    \n    try:\n        parsed_url = urlparse(url)\n        if not all([parsed_url.scheme, parsed_url.netloc]):\n            logger.error(f\"Malformed URL: {url}\")\n            return (-5, {})\n    except Exception as e:\n        logger.error(f\"URL parsing error: {e}\")\n        return (-5, {})\n\n    try:\n        headers = {\n            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n        }\n        response = requests.get(url, headers=headers, timeout=10)\n        return (response.status_code, response.json() if response.content else {})\n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Request failed: {e}\")\n        return (-7, {})\n    except ValueError as e:\n        logger.error(f\"JSON decode error: {e}\")\n        return (response.status_code, {})"
  },
  {
    "Task_id": 341,
    "Instruction": "Task Description: Create a Python function that establishes a TCP connection to a server, receives initial data from the server, sends a buffer with a specific prefix, and handles connection errors gracefully.\n\nFunction Description: The function `send_buffer_to_server` creates a TCP socket, connects to a specified server, receives initial data, sends a buffer prefixed with \"GMON /\", and ensures proper connection closure. It includes error handling for connection issues.\n\nInput:\n- host: [str] - The IP address or hostname of the target server\n- port: [int] - The port number on which the server is listening\n- buffer: [str] - The data buffer to be sent to the server (will be prefixed with \"GMON /\")\n\nOutput:\n- None (prints status messages to console: received data, error messages, and confirmation of buffer sending)",
    "Canonical_solution": "import socket\n\ndef send_buffer_to_server(host, port, buffer):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        data = s.recv(1024)\n        print \"\\n\" + data\n        s.send(\"GMON /\" + buffer + '\\r\\n')\n    except:\n        print \"Check your debugger\"\n    finally:\n        s.close()\n        print \"Buffer Sent!\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef send_buffer_to_server(host, port, buffer):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            try:\n                s.connect((host, port))\n                print(f\"Connected to {host}:{port}\")\n                \n                initial_data = s.recv(1024)\n                if initial_data:\n                    print(f\"Received initial data: {initial_data.decode('utf-8', errors='ignore')}\")\n                \n                message = f\"GMON /{buffer}\"\n                s.sendall(message.encode('utf-8'))\n                print(f\"Sent buffer: {message}\")\n                \n            except socket.timeout:\n                print(\"Connection timed out\")\n            except socket.error as e:\n                print(f\"Socket error: {e}\")\n            except Exception as e:\n                print(f\"Unexpected error: {e}\")\n    except Exception as e:\n        print(f\"Failed to create socket: {e}\")"
  },
  {
    "Task_id": 344,
    "Instruction": "Task Description: Create a Python function that establishes a TCP connection to a specified server, sends a payload consisting of a command and optional padding, and then closes the connection.\n\nFunction Description: The function `send_tcp_payload` creates a TCP socket, connects to a target server using the provided IP address and port, sends a payload formed by combining a command string with optional padding, and finally closes the socket connection. This is useful for simple TCP-based client-server communication scenarios.\n\nInput:\n- `ipaddr`: [str] - The IP address of the target server to connect to\n- `port`: [int] - The port number on the target server to connect to\n- `cmd`: [str] - The command string to be sent as part of the payload\n- `padding`: [str] - Optional additional data to be appended to the command\n\nOutput: None (The function performs an action but doesn't return any value)",
    "Canonical_solution": "import socket\n\ndef send_tcp_payload(ipaddr, port, cmd, padding):\n    payload = cmd + padding\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((ipaddr, port))\n    s.send(payload.encode())\n    s.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef send_tcp_payload(ipaddr, port, cmd, padding=''):\n    payload = cmd + padding\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((ipaddr, port))\n        s.sendall(payload.encode())"
  },
  {
    "Task_id": 345,
    "Instruction": "Task Description: Create a Python class that interacts with an APC UPS daemon (apcupsd) via TCP socket to collect and process UPS metrics.\n\nClass Description: The ApcupsdCollector class establishes a TCP connection to an apcupsd server, retrieves status information, and processes the raw data into structured metrics. It handles socket communication, data parsing, and metric filtering.\n\nAttributes:\n- config: [dict] - Configuration dictionary containing hostname (str), port (int), and metrics (list of str) to collect\n\nMethods:\n- __init__(hostname='localhost', port=3551, metrics=None) -> None - Initializes the collector with connection parameters and metrics to collect\n- getData() -> str - Establishes TCP connection, sends status request, and returns raw UPS data\n- collect() -> dict - Processes raw data into structured metrics, returning a dictionary of metric_name: value pairs",
    "Canonical_solution": "import socket\nfrom struct import pack\nimport re\nimport time\n\nclass ApcupsdCollector:\n    def __init__(self, hostname='localhost', port=3551, metrics=None):\n        self.config = {\n            'hostname': hostname,\n            'port': port,\n            'metrics': metrics or ['LINEV', 'LOADPCT', 'BCHARGE', 'TIMELEFT', 'BATTV',\n                                 'NUMXFERS', 'TONBATT', 'MAXLINEV', 'MINLINEV',\n                                 'OUTPUTV', 'ITEMP', 'LINEFREQ', 'CUMONBATT']\n        }\n\n    def getData(self):\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((self.config['hostname'], int(self.config['port'])))\n        s.send(pack('xb6s', 6, 'status'))\n        s.recv(1024)\n        time.sleep(.25)\n        data = s.recv(4096)\n        s.close()\n        return data\n\n    def collect(self):\n        metrics = {}\n        raw = {}\n        data = self.getData()\n        data = data.split('\\n\\x00')\n        \n        for d in data:\n            matches = re.search(\"([A-Z]+)\\s+:\\s+(.*)$\", d)\n            if matches:\n                value = matches.group(2).strip()\n                raw[matches.group(1)] = value\n                vmatch = re.search(\"([0-9.]+)\", value)\n                if not vmatch:\n                    continue\n                try:\n                    value = float(vmatch.group(1))\n                except ValueError:\n                    continue\n                metrics[matches.group(1)] = value\n\n        results = {}\n        for metric in self.config['metrics']:\n            if metric not in metrics:\n                continue\n            metric_name = \"%s.%s\" % (raw['UPSNAME'], metric)\n            value = metrics[metric]\n            results[metric_name] = value\n\n        return results",
    "Code_level": "Class-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport re\n\nclass ApcupsdCollector:\n    def __init__(self, hostname='localhost', port=3551, metrics=None):\n        self.config = {\n            'hostname': hostname,\n            'port': port,\n            'metrics': metrics if metrics else []\n        }\n\n    def getData(self):\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as sock:\n                sock.settimeout(5)\n                sock.connect((self.config['hostname'], self.config['port']))\n                sock.sendall(b\"status\\n\")\n                data = sock.recv(4096).decode('utf-8')\n                while True:\n                    chunk = sock.recv(4096).decode('utf-8')\n                    if not chunk:\n                        break\n                    data += chunk\n                return data\n        except Exception as e:\n            raise Exception(f\"Failed to fetch UPS data: {str(e)}\")\n\n    def collect(self):\n        raw_data = self.getData()\n        metrics = {}\n        for line in raw_data.split('\\n'):\n            if not line.strip():\n                continue\n            match = re.match(r'^([A-Z]+[A-Z0-9_]*)\\s*:\\s*(.*?)\\s*$', line)\n            if match:\n                key, value = match.groups()\n                if not self.config['metrics'] or key in self.config['metrics']:\n                    metrics[key] = value\n        return metrics"
  },
  {
    "Task_id": 349,
    "Instruction": "Task Description: Create a Python class that provides TCP socket functionality including connection establishment, data transmission, and SSL support for secure communication.\n\nClass Description: The SocketLibrary class encapsulates TCP socket operations with support for both regular and SSL-wrapped connections. It provides methods to establish connections, send/receive data, and gather connection information.\n\nAttributes: None (This is a utility class without persistent attributes)\n\nMethods:\n\nMethod1: create_tcp_socket(host: str, port: int, timeout: float) -> Optional[Tuple[socket.socket, bool]] - Creates and returns a TCP socket connection to the specified host and port. Attempts SSL connection first, falls back to regular TCP if SSL fails. Returns a tuple containing the socket object and SSL flag (True if SSL succeeded).\n\nMethod2: tcp_connect_only(host: str, port: int, timeout: float) -> Optional[Dict[str, Any]] - Establishes a TCP connection to verify connectivity and returns connection details including peer information, service name, and SSL status without data transmission.\n\nMethod3: tcp_connect_send_and_receive(host: str, port: int, timeout: float) -> Optional[Dict[str, Any]] - Establishes a TCP connection, sends test data, receives response, and returns connection details including peer information, service name, response data, and SSL status.",
    "Canonical_solution": "import socket\nimport ssl\nfrom typing import Optional, Tuple, Dict, Any\n\nclass SocketLibrary:\n    def create_tcp_socket(self, host: str, port: int, timeout: float) -> Optional[Tuple[socket.socket, bool]]:\n        try:\n            socket_connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            socket_connection.settimeout(timeout)\n            socket_connection.connect((host, port))\n            ssl_flag = False\n        except ConnectionRefusedError:\n            return None\n\n        try:\n            socket_connection = ssl.wrap_socket(socket_connection)\n            ssl_flag = True\n        except Exception:\n            socket_connection = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            socket_connection.settimeout(timeout)\n            socket_connection.connect((host, port))\n\n        return socket_connection, ssl_flag\n\n    def tcp_connect_only(self, host: str, port: int, timeout: float) -> Optional[Dict[str, Any]]:\n        tcp_socket = self.create_tcp_socket(host, port, timeout)\n        if tcp_socket is None:\n            return None\n\n        socket_connection, ssl_flag = tcp_socket\n        peer_name = socket_connection.getpeername()\n        socket_connection.close()\n        return {\n            \"peer_name\": peer_name,\n            \"service\": socket.getservbyport(int(port)),\n            \"ssl_flag\": ssl_flag,\n        }\n\n    def tcp_connect_send_and_receive(self, host: str, port: int, timeout: float) -> Optional[Dict[str, Any]]:\n        tcp_socket = self.create_tcp_socket(host, port, timeout)\n        if tcp_socket is None:\n            return None\n\n        socket_connection, ssl_flag = tcp_socket\n        peer_name = socket_connection.getpeername()\n        try:\n            socket_connection.send(b\"ABC\\x00\\r\\n\\r\\n\\r\\n\" * 10)\n            response = socket_connection.recv(1024 * 1024 * 10)\n            socket_connection.close()\n        except Exception:\n            try:\n                socket_connection.close()\n                response = b\"\"\n            except Exception:\n                response = b\"\"\n        return {\n            \"peer_name\": peer_name,\n            \"service\": socket.getservbyport(port),\n            \"response\": response.decode(errors=\"ignore\"),\n            \"ssl_flag\": ssl_flag,\n        }",
    "Code_level": "Class-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport ssl\nfrom typing import Optional, Tuple, Dict, Any\n\nclass SocketLibrary:\n    @staticmethod\n    def create_tcp_socket(host: str, port: int, timeout: float) -> Optional[Tuple[socket.socket, bool]]:\n        try:\n            sock = socket.create_connection((host, port), timeout=timeout)\n            try:\n                context = ssl.create_default_context()\n                ssl_sock = context.wrap_socket(sock, server_hostname=host)\n                return (ssl_sock, True)\n            except ssl.SSLError:\n                return (sock, False)\n        except (socket.timeout, socket.error):\n            return None\n\n    @staticmethod\n    def tcp_connect_only(host: str, port: int, timeout: float) -> Optional[Dict[str, Any]]:\n        result = SocketLibrary.create_tcp_socket(host, port, timeout)\n        if not result:\n            return None\n        sock, is_ssl = result\n        try:\n            peer_info = sock.getpeername()\n            service_name = socket.getservbyport(port) if port <= 65535 else None\n            return {\n                'host': host,\n                'port': port,\n                'peer_address': peer_info[0],\n                'peer_port': peer_info[1],\n                'service_name': service_name,\n                'ssl_enabled': is_ssl\n            }\n        finally:\n            sock.close()\n\n    @staticmethod\n    def tcp_connect_send_and_receive(host: str, port: int, timeout: float) -> Optional[Dict[str, Any]]:\n        result = SocketLibrary.create_tcp_socket(host, port, timeout)\n        if not result:\n            return None\n        sock, is_ssl = result\n        try:\n            peer_info = sock.getpeername()\n            service_name = socket.getservbyport(port) if port <= 65535 else None\n            test_data = b\"TEST\"\n            sock.sendall(test_data)\n            response = sock.recv(4096)\n            return {\n                'host': host,\n                'port': port,\n                'peer_address': peer_info[0],\n                'peer_port': peer_info[1],\n                'service_name': service_name,\n                'ssl_enabled': is_ssl,\n                'sent_data': test_data,\n                'received_data': response\n            }\n        finally:\n            sock.close()"
  },
  {
    "Task_id": 352,
    "Instruction": "Task Description: Write a Python function that tests command responses from a Kamstrup management server by sending specific commands and verifying the responses against expected values.\n\nFunction Description: The function establishes a TCP connection to a Kamstrup management server, sends a help command and a specific packet command, then verifies if the responses match the expected values from a predefined response dictionary. The function returns True only if both responses match their expected values.\n\nInput:\n- packet_type: [str] - The key used to look up the expected response in the RESPONSES dictionary for the packet command.\n- help_msg_command: [bytes] - The byte string representing the help command to be sent to the server.\n- packet_msg_command: [bytes] - The byte string representing the packet command to be sent to the server.\n- kamstrup_management_server: [object] - An object containing the server's connection details, specifically the server port.\n\nOutput:\n- [bool] - Returns True if both the help command response and packet command response match their expected values in the RESPONSES dictionary, otherwise returns False.",
    "Canonical_solution": "from gevent import socket\nfrom conpot.tests.data.kamstrup_management_data import RESPONSES\n\ndef check_command_resp_help_message(\n    packet_type, help_msg_command, packet_msg_command, kamstrup_management_server\n):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((\"127.0.0.1\", kamstrup_management_server.server.server_port))\n    _ = s.recv(1024)  # receive the banner\n    s.sendall(help_msg_command)  # test the help command\n    help_data = s.recv(1024)\n    help_response = help_data == RESPONSES[\"H\"][packet_type]\n    s.sendall(packet_msg_command)\n    pkt_data = s.recv(1024)\n    packet_resp = pkt_data == RESPONSES[packet_type]\n    s.close()\n    return help_response and packet_resp",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\nRESPONSES = {\n    'help': b'Supported commands: help, packet, quit',\n    'packet1': b'Packet type 1 response',\n    'packet2': b'Packet type 2 response',\n    'packet3': b'Packet type 3 response'\n}\n\ndef test_kamstrup_responses(packet_type, help_msg_command, packet_msg_command, kamstrup_management_server):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.connect(('localhost', kamstrup_management_server.server_port))\n            \n            s.sendall(help_msg_command)\n            help_response = s.recv(1024)\n            \n            s.sendall(packet_msg_command)\n            packet_response = s.recv(1024)\n            \n            expected_help = RESPONSES['help']\n            expected_packet = RESPONSES.get(packet_type, b'Unknown packet type')\n            \n            return help_response == expected_help and packet_response == expected_packet\n    except:\n        return False"
  },
  {
    "Task_id": 353,
    "Instruction": "Task Description: Implement a TCP client function in Python that communicates with a server using sockets. The function should establish a connection, send a message, receive a response, and close the connection.\n\nFunction Description: The function `tcp_client_communication` creates a TCP socket, connects to a specified server, sends a message, receives a response, and then closes the connection. It uses IPv4 for the socket connection.\n\nInput:\n- `host` (str, optional): The IP address or hostname of the server. Defaults to 'localhost'.\n- `port` (int, optional): The port number on which the server is listening. Defaults to 16000.\n- `message` (bytes, optional): The message to be sent to the server. Defaults to b'Hello\\n'.\n\nOutput:\n- `response` (bytes): The data received from the server as a response.",
    "Canonical_solution": "from socket import socket, AF_INET, SOCK_STREAM\n\ndef tcp_client_communication(host='localhost', port=16000, message=b'Hello\\n'):\n    s = socket(AF_INET, SOCK_STREAM)\n    s.connect((host, port))\n    s.send(message)\n    response = s.recv(8192)\n    s.close()\n    return response",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client_communication(host='localhost', port=16000, message=b'Hello\\n'):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((host, port))\n        s.sendall(message)\n        response = s.recv(1024)\n    return response"
  },
  {
    "Task_id": 357,
    "Instruction": "Task Description: Create a Python function that tests TCP connectivity to a target server through a SOCKS5 proxy, sends an HTTP request, and measures the response time.\n\nFunction Description: The function establishes a TCP connection to a specified target server through a SOCKS5 proxy, sends a basic HTTP GET request, verifies the response, and measures the time taken for the entire operation. The function handles connection setup, data transmission, response validation, and cleanup.\n\nInput: None (The target server address 'TARGET' is defined as a module-level constant)\n\nOutput: None (The function prints diagnostic messages and timing information to stdout)",
    "Canonical_solution": "import socket\nimport socks\nimport time\n\nTARGET = \"1.1.1.1\"\n\ndef test_tcp() -> None:\n    s = socks.socksocket(socket.AF_INET, socket.SOCK_STREAM)\n    s.set_proxy(socks.SOCKS5, \"127.0.0.1\", 1080)\n\n    print(f\"TCP - Sending HTTP request to {TARGET}\")\n    start = time.time()\n    s.connect((TARGET, 80))\n    s.send(b\"GET / HTTP/1.1\\r\\nHost: \" + TARGET.encode() + b\"\\r\\n\\r\\n\")\n    data = s.recv(1024)\n    if not data:\n        print(\"No data received\")\n    elif not data.startswith(b\"HTTP/1.1 \"):\n        print(\"Invalid response received\")\n    else:\n        print(\"TCP test passed\")\n    end = time.time()\n    s.close()\n\n    print(f\"Time: {round((end - start) * 1000, 2)} ms\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport socks\nimport time\nfrom urllib.request import urlopen, Request\nfrom urllib.error import URLError\n\nTARGET = \"http://example.com\"\n\ndef test_socks5_proxy_connectivity(proxy_host, proxy_port):\n    start_time = time.time()\n    \n    try:\n        socks.set_default_proxy(socks.SOCKS5, proxy_host, proxy_port)\n        socket.socket = socks.socksocket\n        \n        req = Request(TARGET, headers={'User-Agent': 'Mozilla/5.0'})\n        response = urlopen(req, timeout=10)\n        \n        if response.getcode() == 200:\n            print(f\"Successfully connected to {TARGET}\")\n            print(f\"Response code: {response.getcode()}\")\n        else:\n            print(f\"Unexpected response code: {response.getcode()}\")\n            \n    except socks.ProxyConnectionError as e:\n        print(f\"Failed to connect to SOCKS5 proxy: {e}\")\n    except URLError as e:\n        print(f\"Failed to connect to target: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")\n    finally:\n        socks.set_default_proxy()\n        socket.socket = socket._socketobject\n        \n    end_time = time.time()\n    elapsed_time = end_time - start_time\n    print(f\"Total operation time: {elapsed_time:.2f} seconds\")"
  },
  {
    "Task_id": 358,
    "Instruction": "Task Description: Create a Python function that establishes a TCP client connection to a server, sends a JSON-formatted command with arguments, receives the server's response, and then closes the connection.\n\nFunction Description: The function `connect_client` creates a TCP socket, connects to a specified server, sends a command with arguments in JSON format, waits for and prints the server's response, and finally closes the socket connection.\n\nInput:\n- `machine` (str): The IP address or hostname of the target server.\n- `port` (int): The port number on which the server is listening.\n- `blendpath` (str): A file path or identifier to be included in the command arguments.\n- `obj_num` (int): A numerical argument to be included in the command arguments.\n\nOutput:\n- The function does not return any value but prints the server's response to the console.",
    "Canonical_solution": "import socket, json\n\ndef connect_client(machine, port, blendpath, obj_num):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.connect((machine, port))\n    args = [blendpath, obj_num]\n    command = json.dumps({'call':1, 'command':1, 'args':args})\n    sock.send((command).encode())\n    response_data = sock.recv(1024)\n    print(response_data.decode())\n    sock.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport json\n\ndef connect_client(machine, port, blendpath, obj_num):\n    command = {\n        'command': 'process',\n        'args': {\n            'blendpath': blendpath,\n            'obj_num': obj_num\n        }\n    }\n    json_command = json.dumps(command).encode('utf-8')\n    \n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((machine, port))\n        s.sendall(json_command)\n        response = s.recv(1024)\n        print(response.decode('utf-8'))"
  },
  {
    "Task_id": 360,
    "Instruction": "Task Description: Create a Python function that establishes a TCP connection to a server, receives initial data from the server, sends a buffer with a specific prefix, and handles connection errors gracefully.\n\nFunction Description: The function `send_buffer_to_server` creates a TCP socket, connects to a specified server, receives initial data, sends a buffer prefixed with \"GMON /\", and ensures proper connection closure. It includes error handling for connection issues.\n\nInput:\n- host: [str] - The IP address or hostname of the target server\n- port: [int] - The port number on which the server is listening\n- buffer: [str] - The data buffer to be sent to the server (will be prefixed with \"GMON /\")\n\nOutput:\n- None (prints status messages to console: received data, error notifications, and confirmation of buffer sending)",
    "Canonical_solution": "import socket\n\ndef send_buffer_to_server(host, port, buffer):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        data = s.recv(1024)\n        print \"\\n\" + data\n        s.send(\"GMON /\" + buffer + '\\r\\n')\n    except:\n        print \"Check your debugger\"\n    finally:\n        s.close()\n        print \"Buffer Sent!\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef send_buffer_to_server(host, port, buffer):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            try:\n                s.connect((host, port))\n                print(f\"Connected to {host}:{port}\")\n                \n                initial_data = s.recv(1024)\n                if initial_data:\n                    print(f\"Received initial data: {initial_data.decode('utf-8', errors='replace')}\")\n                \n                message = f\"GMON /{buffer}\"\n                s.sendall(message.encode('utf-8'))\n                print(f\"Sent buffer with prefix: {message}\")\n                \n            except socket.timeout:\n                print(\"Connection timed out\")\n            except socket.error as e:\n                print(f\"Socket error: {e}\")\n            except Exception as e:\n                print(f\"Unexpected error: {e}\")\n    except Exception as e:\n        print(f\"Failed to create socket: {e}\")"
  },
  {
    "Task_id": 364,
    "Instruction": "Task Description: Implement a TCP client function in Python that connects to a server, receives initial data, sends a formatted message, and handles connection errors gracefully.\n\nFunction Description: The function establishes a TCP connection to a specified server, receives initial data from the server, sends a formatted message (prefixed with \"USV \" and suffixed with '\\r\\n\\r\\n'), and ensures proper connection closure. Error handling is included to manage connection issues.\n\nInput:\n- host: str - The IP address or hostname of the target server\n- port: int - The port number on which the server is listening\n- buffer: str - The message content to be sent to the server\n\nOutput:\n- None (prints received data and status messages to console)\n- Success: Prints \"Buffer Sent Successfully!\" upon completion\n- Failure: Prints \"Check your debugger\" if connection fails",
    "Canonical_solution": "import socket\n\ndef tcp_client_interaction(host, port, buffer):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        data = s.recv(1024)\n        print(\"\\n\" + data)\n        s.send(\"USV \" + buffer + '\\r\\n\\r\\n')\n    except:\n        print(\"Check your debugger\")\n    finally:\n        s.close()\n        print(\"Buffer Sent Successfully!\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client(host, port, buffer):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            s.connect((host, port))\n            initial_data = s.recv(1024)\n            print(f\"Received initial data: {initial_data.decode()}\")\n            formatted_msg = f\"USV {buffer}\\r\\n\\r\\n\"\n            s.sendall(formatted_msg.encode())\n            print(\"Buffer Sent Successfully!\")\n    except socket.timeout:\n        print(\"Connection timed out. Check your debugger\")\n    except ConnectionRefusedError:\n        print(\"Connection refused. Check your debugger\")\n    except Exception as e:\n        print(f\"An error occurred: {e}. Check your debugger\")"
  },
  {
    "Task_id": 365,
    "Instruction": "Task Description: Create a Python function that implements a TCP client for basic communication with a server using sockets. The function should establish a connection, send a message, receive a response, and close the connection.\n\nFunction Description: The function `tcp_client_communication` creates a TCP socket, connects to a specified server, sends a message, receives a response, and then closes the connection. It handles IPv4 addresses and uses a stream-oriented connection (TCP).\n\nInput:\n- `host` (str, optional): The IP address or hostname of the server. Defaults to 'localhost'.\n- `port` (int, optional): The port number on which the server is listening. Defaults to 20000.\n- `message` (bytes, optional): The message to send to the server. Defaults to b'Hello\\n'.\n\nOutput:\n- `resp` (bytes): The response received from the server.",
    "Canonical_solution": "from socket import socket, AF_INET, SOCK_STREAM\n\ndef tcp_client_communication(host='localhost', port=20000, message=b'Hello\\n'):\n    s = socket(AF_INET, SOCK_STREAM)\n    s.connect((host, port))\n    s.send(message)\n    resp = s.recv(8192)\n    s.close()\n    return resp",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\n\ndef tcp_client_communication(host='localhost', port=20000, message=b'Hello\\n'):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((host, port))\n        s.sendall(message)\n        resp = s.recv(1024)\n    return resp"
  },
  {
    "Task_id": 371,
    "Instruction": "Task Description: Implement a TCP client function in Python that connects to a server, receives initial data, sends a formatted message, and handles connection errors gracefully.\n\nFunction Description: The function establishes a TCP connection to a specified server, receives initial data from the server, sends a formatted message (prefixed with \"USV \" and suffixed with '\\r\\n\\r\\n'), and ensures proper connection closure. Error handling is included to manage connection issues.\n\nInput:\n- host: [str] - The IP address or hostname of the target server\n- port: [int] - The port number on which the server is listening\n- buffer: [str] - The message content to be sent to the server\n\nOutput:\n- None (prints received data and status messages to console)\n- Prints \"Buffer Sent Successfully!\" upon successful completion\n- Prints \"Check your debugger\" if connection errors occur",
    "Canonical_solution": "import socket\n\ndef tcp_client_interaction(host, port, buffer):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        s.connect((host, port))\n        data = s.recv(1024)\n        print(\"\\n\" + data)\n        s.send(\"USV \" + buffer + '\\r\\n\\r\\n')\n    except:\n        print(\"Check your debugger\")\n    finally:\n        s.close()\n        print(\"Buffer Sent Successfully!\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport sys\n\ndef tcp_client(host, port, buffer):\n    try:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.settimeout(5)\n            try:\n                s.connect((host, port))\n                print(f\"Connected to {host}:{port}\")\n                initial_data = s.recv(1024)\n                if initial_data:\n                    print(f\"Received: {initial_data.decode('utf-8')}\")\n                formatted_msg = f\"USV {buffer}\\r\\n\\r\\n\"\n                s.sendall(formatted_msg.encode('utf-8'))\n                print(\"Buffer Sent Successfully!\")\n            except socket.timeout:\n                print(\"Connection timed out. Check your debugger\")\n            except ConnectionRefusedError:\n                print(\"Connection refused. Check your debugger\")\n            except socket.error as e:\n                print(f\"Socket error: {e}. Check your debugger\")\n    except Exception as e:\n        print(f\"Error: {e}. Check your debugger\")"
  },
  {
    "Task_id": 373,
    "Instruction": "Task Description: Implement a TCP client in Python that can handle multiple types of HTTP requests concurrently using threading and locking mechanisms to manage shared resources.\n\nFunction Description: The code consists of two main functions that perform HTTP GET requests to a server. `long_request` handles a request with a large response, processing it in chunks with delays, while `time_request` handles a quicker request. A threading lock is used to synchronize access to shared resources between these concurrent requests.\n\nInput:\n- `hostname` (str): The target server's hostname or IP address\n- `port` (int): The target server's port number\n- `headers` (str): Additional HTTP headers to include in the request\n\nOutput:\n- Both functions print status messages ('acquire' and 'release') to indicate lock status\n- No explicit return value, but data is received from the server and processed\n- Network connections are properly closed when complete\n\nMethods:\n- `long_request(hostname, port, headers) -> None`: \n  Performs a long-running HTTP GET request, receiving data in chunks with delays between receives. Releases a lock after initial processing and continues receiving more data.\n\n- `time_request(hostname, port, headers) -> None`: \n  Performs a quicker HTTP GET request, acquires a lock before execution, receives all data at once, and closes the connection.\n\nShared Resource:\n- `lock` (threading.Lock): A threading lock used to synchronize access between the two request functions",
    "Canonical_solution": "import sys\nimport socket\nimport time\nimport threading\n\ndef long_request(hostname, port, headers):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((hostname, port))\n    s.send('GET /huge HTTP/1.0' + headers)\n    for i in range(1):\n        s.recv(80)\n        time.sleep(0.1)\n    lock.release()\n    print('release')\n    for i in range(10):\n        s.recv(80)\n        time.sleep(0.1)\n    s.close()\n\ndef time_request(hostname, port, headers):\n    lock.acquire()\n    print('acquire')\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.connect((hostname, port))\n    s.send('GET /tuple HTTP/1.0' + headers)\n    s.recv(900)\n    s.close()\n\nlock = threading.Lock()",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport threading\nimport time\n\nlock = threading.Lock()\n\ndef long_request(hostname, port, headers):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((hostname, port))\n        request = f\"GET /long HTTP/1.1\\r\\nHost: {hostname}\\r\\n{headers}\\r\\n\\r\\n\"\n        s.sendall(request.encode())\n        \n        lock.acquire()\n        try:\n            print('acquire')\n            data = s.recv(1024)\n            print(f\"Received initial chunk: {data.decode()}\")\n        finally:\n            lock.release()\n            print('release')\n        \n        time.sleep(1)\n        while True:\n            data = s.recv(1024)\n            if not data:\n                break\n            print(f\"Received subsequent chunk: {data.decode()}\")\n\ndef time_request(hostname, port, headers):\n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.connect((hostname, port))\n        request = f\"GET /time HTTP/1.1\\r\\nHost: {hostname}\\r\\n{headers}\\r\\n\\r\\n\"\n        s.sendall(request.encode())\n        \n        lock.acquire()\n        try:\n            print('acquire')\n            data = s.recv(4096)\n            print(f\"Received data: {data.decode()}\")\n        finally:\n            lock.release()\n            print('release')"
  },
  {
    "Task_id": 374,
    "Instruction": "Task Description: The task involves creating a Python function that reads log files, extracts specific messages, and sends them to a server using TCP socket communication. The function should parse log entries, identify relevant messages, and transmit them to the appropriate server ports based on the protocol specified in the logs.\n\nFunction Description: The function `send_messages_to_server` processes a log file to extract session IDs, protocols, and hexadecimal messages. It then establishes TCP connections to servers based on the protocols (using ports defined in an XML configuration file) and sends the decoded messages to the respective servers. The function includes a delay between messages to avoid overwhelming the server.\n\nInput: \n- `path` (str): The file path to the log file that contains the messages to be processed and sent. The log file should have entries matching the pattern `YYYY-MM-DD HH:MM:SS  INFO: [SESSION_ID: PROTOCOL < IP_ADDRESS] HEX_MESSAGE`.\n\nOutput: \n- None: The function does not return any value but performs side effects by sending messages to the server and closing the connections.\n\nNote: The function relies on an external XML file (`default.xml`) located in a `../setup/` directory relative to the script's location. This XML file should contain port mappings for different protocols under entries with keys ending in `.port`.",
    "Canonical_solution": "import sys\nimport re\nimport os\nimport xml.etree.ElementTree\nimport socket\nimport binascii\nimport time\n\ndef load_ports():\n    ports = {}\n    dir = os.path.dirname(os.path.abspath(__file__))\n    root = xml.etree.ElementTree.parse(dir + '/../setup/default.xml').getroot()\n    for entry in root.findall('entry'):\n        key = entry.attrib['key']\n        if key.endswith('.port'):\n            ports[key[:-5]] = int(entry.text)\n    return ports\n\ndef send_messages_to_server(path):\n    p = re.compile(r\"\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}  INFO: \\[([TU][0-9a-fA-F]{8}): (\\S+) < [\\d.]+] ([0-9a-fA-F]+)\")\n    ports = load_ports()\n    protocols = {}\n    messages = {}\n\n    for line in open(path):\n        m = p.match(line)\n        if m:\n            session = m.group(1)\n            protocol = m.group(2)\n            message = m.group(3)\n            protocols[session] = protocol\n            if session not in messages:\n                messages[session] = []\n            messages[session].append(message)\n\n    for session in protocols:\n        port = ports[protocols[session]]\n        s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        s.connect((\"localhost\", int(port)))\n        for message in messages[session]:\n            s.send(binascii.unhexlify(message))\n            time.sleep(0.1)\n        s.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 67,
    "wrong_result": "import socket\nimport time\nimport re\nimport os\nimport xml.etree.ElementTree as ET\n\ndef send_messages_to_server(path):\n    with open(path, 'r') as file:\n        logs = file.readlines()\n\n    config_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'setup', 'default.xml')\n    tree = ET.parse(config_path)\n    root = tree.getroot()\n    ports = {}\n    for elem in root.iter():\n        if elem.tag.endswith('.port'):\n            protocol = elem.tag[:-5]\n            ports[protocol] = int(elem.text)\n\n    pattern = re.compile(r'\\[\\s*([^:]+):\\s*([^\\s<]+)\\s*<\\s*([^\\]]+)\\]\\s*([0-9A-Fa-f]+)')\n    for log in logs:\n        match = pattern.search(log)\n        if not match:\n            continue\n        session_id, protocol, ip_address, hex_message = match.groups()\n        if protocol not in ports:\n            continue\n        port = ports[protocol]\n        message = bytes.fromhex(hex_message)\n        try:\n            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n                s.connect((ip_address, port))\n                s.sendall(message)\n                time.sleep(0.1)\n        except Exception:\n            continue"
  },
  {
    "Task_id": 38,
    "Instruction": "Task Description: Implement a function that handles Server-Sent Events (SSE) from an HTTP POST request, processes the incoming data streams, and yields appropriate message objects or raises errors when encountered.\n\nFunction Description: The function `_sse_post` establishes an SSE connection to a specified URL, sends a POST request with provided data and headers, and processes the incoming event stream. It handles different message types by yielding appropriate message objects (ReasoningMessage, ToolCallMessage, ToolReturnMessage, LettaUsageStatistics) or stream status updates (MessageStreamStatus). It also handles errors, including context window errors and JSON parsing errors, by raising appropriate exceptions.\n\nInput:\n- url: str - The URL to which the POST request will be sent.\n- data: dict - The JSON data to be sent in the POST request body.\n- headers: dict - The headers to be included in the POST request.\n\nOutput:\n- Generator[LettaStreamingResponse, None, None] - A generator that yields objects of type LettaStreamingResponse (which can be ReasoningMessage, ToolCallMessage, ToolReturnMessage, LettaUsageStatistics, or MessageStreamStatus) as the events are received from the server. Raises LLMError for context window errors or other appropriate exceptions for other errors.",
    "Canonical_solution": "import json\nfrom typing import Generator\nimport httpx\nfrom httpx_sse import SSEError, connect_sse\nfrom letta.constants import OPENAI_CONTEXT_WINDOW_ERROR_SUBSTRING\nfrom letta.errors import LLMError\nfrom letta.schemas.enums import MessageStreamStatus\nfrom letta.schemas.letta_message import ReasoningMessage, ToolCallMessage, ToolReturnMessage\nfrom letta.schemas.letta_response import LettaStreamingResponse\nfrom letta.schemas.usage import LettaUsageStatistics\n\ndef _sse_post(url: str, data: dict, headers: dict) -> Generator[LettaStreamingResponse, None, None]:\n    with httpx.Client() as client:\n        with connect_sse(client, method=\"POST\", url=url, json=data, headers=headers) as event_source:\n            if not event_source.response.is_success:\n                response_bytes = event_source.response.read()\n                response_dict = json.loads(response_bytes.decode(\"utf-8\"))\n                error_message = response_dict[\"error\"][\"message\"]\n                if OPENAI_CONTEXT_WINDOW_ERROR_SUBSTRING in error_message:\n                    raise LLMError(error_message)\n                event_source.response.raise_for_status()\n\n            try:\n                for sse in event_source.iter_sse():\n                    if sse.data in [status.value for status in MessageStreamStatus]:\n                        yield MessageStreamStatus(sse.data)\n                    else:\n                        chunk_data = json.loads(sse.data)\n                        if \"reasoning\" in chunk_data:\n                            yield ReasoningMessage(**chunk_data)\n                        elif \"tool_call\" in chunk_data:\n                            yield ToolCallMessage(**chunk_data)\n                        elif \"tool_return\" in chunk_data:\n                            yield ToolReturnMessage(**chunk_data)\n                        elif \"step_count\" in chunk_data:\n                            yield LettaUsageStatistics(**chunk_data)\n                        else:\n                            raise ValueError(f\"Unknown message type in chunk_data: {chunk_data}\")\n\n            except SSEError as e:\n                if \"application/json\" in str(e):\n                    response = client.post(url=url, json=data, headers=headers)\n                    if response.headers[\"Content-Type\"].startswith(\"application/json\"):\n                        error_details = response.json()\n                raise e\n\n            except Exception as e:\n                raise e",
    "Code_level": "Function-level",
    "FSMID_for_test": 58,
    "wrong_result": "import json\nimport requests\nfrom typing import Generator, Dict, Any\n\nclass LettaStreamingResponse:\n    pass\n\nclass ReasoningMessage(LettaStreamingResponse):\n    def __init__(self, content: str):\n        self.content = content\n\nclass ToolCallMessage(LettaStreamingResponse):\n    def __init__(self, tool_name: str, parameters: Dict[str, Any]):\n        self.tool_name = tool_name\n        self.parameters = parameters\n\nclass ToolReturnMessage(LettaStreamingResponse):\n    def __init__(self, tool_name: str, result: Any):\n        self.tool_name = tool_name\n        self.result = result\n\nclass LettaUsageStatistics(LettaStreamingResponse):\n    def __init__(self, stats: Dict[str, Any]):\n        self.stats = stats\n\nclass MessageStreamStatus(LettaStreamingResponse):\n    def __init__(self, status: str):\n        self.status = status\n\nclass LLMError(Exception):\n    pass\n\ndef _sse_post(url: str, data: dict, headers: dict) -> Generator[LettaStreamingResponse, None, None]:\n    try:\n        response = requests.post(url, json=data, headers=headers, stream=True)\n        response.raise_for_status()\n\n        buffer = \"\"\n        for chunk in response.iter_content(chunk_size=1024):\n            if chunk:\n                buffer += chunk.decode('utf-8')\n                while '\\n\\n' in buffer:\n                    event, buffer = buffer.split('\\n\\n', 1)\n                    if not event.strip():\n                        continue\n                    lines = event.split('\\n')\n                    event_data = {}\n                    for line in lines:\n                        if ':' in line:\n                            key, value = line.split(':', 1)\n                            event_data[key.strip()] = value.strip()\n                    \n                    if 'event' not in event_data or 'data' not in event_data:\n                        continue\n\n                    event_type = event_data['event']\n                    data = event_data['data']\n                    \n                    try:\n                        parsed_data = json.loads(data)\n                    except json.JSONDecodeError:\n                        raise ValueError(f\"Failed to parse JSON data: {data}\")\n\n                    if event_type == 'reasoning':\n                        yield ReasoningMessage(parsed_data.get('content', ''))\n                    elif event_type == 'tool_call':\n                        yield ToolCallMessage(\n                            parsed_data.get('tool_name', ''),\n                            parsed_data.get('parameters', {})\n                        )\n                    elif event_type == 'tool_return':\n                        yield ToolReturnMessage(\n                            parsed_data.get('tool_name', ''),\n                            parsed_data.get('result', None)\n                        )\n                    elif event_type == 'usage':\n                        yield LettaUsageStatistics(parsed_data)\n                    elif event_type == 'status':\n                        yield MessageStreamStatus(parsed_data.get('status', ''))\n                    elif event_type == 'error':\n                        if parsed_data.get('type') == 'context_window':\n                            raise LLMError(\"Context window error\")\n                        else:\n                            raise ValueError(f\"Server error: {parsed_data.get('message', 'Unknown error')}\")\n\n    except requests.exceptions.RequestException as e:\n        raise ValueError(f\"Request failed: {str(e)}\")"
  },
  {
    "Task_id": 380,
    "Instruction": "Task Description: Implement a TCP server that can handle multiple client connections, process incoming packets, and send replies asynchronously using Python's socket module and Scapy's networking capabilities.\n\nClass Description: AnsweringMachineTCP is a TCP server implementation that listens on a specified port, accepts client connections, and processes incoming packets using Scapy's packet handling capabilities. It supports asynchronous packet sniffing and reply generation for each connected client.\n\nAttributes:\nport: [int] - The port number on which the server listens for incoming connections\ncls: [Type[Packet]] - The Scapy packet class used for parsing incoming data\noptsniff: [dict] - Dictionary of additional options for the sniffer configuration\nsniffer: [threading.Thread] - Background thread running the sniffing process\n\nMethods:\n__init__(port=80, cls=conf.raw_layer, **kargs) -> [None] - Initializes the answering machine with specified port, packet class, and additional options\nclose() -> [None] - Closes all resources (placeholder implementation)\nreply(pkt, send_function=None, address=None) -> [None] - Processes incoming packets and sends replies if appropriate\nsniff() -> [None] - Main server loop that accepts connections and starts sniffers for each client\nsniff_bg() -> [None] - Starts the sniffing process in a background thread\nis_request(req) -> [bool] - Determines if a packet should be processed (default always True)\nmake_reply(req, address=None) -> [Packet] - Generates a reply packet (default returns input packet)\nsend_reply(reply, send_function=None) -> [None] - Sends the generated reply to the client",
    "Canonical_solution": "import socket\nfrom typing import Optional, Type, Any\nfrom scapy.arch import get_if_addr\nfrom scapy.config import conf\nfrom scapy.packet import Packet\nfrom scapy.supersocket import StreamSocket\nimport functools\nfrom scapy.sendrecv import AsyncSniffer\n\nclass AnsweringMachineTCP:\n    \"\"\"\n    An answering machine that use the classic socket.socket to\n    answer multiple TCP clients\n    \"\"\"\n    TYPE = socket.SOCK_STREAM\n\n    def __init__(self, port=80, cls=conf.raw_layer, **kargs):\n        self.port = port\n        self.cls = cls\n        self.optsniff = kargs\n\n    def close(self):\n        pass\n\n    def reply(self, pkt, send_function=None, address=None):\n        if not self.is_request(pkt):\n            return\n        if address:\n            reply = self.make_reply(pkt, address=address)\n        else:\n            reply = self.make_reply(pkt)\n        if not reply:\n            return\n        if send_function:\n            self.send_reply(reply, send_function=send_function)\n        else:\n            self.send_reply(reply)\n\n    def sniff(self):\n        ssock = socket.socket(socket.AF_INET, self.TYPE)\n        try:\n            ssock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        except OSError:\n            pass\n        ssock.bind(\n            (get_if_addr(self.optsniff.get(\"iface\", conf.iface)), self.port))\n        ssock.listen()\n        sniffers = []\n        try:\n            while True:\n                clientsocket, address = ssock.accept()\n                sock = StreamSocket(clientsocket, self.cls)\n                optsniff = self.optsniff.copy()\n                optsniff[\"prn\"] = functools.partial(self.reply,\n                                                    send_function=sock.send,\n                                                    address=address)\n                del optsniff[\"iface\"]\n                sniffer = AsyncSniffer(opened_socket=sock, **optsniff)\n                sniffer.start()\n                sniffers.append((sniffer, sock))\n        finally:\n            for (sniffer, sock) in sniffers:\n                try:\n                    sniffer.stop()\n                except Exception:\n                    pass\n                sock.close()\n            self.close()\n            ssock.close()\n\n    def sniff_bg(self):\n        self.sniffer = threading.Thread(target=self.sniff)\n        self.sniffer.start()\n\n    def is_request(self, req):\n        return 1\n\n    def make_reply(self, req, address=None):\n        return req\n\n    def send_reply(self, reply, send_function=None):\n        if send_function:\n            send_function(reply)",
    "Code_level": "Class-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport threading\nfrom scapy.all import conf, Packet, sniff\n\nclass AnsweringMachineTCP:\n    def __init__(self, port=80, cls=conf.raw_layer, **kargs):\n        self.port = port\n        self.cls = cls\n        self.optsniff = kargs\n        self.sniffer = None\n        self.socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.socket.bind(('', self.port))\n        self.socket.listen(5)\n        self.running = False\n\n    def close(self):\n        self.running = False\n        if self.sniffer and self.sniffer.is_alive():\n            self.sniffer.join()\n        self.socket.close()\n\n    def reply(self, pkt, send_function=None, address=None):\n        if self.is_request(pkt):\n            reply = self.make_reply(pkt, address)\n            if reply:\n                self.send_reply(reply, send_function)\n\n    def sniff(self):\n        self.running = True\n        while self.running:\n            try:\n                client_socket, address = self.socket.accept()\n                def handle_client(client_socket, address):\n                    def send_pkt(pkt):\n                        client_socket.send(bytes(pkt))\n                    sniff(opened_socket=client_socket, prn=lambda pkt: self.reply(pkt, send_pkt, address), stop_filter=lambda _: not self.running, **self.optsniff)\n                threading.Thread(target=handle_client, args=(client_socket, address)).start()\n            except:\n                if self.running:\n                    raise\n\n    def sniff_bg(self):\n        self.sniffer = threading.Thread(target=self.sniff)\n        self.sniffer.daemon = True\n        self.sniffer.start()\n\n    def is_request(self, req):\n        return True\n\n    def make_reply(self, req, address=None):\n        return req\n\n    def send_reply(self, reply, send_function=None):\n        if send_function:\n            send_function(reply)"
  },
  {
    "Task_id": 384,
    "Instruction": "Task Description: Implement a TCP server class that receives OAuth tokens through a local socket connection, handling the incoming HTTP request and extracting the authorization code or error message.\n\nClass Description: OAuthTokenReceiver is a class that creates a TCP server to listen for OAuth token responses on a specified local port. It handles the HTTP response, extracts the authorization code, and manages error cases.\n\nAttributes:\nport: int - The local port number on which the server will listen for incoming OAuth token responses (default: 31337)\n\nMethods:\nreceive_oauth_token: receive_oauth_token() -> str - Listens for an incoming connection, receives the OAuth token response, sends an HTTP success response, and returns the extracted authorization code. Raises an exception if the response contains an error description.",
    "Canonical_solution": "import socket\nimport re\nimport urllib.parse\n\nclass OAuthTokenReceiver:\n    def __init__(self, port=31337):\n        self.port = port\n\n    def receive_oauth_token(self):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        sock.bind(('127.0.0.1', self.port))\n        sock.listen(1)\n        conn, addr = sock.accept()\n        data = conn.recv(1024)\n        conn.sendall('HTTP/1.1 200 OK\\r\\nContent-Type: text/html\\r\\n\\r\\n<html><head><title>Recon-ng</title></head><body>Response received. Return to Recon-ng.</body></html>')\n        conn.close()\n        \n        if 'error_description' in data:\n            error = urllib.parse.unquote_plus(re.search(r'error_description=([^\\s&]*)', data).group(1))\n            raise Exception(error)\n        \n        authorization_code = re.search(r'code=([^\\s&]*)', data).group(1)\n        return authorization_code",
    "Code_level": "Class-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nfrom urllib.parse import parse_qs, urlparse\n\nclass OAuthTokenReceiver:\n    def __init__(self, port=31337):\n        self.port = port\n\n    def receive_oauth_token(self):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            s.bind(('localhost', self.port))\n            s.listen(1)\n            conn, addr = s.accept()\n            with conn:\n                data = conn.recv(4096).decode('utf-8')\n                if not data:\n                    raise ValueError(\"No data received\")\n                \n                request_lines = data.split('\\r\\n')\n                if not request_lines:\n                    raise ValueError(\"Invalid request format\")\n                \n                first_line = request_lines[0]\n                if not first_line.startswith('GET '):\n                    raise ValueError(\"Not a GET request\")\n                \n                path = first_line.split(' ')[1]\n                parsed = urlparse(path)\n                query = parse_qs(parsed.query)\n                \n                if 'error' in query:\n                    error = query['error'][0]\n                    description = query.get('error_description', [''])[0]\n                    raise ValueError(f\"OAuth error: {error}. Description: {description}\")\n                \n                if 'code' not in query:\n                    raise ValueError(\"No authorization code in response\")\n                \n                code = query['code'][0]\n                \n                response = (\n                    \"HTTP/1.1 200 OK\\r\\n\"\n                    \"Content-Type: text/html\\r\\n\"\n                    \"\\r\\n\"\n                    \"<html><body><h1>Authentication successful</h1></body></html>\"\n                )\n                conn.sendall(response.encode('utf-8'))\n                \n                return code"
  },
  {
    "Task_id": 387,
    "Instruction": "Task Description: Create a TCP server that collects and serves Jetson device statistics using the jtop library, with optional HTTP response formatting.\n\nFunction Description: The function establishes a TCP server that listens for incoming connections. When a client connects, it retrieves system statistics from the Jetson device using jtop, formats them as JSON, and sends them to the client. The function can optionally wrap the JSON data in a basic HTTP response.\n\nInput:\n- host (str, optional): IP address to bind the server to (default: \"127.0.0.1\")\n- port (int, optional): Port number to listen on (default: 65432)\n- http (bool, optional): Flag to enable HTTP response formatting (default: False)\n\nOutput: None (The function runs indefinitely until interrupted or an error occurs, sending data to connected clients but not returning any value)",
    "Canonical_solution": "import socket\nimport json\nfrom jtop import jtop, JtopException\n\ndef jtop_server(host=\"127.0.0.1\", port=65432, http=False):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.bind((host, port))\n    sock.listen(1)\n    \n    try:\n        with jtop() as jetson:\n            while jetson.ok():\n                conn, addr = sock.accept()\n                stats = json.dumps(jetson.stats)\n                if http:\n                    message = \"HTTP/1.1 200 OK\\r\\nHost: {host}:{port}\\r\\nContent-Type: application/json\\r\\nContent-Length: {length}\\r\\n\\r\\n{stats}\"\n                    conn.send(message.format(host=host, port=port, length=len(stats), stats=stats.encode()))\n                else:\n                    conn.send(stats.encode())\n                conn.close()\n    except JtopException as e:\n        print(e)\n    except Exception as e:\n        print(e)\n    finally:\n        sock.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport json\nfrom jtop import jtop\n\ndef serve_jetson_stats(host=\"127.0.0.1\", port=65432, http=False):\n    with jtop() as jetson:\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((host, port))\n            s.listen()\n            while True:\n                conn, addr = s.accept()\n                with conn:\n                    stats = jetson.stats\n                    data = json.dumps(stats).encode('utf-8')\n                    if http:\n                        response = (\n                            \"HTTP/1.1 200 OK\\r\\n\"\n                            \"Content-Type: application/json\\r\\n\"\n                            f\"Content-Length: {len(data)}\\r\\n\"\n                            \"\\r\\n\"\n                        ).encode('utf-8') + data\n                        conn.sendall(response)\n                    else:\n                        conn.sendall(data)"
  },
  {
    "Task_id": 389,
    "Instruction": "Task Description: Create a TCP server class that handles incoming client connections, receives data, and writes it to a file, with options for debugging and multiprocessing support.\n\nClass Description: EventGeneric is a TCP server implementation that listens for incoming connections, receives byte data from clients, and writes it to a binary file. It supports running in a separate process and provides debugging capabilities through logging.\n\nAttributes:\n- name: str - Identifier for the event instance and log file\n- isDebug: bool - Flag to enable debug logging\n- shared_key: Any - Optional shared key for inter-process communication\n- port: int - TCP port to listen on (auto-assigned if not specified)\n- addr: str - IP address to bind to (defaults to \"0.0.0.0\")\n- file: str - Path to the output binary file\n- logger: logging.Logger - Debug logger instance (only when isDebug=True)\n\nMethods:\n- __init__(addr=None, port=None, name='events', shared_key=None, isDebug=False) -> None - Initializes the TCP server with optional address, port, name, shared key, and debug flag\n- serve() -> None - Main server loop that binds to the socket, accepts connections, and handles data reception\n- spawn(addr=None, port=None, name='events', shared_key=None, isDebug=False) -> Tuple[int, multiprocessing.Process] - Static method to create and start the server in a separate process, returns the port number and process object",
    "Canonical_solution": "import socket\nimport multiprocessing\nimport os\n\nclass EventGeneric:\n    def __init__(self, addr=None, port=None, name='events', shared_key=None, isDebug=False):\n        self.name = name\n        self.isDebug = isDebug\n        self.shared_key = shared_key\n        if port is None:\n            self.port = get_free_port()\n        else:\n            self.port = port\n        if addr is not None:\n            self.addr, _ = addr\n        else:\n            self.addr = \"0.0.0.0\"\n        self.file = f\"./{self.name}.bin\"\n\n    def serve(self):\n        if self.isDebug:\n            self.logger = get_file_logger(self.name, level=\"DEBUG\")\n        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        local_addr_port = (self.addr, self.port)\n        sock.bind(local_addr_port)\n        sock.listen(1)\n\n        try:\n            conn, addr = sock.accept()\n            if self.isDebug:\n                self.logger.debug(f\"Open connection from {addr[0]}:{addr[1]}\")\n                event_file = open(self.file, \"wb\")\n            try:\n                while True:\n                    data = conn.recv(1, socket.MSG_WAITALL)\n                    if data:\n                        event_file.write(data)\n                    else:\n                        break\n                        raise KeyboardInterrupt\n            except KeyboardInterrupt:\n                pass\n            finally:\n                try:\n                    if self.isDebug:\n                        os.remove(self.file)\n                except OSError:\n                    pass\n                conn.close()\n                if self.isDebug:\n                    self.logger.debug(f\"Close connection from {addr[0]}:{addr[1]}\")\n            sock.close()\n        except KeyboardInterrupt:\n            pass\n        except BrokenPipeError:\n            pass\n        finally:\n            sock.close()\n            if self.isDebug:\n                self.logger.debug(f\"Closed listen on {self.addr}:{self.port}\")\n\n    @staticmethod\n    def spawn(addr=None, port=None, name='events', shared_key=None, isDebug=False):\n        event = EventGeneric(addr, port, name, shared_key, isDebug)\n        p = multiprocessing.Process(target=event.serve)\n        p.start()\n        return event.port, p",
    "Code_level": "Class-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport logging\nimport multiprocessing\nimport os\nfrom typing import Any, Tuple\n\nclass EventGeneric:\n    def __init__(self, addr=None, port=None, name='events', shared_key=None, isDebug=False):\n        self.name = name\n        self.isDebug = isDebug\n        self.shared_key = shared_key\n        self.port = port if port is not None else 0\n        self.addr = addr if addr is not None else \"0.0.0.0\"\n        self.file = f\"{name}.bin\"\n        self.logger = None\n        if isDebug:\n            logging.basicConfig(filename=f\"{name}.log\", level=logging.DEBUG)\n            self.logger = logging.getLogger(name)\n\n    def serve(self):\n        if self.logger:\n            self.logger.debug(f\"Starting server {self.name} on {self.addr}:{self.port}\")\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.bind((self.addr, self.port))\n            self.port = s.getsockname()[1]\n            if self.logger:\n                self.logger.debug(f\"Bound to port {self.port}\")\n            s.listen()\n            if self.logger:\n                self.logger.debug(\"Listening for connections\")\n            while True:\n                conn, addr = s.accept()\n                if self.logger:\n                    self.logger.debug(f\"Connected by {addr}\")\n                with conn:\n                    with open(self.file, 'ab') as f:\n                        while True:\n                            data = conn.recv(1024)\n                            if not data:\n                                break\n                            f.write(data)\n                            if self.logger:\n                                self.logger.debug(f\"Received {len(data)} bytes from {addr}\")\n\n    @staticmethod\n    def spawn(addr=None, port=None, name='events', shared_key=None, isDebug=False) -> Tuple[int, multiprocessing.Process]:\n        event = EventGeneric(addr=addr, port=port, name=name, shared_key=shared_key, isDebug=isDebug)\n        process = multiprocessing.Process(target=event.serve)\n        process.start()\n        while event.port == 0:\n            pass\n        return event.port, process"
  },
  {
    "Task_id": 39,
    "Instruction": "Task Description: Create a Python class that handles downloading files from a given URL and provides methods to retrieve the file name and read the file content.\n\nClass Description: The `UrlFile` class is designed to download files from a specified URL, manage temporary storage of the downloaded file, and provide methods to access the file name and content. It uses the `httpx` library for HTTP requests and handles temporary file storage with `tempfile`.\n\nAttributes:\n\n- url: str - The URL from which the file will be downloaded.\n- name: Optional[str] - An optional custom name for the downloaded file. If not provided, the name is derived from the URL.\n\nMethods:\n\n- get_filename() -> Optional[str] - Returns the filename derived from the URL or the custom name if provided.\n- read() -> Any - Downloads the file from the URL, saves it to a temporary directory, and returns a file-like object for reading the content. Raises an exception if the download fails or if the filename cannot be determined.",
    "Canonical_solution": "import httpx\nfrom pathlib import Path\nfrom typing import Any, Optional\nfrom tempfile import TemporaryDirectory\n\nclass UrlFile:\n    def __init__(self, url: str, name: Optional[str] = None):\n        self.url = url\n        self.name = name\n\n    def get_filename(self) -> Optional[str]:\n        return self.name or self.url.split(\"/\")[-1]\n\n    def read(self) -> Any:\n        logger.debug(f\"Downloading url: {self.url}\")\n        with httpx.Client() as client:\n            response = client.get(self.url)\n            response.raise_for_status()\n\n            with TemporaryDirectory() as temp_dir:\n                file_name = self.get_filename()\n                if file_name is None:\n                    raise ValueError(\"Could not determine a file name, please set `name`\")\n\n                file_path = Path(temp_dir).joinpath(file_name)\n                file_path.write_bytes(response.content)\n                logger.debug(f\"File downloaded and saved to {file_path.name}\")\n                return file_path.open(\"rb\")",
    "Code_level": "Class-level",
    "FSMID_for_test": 58,
    "wrong_result": "import httpx\nimport tempfile\nimport os\nfrom typing import Optional, Any\n\nclass UrlFile:\n    def __init__(self, url: str, name: Optional[str] = None):\n        self.url = url\n        self.name = name\n        self._temp_file = None\n\n    def get_filename(self) -> Optional[str]:\n        if self.name:\n            return self.name\n        if self.url:\n            return os.path.basename(self.url)\n        return None\n\n    def read(self) -> Any:\n        try:\n            response = httpx.get(self.url, follow_redirects=True)\n            response.raise_for_status()\n            filename = self.get_filename()\n            if not filename:\n                raise ValueError(\"Could not determine filename from URL\")\n            self._temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=filename)\n            self._temp_file.write(response.content)\n            self._temp_file.close()\n            return open(self._temp_file.name, 'rb')\n        except Exception as e:\n            if self._temp_file and os.path.exists(self._temp_file.name):\n                os.unlink(self._temp_file.name)\n            raise e"
  },
  {
    "Task_id": 393,
    "Instruction": "Task Description: Implement a TCP server in Python that listens for incoming client connections, processes received commands, and sends responses back to the client. The server should handle multiple commands and arguments, and provide appropriate acknowledgments or negative acknowledgments based on command execution.\n\nFunction Description: The function `do_tcp_server` establishes a TCP server that listens on a specified port, accepts client connections, receives and processes commands, and sends responses. It uses a handler function to process commands and arguments, and manages the connection lifecycle.\n\nInput:\n- `client`: [Object] - A client object that provides a printer method for sending data.\n- `arguments`: [Dictionary] - A dictionary containing server configuration, including the port number under the key \"--tcpport\".\n- `handler`: [Function] - A callback function that processes commands and arguments. It takes two parameters: the command string and the parsed arguments, and returns a boolean indicating success (True for <ACK>, False for <NAK>).\n\nOutput:\n- None. The function runs indefinitely, handling client connections and commands until manually stopped. Responses are sent back to the client via the TCP connection.",
    "Canonical_solution": "import socket\n\ndef do_tcp_server(client, arguments, handler):\n    def tcpprint(arg):\n        if isinstance(arg, bytes) or isinstance(arg, bytearray):\n            return connection.sendall(arg)\n        else:\n            return connection.sendall(bytes(str(arg), 'utf-8'))\n\n    client.printer = tcpprint\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    port = int(arguments[\"--tcpport\"])\n    server_address = ('localhost', port)\n    print('starting up on %s port %s' % server_address)\n    sock.bind(server_address)\n    sock.listen(1)\n    response = None\n    while True:\n        print('waiting for a connection')\n        connection, client_address = sock.accept()\n        try:\n            print('connection from', client_address)\n            while True:\n                data = connection.recv(4096).decode('utf-8')\n                if data == '':\n                    break\n                print('received %s' % data)\n                if data:\n                    print('handling request')\n                    lines = data.split(\"\\n\")\n                    for line in lines:\n                        if \":\" in line:\n                            cmd = line.split(\":\")[0]\n                            marguments = line.split(\":\")[1]\n                            try:\n                                opts = parse_args(cmd, marguments, arguments)\n                            except:\n                                response = \"Wrong arguments\\n<NAK>\\n\"\n                                opts = None\n                            if opts is not None:\n                                if handler(cmd, opts):\n                                    response = \"<ACK>\\n\"\n                                else:\n                                    response = \"<NAK>\\n\"\n                            connection.sendall(bytes(response, 'utf-8'))\n        finally:\n            connection.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport threading\n\ndef do_tcp_server(client, arguments, handler):\n    port = int(arguments.get(\"--tcpport\", 0))\n    if port <= 0:\n        return\n\n    server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    server_socket.bind(('0.0.0.0', port))\n    server_socket.listen(5)\n\n    def handle_client_connection(connection):\n        while True:\n            try:\n                data = connection.recv(1024).decode('utf-8').strip()\n                if not data:\n                    break\n                parts = data.split()\n                if not parts:\n                    continue\n                command = parts[0]\n                cmd_args = parts[1:] if len(parts) > 1 else []\n                success = handler(command, cmd_args)\n                response = \"<ACK>\" if success else \"<NAK>\"\n                connection.sendall(response.encode('utf-8'))\n            except Exception:\n                break\n        connection.close()\n\n    try:\n        while True:\n            conn, addr = server_socket.accept()\n            client_thread = threading.Thread(target=handle_client_connection, args=(conn,))\n            client_thread.daemon = True\n            client_thread.start()\n    except KeyboardInterrupt:\n        pass\n    finally:\n        server_socket.close()"
  },
  {
    "Task_id": 395,
    "Instruction": "Task Description: Create a TCP socket server implementation in Python that can handle multiple client connections asynchronously, with support for basic socket operations like accepting connections, sending/receiving data, and reading lines.\n\nClass Description: The implementation consists of two main classes: Listener (for server sockets) and Connection (for client connections), along with supporting event classes for asynchronous operations.\n\nAttributes:\n\nListener:\n- _closed: bool - Flag indicating if the socket is closed\n- host: str - The host address the server is bound to\n- port: int - The port number the server is listening on\n- sock: socket.socket - The underlying TCP socket object\n\nConnection:\n- sock: socket.socket - The connected socket object\n- addr: tuple - The client address (host, port)\n- _buf: bytes - Internal buffer for storing received data\n- _closed: bool - Flag indicating if the connection is closed\n\nMethods:\n\nListener:\n- __init__(host: str, port: int) -> None - Creates and binds a TCP socket to the specified host and port\n- accept() -> AcceptEvent - Returns an event that waits for and accepts incoming connections\n- close() -> None - Immediately closes the listening socket\n\nConnection:\n- __init__(sock: socket.socket, addr: tuple) -> None - Initializes a new connection with given socket and address\n- close() -> None - Closes the connection\n- recv(size: int) -> ReceiveEvent - Returns an event that receives up to size bytes of data\n- send(data: bytes) -> SendEvent - Returns an event that sends data (returns bytes sent)\n- sendall(data: bytes) -> SendEvent - Returns an event that sends all data\n- readline(terminator: bytes = b\"\\n\", bufsize: int = 1024) -> Generator[ReturnEvent, None, None] - Returns a generator that yields events to read a line\n\nSupporting Event Classes:\n- AcceptEvent(listener: Listener) - Event for accepting connections\n- ReceiveEvent(conn: Connection, bufsize: int) - Event for receiving data\n- SendEvent(conn: Connection, data: bytes, sendall: bool = False) - Event for sending data\n- WaitableEvent - Base class for all waitable events\n- ReturnEvent(value: Any) - Event for returning a value",
    "Canonical_solution": "import socket\nimport select\nimport time\nimport types\nimport collections\nimport errno\nimport sys\nimport traceback\n\nclass Listener:\n    \"\"\"A socket wrapper object for listening sockets.\"\"\"\n\n    def __init__(self, host, port):\n        \"\"\"Create a listening socket on the given hostname and port.\"\"\"\n        self._closed = False\n        self.host = host\n        self.port = port\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.sock.bind((host, port))\n        self.sock.listen(5)\n\n    def accept(self):\n        \"\"\"An event that waits for a connection on the listening socket.\n        When a connection is made, the event returns a Connection\n        object.\n        \"\"\"\n        if self._closed:\n            raise SocketClosedError()\n        return AcceptEvent(self)\n\n    def close(self):\n        \"\"\"Immediately close the listening socket. (Not an event.)\"\"\"\n        self._closed = True\n        self.sock.close()\n\nclass Connection:\n    \"\"\"A socket wrapper object for connected sockets.\"\"\"\n\n    def __init__(self, sock, addr):\n        self.sock = sock\n        self.addr = addr\n        self._buf = b\"\"\n        self._closed = False\n\n    def close(self):\n        \"\"\"Close the connection.\"\"\"\n        self._closed = True\n        self.sock.close()\n\n    def recv(self, size):\n        \"\"\"Read at most size bytes of data from the socket.\"\"\"\n        if self._closed:\n            raise SocketClosedError()\n\n        if self._buf:\n            out = self._buf[:size]\n            self._buf = self._buf[size:]\n            return ValueEvent(out)\n        else:\n            return ReceiveEvent(self, size)\n\n    def send(self, data):\n        \"\"\"Sends data on the socket, returning the number of bytes\n        successfully sent.\n        \"\"\"\n        if self._closed:\n            raise SocketClosedError()\n        return SendEvent(self, data)\n\n    def sendall(self, data):\n        \"\"\"Send all of data on the socket.\"\"\"\n        if self._closed:\n            raise SocketClosedError()\n        return SendEvent(self, data, True)\n\n    def readline(self, terminator=b\"\\n\", bufsize=1024):\n        \"\"\"Reads a line (delimited by terminator) from the socket.\"\"\"\n        if self._closed:\n            raise SocketClosedError()\n\n        while True:\n            if terminator in self._buf:\n                line, self._buf = self._buf.split(terminator, 1)\n                line += terminator\n                yield ReturnEvent(line)\n                break\n            data = yield ReceiveEvent(self, bufsize)\n            if data:\n                self._buf += data\n            else:\n                line = self._buf\n                self._buf = b\"\"\n                yield ReturnEvent(line)\n                break\n\nclass AcceptEvent(WaitableEvent):\n    \"\"\"An event for Listener objects (listening sockets) that suspends\n    execution until the socket gets a connection.\n    \"\"\"\n\n    def __init__(self, listener):\n        self.listener = listener\n\n    def waitables(self):\n        return (self.listener.sock,), (), ()\n\n    def fire(self):\n        sock, addr = self.listener.sock.accept()\n        return Connection(sock, addr)\n\nclass ReceiveEvent(WaitableEvent):\n    \"\"\"An event for Connection objects (connected sockets) for\n    asynchronously reading data.\n    \"\"\"\n\n    def __init__(self, conn, bufsize):\n        self.conn = conn\n        self.bufsize = bufsize\n\n    def waitables(self):\n        return (self.conn.sock,), (), ()\n\n    def fire(self):\n        return self.conn.sock.recv(self.bufsize)\n\nclass SendEvent(WaitableEvent):\n    \"\"\"An event for Connection objects (connected sockets) for\n    asynchronously writing data.\n    \"\"\"\n\n    def __init__(self, conn, data, sendall=False):\n        self.conn = conn\n        self.data = data\n        self.sendall = sendall\n\n    def waitables(self):\n        return (), (self.conn.sock,), ()\n\n    def fire(self):\n        if self.sendall:\n            return self.conn.sock.sendall(self.data)\n        else:\n            return self.conn.sock.send(self.data)",
    "Code_level": "Class-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport select\nfrom typing import Optional, Generator, Any, Tuple\n\nclass WaitableEvent:\n    def __init__(self):\n        self._done = False\n        self._result = None\n        self._exception = None\n\n    def set_result(self, result: Any) -> None:\n        self._result = result\n        self._done = True\n\n    def set_exception(self, exc: Exception) -> None:\n        self._exception = exc\n        self._done = True\n\n    def done(self) -> bool:\n        return self._done\n\n    def result(self) -> Any:\n        if self._exception is not None:\n            raise self._exception\n        return self._result\n\nclass AcceptEvent(WaitableEvent):\n    def __init__(self, listener: 'Listener'):\n        super().__init__()\n        self.listener = listener\n\nclass ReceiveEvent(WaitableEvent):\n    def __init__(self, conn: 'Connection', bufsize: int):\n        super().__init__()\n        self.conn = conn\n        self.bufsize = bufsize\n\nclass SendEvent(WaitableEvent):\n    def __init__(self, conn: 'Connection', data: bytes, sendall: bool = False):\n        super().__init__()\n        self.conn = conn\n        self.data = data\n        self.sendall = sendall\n\nclass ReturnEvent(WaitableEvent):\n    def __init__(self, value: Any):\n        super().__init__()\n        self.set_result(value)\n\nclass Listener:\n    def __init__(self, host: str, port: int):\n        self._closed = False\n        self.host = host\n        self.port = port\n        self.sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n        self.sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        self.sock.bind((host, port))\n        self.sock.setblocking(False)\n        self.sock.listen()\n\n    def accept(self) -> AcceptEvent:\n        return AcceptEvent(self)\n\n    def close(self) -> None:\n        if not self._closed:\n            self._closed = True\n            self.sock.close()\n\nclass Connection:\n    def __init__(self, sock: socket.socket, addr: Tuple[str, int]):\n        self.sock = sock\n        self.addr = addr\n        self._buf = b''\n        self._closed = False\n        self.sock.setblocking(False)\n\n    def close(self) -> None:\n        if not self._closed:\n            self._closed = True\n            self.sock.close()\n\n    def recv(self, size: int) -> ReceiveEvent:\n        return ReceiveEvent(self, size)\n\n    def send(self, data: bytes) -> SendEvent:\n        return SendEvent(self, data)\n\n    def sendall(self, data: bytes) -> SendEvent:\n        return SendEvent(self, data, sendall=True)\n\n    def readline(self, terminator: bytes = b\"\\n\", bufsize: int = 1024) -> Generator[ReturnEvent, None, None]:\n        while True:\n            if terminator in self._buf:\n                line, sep, self._buf = self._buf.partition(terminator)\n                yield ReturnEvent(line + sep)\n                break\n            event = self.recv(bufsize)\n            yield event\n            data = event.result()\n            if not data:\n                if self._buf:\n                    yield ReturnEvent(self._buf)\n                    self._buf = b''\n                break\n            self._buf += data"
  },
  {
    "Task_id": 397,
    "Instruction": "Task Description: Create a Python function that implements a TCP server which listens for client connections and sends the current server time upon connection.\n\nFunction Description: The function sets up a TCP server socket using IPv4, binds it to a specified IP address and port, listens for incoming connections, and sends the current datetime to each connected client before closing the connection. The server runs indefinitely until manually stopped.\n\nInput: None\n\nOutput: None (The function runs indefinitely, printing connection messages to console and sending datetime to clients)",
    "Canonical_solution": "from socket import socket, SOCK_STREAM, AF_INET\nfrom datetime import datetime\n\ndef main():\n    server = socket(family=AF_INET, type=SOCK_STREAM)\n    server.bind(('192.168.1.2', 6789))\n    server.listen(512)\n    print('\u670d\u52a1\u5668\u542f\u52a8\u5f00\u59cb\u76d1\u542c...')\n    while True:\n        client, addr = server.accept()\n        print(str(addr) + '\u8fde\u63a5\u5230\u4e86\u670d\u52a1\u5668.')\n        client.send(str(datetime.now()).encode('utf-8'))\n        client.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 66,
    "wrong_result": "import socket\nimport datetime\n\ndef run_time_server():\n    host = '0.0.0.0'\n    port = 12345\n    \n    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n        s.bind((host, port))\n        s.listen()\n        print(f\"Server listening on {host}:{port}\")\n        \n        while True:\n            conn, addr = s.accept()\n            with conn:\n                print(f\"Connected by {addr}\")\n                current_time = datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n                conn.sendall(current_time.encode())"
  },
  {
    "Task_id": 408,
    "Instruction": "Task Description: Implement a parameter server that handles client requests for model parameters, replay submissions, and draining operations using a threaded TCP server.\n\nClass Description: ParameterServer is a TCP-based server that manages model parameters and replay buffers for distributed training. It handles concurrent client connections and provides thread-safe operations for parameter exchange and replay buffer management.\n\nAttributes:\n\nstate: [State] - Dataclass containing server state including buffer directories, capacity, locks, and parameter versions\nbuffer_dir: [str] - Directory path for storing incoming replay buffers\ndrain_dir: [str] - Directory path for draining replay buffers to trainers\ncapacity: [int] - Maximum capacity of the replay buffer\nforce_sequential: [bool] - Flag to enforce sequential parameter updates\ndir_lock: [Lock] - Thread lock for directory operations\nparam_lock: [Lock] - Thread lock for parameter operations\nbuffer_size: [int] - Current size of the replay buffer\nsubmission_id: [int] - Counter for replay submissions\nmortal_param: [Optional[OrderedDict]] - Current mortal model parameters\ndqn_param: [Optional[OrderedDict]] - Current DQN model parameters\nparam_version: [int] - Version counter for parameter updates\nidle_param_version: [int] - Version marker for idle trainers\n\nMethods:\n\n__init__: [Name](buffer_dir: str, drain_dir: str, capacity: int, force_sequential: bool) -> [None] - Initializes the parameter server with buffer directories, capacity, and sequential flag\nstart_server: [Name](host: str, port: int) -> [None] - Starts the threaded TCP server on specified host and port\nhandle_get_param: [Name](msg: dict) -> [None] - Handles parameter retrieval requests from clients\nhandle_submit_replay: [Name](msg: dict) -> [None] - Handles replay buffer submissions from clients\nhandle_submit_param: [Name](msg: dict) -> [None] - Handles parameter updates from trainers\nhandle_drain: [Name]() -> [None] - Handles draining operations for transferring replay buffers to trainers",
    "Canonical_solution": "from socketserver import ThreadingTCPServer, BaseRequestHandler\nfrom threading import Lock\nfrom typing import Optional\nfrom collections import OrderedDict\nfrom dataclasses import dataclass\nimport os\nimport shutil\nimport logging\nimport torch\nfrom io import BytesIO\nfrom common import send_msg, recv_msg\n\n@dataclass\nclass State:\n    buffer_dir: str\n    drain_dir: str\n    capacity: int\n    force_sequential: bool\n    dir_lock: Lock\n    param_lock: Lock\n    buffer_size: int\n    submission_id: int\n    mortal_param: Optional[OrderedDict]\n    dqn_param: Optional[OrderedDict]\n    param_version: int\n    idle_param_version: int\n\nclass ParameterServer:\n    def __init__(self, buffer_dir: str, drain_dir: str, capacity: int, force_sequential: bool):\n        self.state = State(\n            buffer_dir=os.path.abspath(buffer_dir),\n            drain_dir=os.path.abspath(drain_dir),\n            capacity=capacity,\n            force_sequential=force_sequential,\n            dir_lock=Lock(),\n            param_lock=Lock(),\n            buffer_size=0,\n            submission_id=0,\n            mortal_param=None,\n            dqn_param=None,\n            param_version=0,\n            idle_param_version=0,\n        )\n\n    def start_server(self, host: str, port: int):\n        if os.path.isdir(self.state.buffer_dir):\n            shutil.rmtree(self.state.buffer_dir)\n        if os.path.isdir(self.state.drain_dir):\n            shutil.rmtree(self.state.drain_dir)\n        os.makedirs(self.state.buffer_dir)\n        os.makedirs(self.state.drain_dir)\n\n        class Handler(BaseRequestHandler):\n            def handle(self):\n                msg = recv_msg(self.request)\n                match msg['type']:\n                    case 'get_param':\n                        self.handle_get_param(msg)\n                    case 'submit_replay':\n                        self.handle_submit_replay(msg)\n                    case 'submit_param':\n                        self.handle_submit_param(msg)\n                    case 'drain':\n                        self.handle_drain()\n\n            def handle_get_param(self, msg):\n                with self.state.dir_lock:\n                    overflow = self.state.buffer_size >= self.state.capacity\n                    with self.state.param_lock:\n                        has_param = self.state.mortal_param is not None and self.state.dqn_param is not None\n                if overflow:\n                    send_msg(self.request, {'status': 'samples overflow'})\n                    return\n                if not has_param:\n                    send_msg(self.request, {'status': 'empty param'})\n                    return\n\n                client_param_version = msg['param_version']\n                buf = BytesIO()\n                with self.state.param_lock:\n                    if self.state.force_sequential and self.state.idle_param_version <= client_param_version:\n                        res = {'status': 'trainer is busy'}\n                    else:\n                        res = {\n                            'status': 'ok',\n                            'mortal': self.state.mortal_param,\n                            'dqn': self.state.dqn_param,\n                            'param_version': self.state.param_version,\n                        }\n                    torch.save(res, buf)\n                send_msg(self.request, buf.getbuffer(), packed=True)\n\n            def handle_submit_replay(self, msg):\n                with self.state.dir_lock:\n                    for filename, content in msg['logs'].items():\n                        filepath = os.path.join(self.state.buffer_dir, f'{self.state.submission_id}_{filename}')\n                        with open(filepath, 'wb') as f:\n                            f.write(content)\n                    self.state.buffer_size += len(msg['logs'])\n                    self.state.submission_id += 1\n                    logging.info(f'total buffer size: {self.state.buffer_size}')\n\n            def handle_submit_param(self, msg):\n                with self.state.param_lock:\n                    self.state.mortal_param = msg['mortal']\n                    self.state.dqn_param = msg['dqn']\n                    self.state.param_version += 1\n                    if msg['is_idle']:\n                        self.state.idle_param_version = self.state.param_version\n\n            def handle_drain(self):\n                drained_size = 0\n                with self.state.dir_lock:\n                    buffer_list = os.listdir(self.state.buffer_dir)\n                    raw_count = len(buffer_list)\n                    assert raw_count == self.state.buffer_size\n                    if (not self.state.force_sequential or raw_count >= self.state.capacity) and raw_count > 0:\n                        old_drain_list = os.listdir(self.state.drain_dir)\n                        for filename in old_drain_list:\n                            filepath = os.path.join(self.state.drain_dir, filename)\n                            os.remove(filepath)\n                        for filename in buffer_list:\n                            src = os.path.join(self.state.buffer_dir, filename)\n                            dst = os.path.join(self.state.drain_dir, filename)\n                            shutil.move(src, dst)\n                        drained_size = raw_count\n                        self.state.buffer_size = 0\n                        logging.info(f'files transferred to trainer: {drained_size}')\n                        logging.info(f'total buffer size: {self.state.buffer_size}')\n                send_msg(self.request, {\n                    'count': drained_size,\n                    'drain_dir': self.state.drain_dir,\n                })\n\n        server = ThreadingTCPServer((host, port), Handler)\n        server.allow_reuse_address = True\n        server.daemon_threads = True\n        logging.info(f'listening on {host}:{port}')\n        server.serve_forever()",
    "Code_level": "Class-level",
    "FSMID_for_test": 70,
    "wrong_result": "import socket\nimport threading\nimport json\nimport os\nimport shutil\nfrom dataclasses import dataclass\nfrom typing import Optional, OrderedDict\nfrom collections import OrderedDict as odict\n\n@dataclass\nclass State:\n    buffer_dir: str\n    drain_dir: str\n    capacity: int\n    force_sequential: bool\n    dir_lock: threading.Lock\n    param_lock: threading.Lock\n    buffer_size: int\n    submission_id: int\n    mortal_param: Optional[OrderedDict]\n    dqn_param: Optional[OrderedDict]\n    param_version: int\n    idle_param_version: int\n\nclass ParameterServer:\n    def __init__(self, buffer_dir: str, drain_dir: str, capacity: int, force_sequential: bool = False):\n        os.makedirs(buffer_dir, exist_ok=True)\n        os.makedirs(drain_dir, exist_ok=True)\n        self.state = State(\n            buffer_dir=buffer_dir,\n            drain_dir=drain_dir,\n            capacity=capacity,\n            force_sequential=force_sequential,\n            dir_lock=threading.Lock(),\n            param_lock=threading.Lock(),\n            buffer_size=0,\n            submission_id=0,\n            mortal_param=None,\n            dqn_param=None,\n            param_version=0,\n            idle_param_version=-1\n        )\n\n    def start_server(self, host: str, port: int):\n        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:\n            s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n            s.bind((host, port))\n            s.listen()\n            print(f\"Server started on {host}:{port}\")\n            while True:\n                conn, addr = s.accept()\n                threading.Thread(target=self._handle_client, args=(conn, addr)).start()\n\n    def _handle_client(self, conn: socket.socket, addr: tuple):\n        try:\n            with conn:\n                data = conn.recv(4096)\n                if not data:\n                    return\n                msg = json.loads(data.decode())\n                if msg['type'] == 'get_param':\n                    self.handle_get_param(msg)\n                elif msg['type'] == 'submit_replay':\n                    self.handle_submit_replay(msg)\n                elif msg['type'] == 'submit_param':\n                    self.handle_submit_param(msg)\n                elif msg['type'] == 'drain':\n                    self.handle_drain()\n        except Exception as e:\n            print(f\"Error handling client {addr}: {e}\")\n\n    def handle_get_param(self, msg: dict):\n        with self.state.param_lock:\n            response = {\n                'mortal_param': self.state.mortal_param,\n                'dqn_param': self.state.dqn_param,\n                'param_version': self.state.param_version,\n                'idle_param_version': self.state.idle_param_version\n            }\n            if msg.get('trainer_idle', False):\n                self.state.idle_param_version = self.state.param_version\n        return json.dumps(response)\n\n    def handle_submit_replay(self, msg: dict):\n        with self.state.dir_lock:\n            if self.state.buffer_size >= self.state.capacity:\n                return json.dumps({'status': 'buffer_full'})\n            filename = os.path.join(self.state.buffer_dir, f\"replay_{self.state.submission_id}.bin\")\n            with open(filename, 'wb') as f:\n                f.write(msg['data'])\n            self.state.submission_id += 1\n            self.state.buffer_size += 1\n        return json.dumps({'status': 'success'})\n\n    def handle_submit_param(self, msg: dict):\n        with self.state.param_lock:\n            if self.state.force_sequential and msg['param_version'] != self.state.param_version:\n                return json.dumps({'status': 'version_mismatch'})\n            if msg.get('mortal_param'):\n                self.state.mortal_param = odict(msg['mortal_param'])\n            if msg.get('dqn_param'):\n                self.state.dqn_param = odict(msg['dqn_param'])\n            self.state.param_version += 1\n        return json.dumps({'status': 'success', 'param_version': self.state.param_version})\n\n    def handle_drain(self):\n        with self.state.dir_lock:\n            if self.state.buffer_size == 0:\n                return json.dumps({'status': 'buffer_empty'})\n            files = sorted(os.listdir(self.state.buffer_dir))\n            for file in files[:self.state.buffer_size]:\n                src = os.path.join(self.state.buffer_dir, file)\n                dst = os.path.join(self.state.drain_dir, file)\n                shutil.move(src, dst)\n            self.state.buffer_size = 0\n        return json.dumps({'status': 'success', 'drained_count': len(files)})"
  },
  {
    "Task_id": 419,
    "Instruction": "Task Description: Implement a TCP server using Twisted framework in Python that supports WebSocket connections, handles incoming data, and manages connection lifecycle events.\n\nClass Description: The solution consists of two classes: `HelloServerProtocol` (handles protocol-level operations) and `HelloServerFactory` (manages server configuration and startup).\n\nAttributes:\n\n- `HelloServerProtocol`:\n  - transport: [object] - Twisted transport object representing the connection\n- `HelloServerFactory`:\n  - host: [str] - Server host address (default: \"127.0.0.1\")\n  - port: [int] - Server port number (default: 9000)\n  - wrappedFactory: [object] - Twisted Factory instance for protocol handling\n  - factory: [object] - WebSocket-wrapped factory instance\n\nMethods:\n\n- `HelloServerProtocol`:\n  - connectionMade() -> None - Called when a new connection is established\n  - dataReceived(data: bytes) -> None - Called when data is received from client\n- `HelloServerFactory`:\n  - __init__(host: str = \"127.0.0.1\", port: int = 9000, enableCompression: bool = False, autoFragmentSize: int = 1024) -> None - Initializes factory with configuration\n  - start() -> None - Starts the TCP server and enters event loop",
    "Canonical_solution": "from twisted.internet.protocol import Protocol, Factory\nfrom twisted.internet import reactor\nfrom autobahn.twisted.websocket import WrappingWebSocketServerFactory\n\nclass HelloServerProtocol(Protocol):\n    def connectionMade(self):\n        print(\"connectionMade\", self.transport.getHost(), self.transport.getPeer())\n        self.transport.write('how are you?' * 100)\n\n    def dataReceived(self, data):\n        print(\"dataReceived: {}\".format(data))\n\nclass HelloServerFactory:\n    def __init__(self, host=\"127.0.0.1\", port=9000, enableCompression=False, autoFragmentSize=1024):\n        self.host = host\n        self.port = port\n        self.wrappedFactory = Factory.forProtocol(HelloServerProtocol)\n        self.factory = WrappingWebSocketServerFactory(\n            self.wrappedFactory,\n            f\"ws://{self.host}:{self.port}\",\n            enableCompression=enableCompression,\n            autoFragmentSize=autoFragmentSize\n        )\n\n    def start(self):\n        reactor.listenTCP(self.port, self.factory)\n        reactor.run()",
    "Code_level": "Class-level",
    "FSMID_for_test": 68,
    "wrong_result": "from twisted.internet import reactor\nfrom twisted.internet.protocol import Factory\nfrom twisted.protocols.basic import LineReceiver\nfrom autobahn.twisted.websocket import WebSocketServerFactory, WebSocketServerProtocol\nfrom autobahn.twisted.resource import WebSocketResource\n\nclass HelloServerProtocol(WebSocketServerProtocol):\n    def onConnect(self, request):\n        print(f\"Client connecting: {request.peer}\")\n\n    def onOpen(self):\n        print(\"WebSocket connection open\")\n\n    def onMessage(self, payload, isBinary):\n        if isBinary:\n            print(f\"Received binary message: {len(payload)} bytes\")\n        else:\n            print(f\"Received text message: {payload.decode('utf8')}\")\n        self.sendMessage(payload, isBinary)\n\n    def onClose(self, wasClean, code, reason):\n        print(f\"WebSocket connection closed: {reason}\")\n\nclass HelloServerFactory:\n    def __init__(self, host=\"127.0.0.1\", port=9000, enableCompression=False, autoFragmentSize=1024):\n        self.host = host\n        self.port = port\n        self.wrappedFactory = WebSocketServerFactory(f\"ws://{host}:{port}\")\n        self.wrappedFactory.protocol = HelloServerProtocol\n        self.wrappedFactory.setProtocolOptions(autoFragmentSize=autoFragmentSize, compress=enableCompression)\n        self.factory = WebSocketResource(self.wrappedFactory)\n\n    def start(self):\n        reactor.listenTCP(self.port, self.factory)\n        reactor.run()"
  },
  {
    "Task_id": 424,
    "Instruction": "Task Description: Implement a UDP communication class that handles socket creation, data transmission, and reception with proper connection management and error handling.\n\nClass Description: The UDPCommunication class provides functionality for UDP-based network communication, including opening/closing connections, sending/receiving data, and managing connection status. It uses threading for asynchronous data reception and includes signal-based notification for connection status changes.\n\nAttributes:\n- conn: [socket.socket] - The UDP socket object used for communication\n- status: [ConnectionStatus] - Current connection status (CONNECTED/CLOSED)\n- config: [dict] - Configuration dictionary containing protocol, port, and target information\n- onConnectionStatus: [pyqtSignal] - Signal emitted when connection status changes\n- hintSignal: [pyqtSignal] - Signal for displaying error messages or hints\n- receiveProcess: [threading.Thread] - Thread handling incoming data reception\n\nMethods:\n- __init__() -> None - Initializes the UDP communication object with default configuration\n- openCloseSerial() -> None - Toggles connection state (opens if closed, closes if open)\n- receiveDataProcess(conn: socket.socket) -> None - Thread function for receiving data (private method)\n- send(data: bytes) -> None - Sends data to configured target\n- checkTarget(target: str) -> tuple - Validates and parses target address into (host, port) tuple\n- isConnected() -> bool - Returns current connection status\n- getConnStatus() -> ConnectionStatus - Returns current connection status object",
    "Canonical_solution": "from PyQt5.QtCore import pyqtSignal, QObject\nimport socket\nimport threading\nimport time\nimport re\n\nclass UDPCommunication:\n    def __init__(self):\n        self.conn = None\n        self.status = ConnectionStatus.CLOSED\n        self.config = {\n            \"protocol\": \"udp\",\n            \"port\": 2345,\n            \"target\": [\"127.0.0.1:2345\", [\"127.0.0.1:2345\"]]\n        }\n        self.onConnectionStatus = pyqtSignal(ConnectionStatus, str)\n        self.hintSignal = pyqtSignal(str, str, str)\n\n    def openCloseSerial(self):\n        if self.status == ConnectionStatus.CLOSED:\n            try:\n                self.conn = socket.socket(type=socket.SOCK_DGRAM)\n                self.conn.bind((\"0.0.0.0\", self.config[\"port\"]))\n                self.status = ConnectionStatus.CONNECTED\n                self.receiveProcess = threading.Thread(target=self.receiveDataProcess, args=(self.conn,))\n                self.receiveProcess.setDaemon(True)\n                self.receiveProcess.start()\n                self.onConnectionStatus.emit(self.status, \"\")\n            except Exception as e:\n                msg = _(\"Connect Failed\") + \"\\n\" + str(e)\n                self.hintSignal.emit(\"error\", _(\"Error\"), msg)\n                self.status = ConnectionStatus.CLOSED\n                self.onConnectionStatus.emit(self.status, msg)\n        else:\n            self.status = ConnectionStatus.CLOSED\n            if self.conn:\n                self.conn.close()\n                self.conn = None\n            self.onConnectionStatus.emit(self.status, \"\")\n\n    def receiveDataProcess(self, conn):\n        buffer = b''\n        t = 0\n        conn.settimeout(0.1)\n        while self.status != ConnectionStatus.CLOSED:\n            try:\n                data, target = conn.recvfrom(4096)\n                if data:\n                    if len(data) > 4096:\n                        flush = False\n                    t = time.time()\n                    buffer += data\n                    if buffer and (time.time() - t > 0.001):\n                        try:\n                            if buffer:\n                                self.onReceived(buffer)\n                        except Exception as e:\n                            print(\"-- error in onReceived callback:\", e)\n                        buffer = b''\n            except socket.timeout:\n                continue\n            except Exception as e:\n                print(\"-- recv error:\", e)\n                self.status = ConnectionStatus.CLOSED\n                self.onConnectionStatus.emit(self.status, _(\"Connection closed!\") + \" \" + str(e))\n                try:\n                    conn.close()\n                except Exception:\n                    pass\n                break\n\n    def send(self, data: bytes):\n        if self.conn and self.status == ConnectionStatus.CONNECTED:\n            target = self.checkTarget(self.config[\"target\"][0])\n            if not target:\n                self.hintSignal.emit(\"error\", _(\"Target error\"), _(\"Target error\") + \": \" + self.config[\"target\"])\n            self.conn.sendto(data, target)\n\n    def checkTarget(self, target):\n        if not target:\n            return None\n        host = target\n        port = 80\n        target = target.replace(\"\uff1a\", \":\")\n        if target.endswith(\":\"):\n            target = target[:-1]\n        _host = re.match('http(.*)://(.*)', target)\n        if _host:\n            s, target = _host.groups()\n            host = target\n        _host = re.match('(.*):(\\d*)', target)\n        if _host:\n            host, port = _host.groups()\n            port = int(port)\n        if host.endswith(\"/\"):\n            host = host[:-1]\n        return (host, port)\n\n    def isConnected(self):\n        return self.status == ConnectionStatus.CONNECTED\n\n    def getConnStatus(self):\n        return self.status",
    "Code_level": "Class-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\nimport threading\nfrom enum import Enum\nfrom PyQt5.QtCore import pyqtSignal, QObject\n\nclass ConnectionStatus(Enum):\n    CONNECTED = 1\n    CLOSED = 2\n\nclass UDPCommunication(QObject):\n    onConnectionStatus = pyqtSignal(ConnectionStatus)\n    hintSignal = pyqtSignal(str)\n\n    def __init__(self):\n        super().__init__()\n        self.conn = None\n        self.status = ConnectionStatus.CLOSED\n        self.config = {\n            'protocol': socket.SOCK_DGRAM,\n            'port': 0,\n            'target': ('0.0.0.0', 0)\n        }\n        self.receiveProcess = None\n\n    def openCloseSerial(self):\n        if self.status == ConnectionStatus.CLOSED:\n            try:\n                self.conn = socket.socket(socket.AF_INET, self.config['protocol'])\n                self.conn.settimeout(1)\n                self.conn.bind(('0.0.0.0', self.config['port']))\n                self.status = ConnectionStatus.CONNECTED\n                self.onConnectionStatus.emit(self.status)\n                self.receiveProcess = threading.Thread(target=self.receiveDataProcess, args=(self.conn,), daemon=True)\n                self.receiveProcess.start()\n            except Exception as e:\n                self.hintSignal.emit(f\"Connection error: {str(e)}\")\n                if self.conn:\n                    self.conn.close()\n                    self.conn = None\n        else:\n            if self.conn:\n                self.conn.close()\n                self.conn = None\n            self.status = ConnectionStatus.CLOSED\n            self.onConnectionStatus.emit(self.status)\n\n    def receiveDataProcess(self, conn):\n        while self.status == ConnectionStatus.CONNECTED:\n            try:\n                data, addr = conn.recvfrom(4096)\n                if data:\n                    self.hintSignal.emit(f\"Received data from {addr}: {data.decode('utf-8', errors='ignore')}\")\n            except socket.timeout:\n                continue\n            except Exception as e:\n                if self.status == ConnectionStatus.CONNECTED:\n                    self.hintSignal.emit(f\"Receive error: {str(e)}\")\n\n    def send(self, data):\n        if self.status != ConnectionStatus.CONNECTED:\n            self.hintSignal.emit(\"Not connected\")\n            return\n        try:\n            self.conn.sendto(data, self.config['target'])\n        except Exception as e:\n            self.hintSignal.emit(f\"Send error: {str(e)}\")\n\n    def checkTarget(self, target):\n        try:\n            host, port = target.split(':')\n            port = int(port)\n            return (host, port)\n        except Exception as e:\n            self.hintSignal.emit(f\"Invalid target format: {str(e)}\")\n            return None\n\n    def isConnected(self):\n        return self.status == ConnectionStatus.CONNECTED\n\n    def getConnStatus(self):\n        return self.status"
  },
  {
    "Task_id": 429,
    "Instruction": "Task Description: Implement a UDP socket handler class that facilitates UDP communication with support for IP transparent proxying and original destination address retrieval.\n\nClass Description: The UDPSocketHandler class provides methods for receiving and sending UDP packets while maintaining information about both source and destination addresses. It supports both IPv4 and IPv6 protocols and includes special handling for transparent proxying scenarios.\n\nAttributes:\n[No explicit attributes are defined in the class, as it primarily operates through its methods]\n\nMethods:\n\nMethod1: recv_udp(listener: socket.socket, bufsize: int) -> tuple - Receives UDP packets and extracts both source and destination address information. Uses recvmsg() to get ancillary data containing the original destination address.\n  Input:\n    - listener: A bound UDP socket object\n    - bufsize: Maximum buffer size for received data\n  Output:\n    - tuple: (srcip: tuple, dstip: tuple, data: bytes) containing source address, destination address, and received data\n\nMethod2: send_udp(sock: socket.socket, srcip: tuple, dstip: tuple, data: bytes) -> None - Sends UDP packets while maintaining source address information through IP transparent option.\n  Input:\n    - sock: The socket to use for sending\n    - srcip: Source address tuple (IP, port)\n    - dstip: Destination address tuple (IP, port)\n    - data: The data to send\n  Output: None\n\nMethod3: setup_udp_listener(udp_listener: object) -> None - Configures UDP listener sockets for transparent proxying and original destination address retrieval.\n  Input:\n    - udp_listener: An object containing v4 and v6 socket instances\n  Output: None\n\nMethod4: setsockopt_error(e: Exception) -> None - Handles permission errors related to socket options and raises a fatal error.\n  Input:\n    - e: The caught PermissionError exception\n  Output: None (raises Fatal exception)",
    "Canonical_solution": "import socket\nimport os\nimport struct\nfrom sshuttle.helpers import debug1, debug3, Fatal\n\nIP_TRANSPARENT = 19\nIP_ORIGDSTADDR = 20\nIP_RECVORIGDSTADDR = IP_ORIGDSTADDR\nSOL_IPV6 = 41\nIPV6_ORIGDSTADDR = 74\nIPV6_RECVORIGDSTADDR = IPV6_ORIGDSTADDR\n\nclass UDPSocketHandler:\n    def __init__(self):\n        pass\n\n    def recv_udp(self, listener, bufsize):\n        debug3('Accept UDP python using recvmsg.')\n        data, ancdata, _, srcip = listener.recvmsg(\n            4096, socket.CMSG_SPACE(24))\n        dstip = None\n        family = None\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level == socket.SOL_IP and cmsg_type == IP_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET:\n                    start = 4\n                    length = 4\n                else:\n                    raise Fatal(\"Unsupported socket type '%s'\" % family)\n                ip = socket.inet_ntop(family, cmsg_data[start:start + length])\n                dstip = (ip, port)\n                break\n            elif cmsg_level == SOL_IPV6 and cmsg_type == IPV6_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET6:\n                    start = 8\n                    length = 16\n                else:\n                    raise Fatal(\"Unsupported socket type '%s'\" % family)\n                ip = socket.inet_ntop(family, cmsg_data[start:start + length])\n                dstip = (ip, port)\n                break\n        return (srcip, dstip, data)\n\n    def send_udp(self, sock, srcip, dstip, data):\n        if not srcip:\n            debug1(\n                \"-- ignored UDP to %r: \"\n                \"couldn't determine source IP address\\n\" % (dstip,))\n            return\n        sender = socket.socket(sock.family, socket.SOCK_DGRAM)\n        sender.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        try:\n            sender.setsockopt(socket.SOL_IP, IP_TRANSPARENT, 1)\n        except PermissionError as e:\n            self.setsockopt_error(e)\n        sender.bind(srcip)\n        sender.sendto(data, dstip)\n        sender.close()\n\n    def setup_udp_listener(self, udp_listener):\n        try:\n            udp_listener.setsockopt(socket.SOL_IP, IP_TRANSPARENT, 1)\n        except PermissionError as e:\n            self.setsockopt_error(e)\n\n        if udp_listener.v4 is not None:\n            udp_listener.v4.setsockopt(\n                socket.SOL_IP, IP_RECVORIGDSTADDR, 1)\n        if udp_listener.v6 is not None:\n            udp_listener.v6.setsockopt(SOL_IPV6, IPV6_RECVORIGDSTADDR, 1)\n\n    def setsockopt_error(self, e):\n        raise Fatal(\"Insufficient permissions for tproxy method.\\n\"\n                   \"Your effective UID is %d, not 0. Try rerunning as root.\\n\"\n                   % os.geteuid())",
    "Code_level": "Class-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\nimport struct\nimport errno\n\nclass UDPSocketHandler:\n    @staticmethod\n    def recv_udp(listener, bufsize):\n        try:\n            data, ancdata, flags, srcip = listener.recvmsg(bufsize, socket.CMSG_SPACE(24))\n            dstip = None\n            for cmsg_level, cmsg_type, cmsg_data in ancdata:\n                if cmsg_level == socket.SOL_IP and cmsg_type == socket.IP_RECVORIGDSTADDR:\n                    family = struct.unpack('=H', cmsg_data[:2])[0]\n                    if family == socket.AF_INET:\n                        port, addr = struct.unpack('!H4s', cmsg_data[2:8])\n                        dstip = (socket.inet_ntop(socket.AF_INET, addr), port)\n                elif cmsg_level == socket.SOL_IPV6 and cmsg_type == socket.IPV6_RECVORIGDSTADDR:\n                    family = struct.unpack('=H', cmsg_data[:2])[0]\n                    if family == socket.AF_INET6:\n                        port, flowinfo, addr, scope_id = struct.unpack('!H4s16sI', cmsg_data[2:28])\n                        dstip = (socket.inet_ntop(socket.AF_INET6, addr), port)\n            return (srcip, dstip, data)\n        except socket.error as e:\n            if e.errno == errno.EAGAIN or e.errno == errno.EWOULDBLOCK:\n                return None\n            raise\n\n    @staticmethod\n    def send_udp(sock, srcip, dstip, data):\n        dst_addr = dstip[0]\n        dst_port = dstip[1]\n        src_addr = srcip[0]\n        src_port = srcip[1]\n        sock.setsockopt(socket.SOL_IP, socket.IP_TRANSPARENT, 1)\n        sock.bind((src_addr, src_port))\n        sock.sendto(data, (dst_addr, dst_port))\n\n    @staticmethod\n    def setup_udp_listener(udp_listener):\n        try:\n            udp_listener.v4.setsockopt(socket.SOL_IP, socket.IP_TRANSPARENT, 1)\n            udp_listener.v4.setsockopt(socket.SOL_IP, socket.IP_RECVORIGDSTADDR, 1)\n        except PermissionError as e:\n            UDPSocketHandler.setsockopt_error(e)\n        try:\n            udp_listener.v6.setsockopt(socket.SOL_IPV6, socket.IPV6_TRANSPARENT, 1)\n            udp_listener.v6.setsockopt(socket.SOL_IPV6, socket.IPV6_RECVORIGDSTADDR, 1)\n        except PermissionError as e:\n            UDPSocketHandler.setsockopt_error(e)\n\n    @staticmethod\n    def setsockopt_error(e):\n        raise RuntimeError(f\"Failed to set socket option: {e}. Requires CAP_NET_ADMIN capability or root privileges.\")"
  },
  {
    "Task_id": 430,
    "Instruction": "Task Description: Implement a UDP echo server that receives data from clients and sends the same data back to the client.\n\nFunction Description: The function `udp_echo_server` creates a UDP socket, binds it to a specified host and port, and continuously listens for incoming data. When data is received, it echoes the same data back to the sender. The function runs indefinitely until manually stopped.\n\nInput:  \n- `host` (str, optional): The local IP address to bind the socket to. Defaults to 'localhost'.  \n- `port` (int, optional): The port number to bind the socket to. Defaults to 6789.  \n\nOutput: None (The function runs indefinitely and does not return any value.)",
    "Canonical_solution": "from socket import *\n\ndef udp_echo_server(host='localhost', port=6789):\n    server = socket(AF_INET, SOCK_DGRAM)\n    server.bind((host, port))\n    while True:\n        data, addr = server.recvfrom(1024)\n        server.sendto(data, addr)\n    server.close()",
    "Code_level": "Function-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\n\ndef udp_echo_server(host='localhost', port=6789):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n    sock.bind((host, port))\n    print(f\"UDP echo server listening on {host}:{port}\")\n    while True:\n        data, addr = sock.recvfrom(4096)\n        print(f\"Received data from {addr}: {data.decode()}\")\n        sock.sendto(data, addr)"
  },
  {
    "Task_id": 431,
    "Instruction": "Task Description: Implement a class for discovering devices on a network using SSDP (Simple Service Discovery Protocol) over UDP multicast. The class should be able to send discovery requests and handle responses from devices.\n\nClass Description: The SSDPDiscover class is responsible for discovering UPnP/DLNA devices on a local network using SSDP. It sends M-SEARCH requests to a multicast address and listens for responses. The class supports both single-socket and multi-socket modes for sending discovery requests.\n\nAttributes:\n- SSDP_ADDRESS: [str] - The multicast address for SSDP (239.255.255.250)\n- SSDP_PORT: [int] - The port number for SSDP (1900)\n- SSDP_MX: [int] - The maximum wait time for responses (in seconds)\n- SSDP_TTL: [int] - The time-to-live for multicast packets\n- SSDP_AMOUNT: [int] - The number of discovery requests to send\n- MSEARCH_PORT: [int] - The local port to bind to for sending discovery requests (0 for random)\n- MSEARCH_MSG: [str] - The M-SEARCH request message template\n- BUFFER_SIZE: [int] - The size of the buffer for receiving responses\n- USE_SINGLE_SOCKET: [bool] - Whether to use a single socket for all requests or one per interface\n- cb_on_device_response: [callable] - Callback function to handle device responses\n- host: [str] - The host address to bind to (None for all interfaces)\n- addresses: [list] - List of local IPv4 addresses\n\nMethods:\n- __init__(cb_on_device_response, host=None) -> [None] - Initializes the SSDPDiscover instance with a callback for device responses and an optional host address.\n- refresh_addresses() -> [None] - Refreshes the list of local IPv4 addresses.\n- search(ssdp_ttl=None, ssdp_mx=None, ssdp_amount=None) -> [None] - Initiates the device discovery process with optional parameters for TTL, MX, and request amount.\n- _search(host, ssdp_ttl, ssdp_mx, ssdp_amount) -> [None] - Internal method to perform the discovery process for a specific host.\n- _send_discover(sock, ssdp_mx) -> [None] - Internal method to send an M-SEARCH request using the provided socket.",
    "Canonical_solution": "import socket\nimport threading\nimport logging\nimport chardet\nimport traceback\nimport pulseaudio_dlna.utils.network\nimport pulseaudio_dlna.plugins.dlna.ssdp\n\nlogger = logging.getLogger('pulseaudio_dlna.discover')\n\nclass SSDPDiscover:\n    SSDP_ADDRESS = '239.255.255.250'\n    SSDP_PORT = 1900\n    SSDP_MX = 3\n    SSDP_TTL = 10\n    SSDP_AMOUNT = 5\n    MSEARCH_PORT = 0\n    MSEARCH_MSG = '\\r\\n'.join([\n        'M-SEARCH * HTTP/1.1',\n        'HOST: {host}:{port}',\n        'MAN: \"ssdp:discover\"',\n        'MX: {mx}',\n        'ST: ssdp:all',\n    ]) + '\\r\\n' * 2\n    BUFFER_SIZE = 1024\n    USE_SINGLE_SOCKET = True\n\n    def __init__(self, cb_on_device_response, host=None):\n        self.cb_on_device_response = cb_on_device_response\n        self.host = host\n        self.addresses = []\n        self.refresh_addresses()\n\n    def refresh_addresses(self):\n        self.addresses = pulseaudio_dlna.utils.network.ipv4_addresses()\n\n    def search(self, ssdp_ttl=None, ssdp_mx=None, ssdp_amount=None):\n        ssdp_mx = ssdp_mx or self.SSDP_MX\n        ssdp_ttl = ssdp_ttl or self.SSDP_TTL\n        ssdp_amount = ssdp_amount or self.SSDP_AMOUNT\n\n        if self.USE_SINGLE_SOCKET:\n            self._search(self.host or '', ssdp_ttl, ssdp_mx, ssdp_amount)\n        else:\n            if self.host:\n                self._search(self.host, ssdp_ttl, ssdp_mx, ssdp_amount)\n            else:\n                threads = []\n                for addr in self.addresses:\n                    thread = threading.Thread(\n                        target=self._search,\n                        args=[addr, ssdp_ttl, ssdp_mx, ssdp_amount])\n                    threads.append(thread)\n                try:\n                    for thread in threads:\n                        thread.start()\n                    for thread in threads:\n                        thread.join()\n                except:\n                    traceback.print_exc()\n\n    def _search(self, host, ssdp_ttl, ssdp_mx, ssdp_amount):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        sock.settimeout(ssdp_mx)\n        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ssdp_ttl)\n        sock.bind((host, self.MSEARCH_PORT))\n\n        for i in range(1, ssdp_amount + 1):\n            t = threading.Timer(\n                float(i) / 2, self._send_discover, args=[sock, ssdp_mx])\n            t.start()\n\n        while True:\n            try:\n                header, address = sock.recvfrom(self.BUFFER_SIZE)\n                if self.cb_on_device_response:\n                    guess = chardet.detect(header)\n                    header = header.decode(guess['encoding'])\n                    header = pulseaudio_dlna.plugins.dlna.ssdp._get_header_map(header)\n                    self.cb_on_device_response(header, address)\n            except socket.timeout:\n                break\n        sock.close()\n\n    def _send_discover(self, sock, ssdp_mx):\n        msg = self.MSEARCH_MSG.format(\n            host=self.SSDP_ADDRESS, port=self.SSDP_PORT, mx=ssdp_mx)\n        if self.USE_SINGLE_SOCKET:\n            for addr in self.addresses:\n                sock.setsockopt(\n                    socket.SOL_IP, socket.IP_MULTICAST_IF,\n                    socket.inet_aton(addr))\n                sock.sendto(msg, (self.SSDP_ADDRESS, self.SSDP_PORT))\n        else:\n            sock.sendto(msg, (self.SSDP_ADDRESS, self.SSDP_PORT))",
    "Code_level": "Class-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\nimport struct\nimport time\nfrom ipaddress import ip_address\nfrom threading import Thread\n\nclass SSDPDiscover:\n    SSDP_ADDRESS = \"239.255.255.250\"\n    SSDP_PORT = 1900\n    SSDP_MX = 1\n    SSDP_TTL = 2\n    SSDP_AMOUNT = 3\n    MSEARCH_PORT = 0\n    MSEARCH_MSG = \"\"\"M-SEARCH * HTTP/1.1\\r\nHOST: {0}:{1}\\r\nMAN: \"ssdp:discover\"\\r\nMX: {2}\\r\nST: ssdp:all\\r\nUSER-AGENT: UPnP/1.0\\r\n\\r\\n\"\"\"\n    BUFFER_SIZE = 4096\n    USE_SINGLE_SOCKET = True\n\n    def __init__(self, cb_on_device_response, host=None):\n        self.cb_on_device_response = cb_on_device_response\n        self.host = host\n        self.addresses = []\n        self.refresh_addresses()\n\n    def refresh_addresses(self):\n        self.addresses = []\n        if self.host:\n            self.addresses.append(self.host)\n        else:\n            for addr in socket.getaddrinfo(socket.gethostname(), None):\n                if addr[0] == socket.AF_INET:\n                    ip = addr[4][0]\n                    if ip_address(ip).is_private:\n                        self.addresses.append(ip)\n            if not self.addresses:\n                self.addresses.append('0.0.0.0')\n\n    def search(self, ssdp_ttl=None, ssdp_mx=None, ssdp_amount=None):\n        ttl = ssdp_ttl if ssdp_ttl is not None else self.SSDP_TTL\n        mx = ssdp_mx if ssdp_mx is not None else self.SSDP_MX\n        amount = ssdp_amount if ssdp_amount is not None else self.SSDP_AMOUNT\n\n        if self.USE_SINGLE_SOCKET:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n            sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ttl)\n            sock.bind(('0.0.0.0', self.MSEARCH_PORT))\n            for _ in range(amount):\n                self._send_discover(sock, mx)\n                time.sleep(0.1)\n            sock.close()\n        else:\n            threads = []\n            for addr in self.addresses:\n                t = Thread(target=self._search, args=(addr, ttl, mx, amount))\n                t.start()\n                threads.append(t)\n            for t in threads:\n                t.join()\n\n    def _search(self, host, ssdp_ttl, ssdp_mx, ssdp_amount):\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM, socket.IPPROTO_UDP)\n        sock.setsockopt(socket.IPPROTO_IP, socket.IP_MULTICAST_TTL, ssdp_ttl)\n        sock.bind((host, self.MSEARCH_PORT))\n        for _ in range(ssdp_amount):\n            self._send_discover(sock, ssdp_mx)\n            time.sleep(0.1)\n        sock.close()\n\n    def _send_discover(self, sock, ssdp_mx):\n        msg = self.MSEARCH_MSG.format(self.SSDP_ADDRESS, self.SSDP_PORT, ssdp_mx)\n        sock.sendto(msg.encode(), (self.SSDP_ADDRESS, self.SSDP_PORT))\n        sock.settimeout(ssdp_mx)\n        try:\n            while True:\n                data, addr = sock.recvfrom(self.BUFFER_SIZE)\n                if self.cb_on_device_response:\n                    self.cb_on_device_response(data, addr)\n        except socket.timeout:\n            pass"
  },
  {
    "Task_id": 432,
    "Instruction": "Task Description: Implement a UDP socket handler class that provides advanced UDP communication capabilities, including receiving original destination addresses and supporting both IPv4 and IPv6 transparent proxying.\n\nClass Description: UDPSocketHandler is a utility class designed to handle UDP socket operations with special features like retrieving original destination addresses and supporting transparent proxying. It provides methods for receiving UDP packets with ancillary data, sending UDP packets with specific source addresses, and setting up UDP listeners with appropriate socket options.\n\nAttributes:\n- recvmsg: Optional[str] - Stores the detected implementation method for recvmsg ('python', 'socket_ext', or None)\n\nMethods:\n- _detect_recvmsg_implementation() -> Optional[str] - Detects available recvmsg implementation methods\n- recv_udp(listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes] - Receives UDP data with source and destination address information\n- _recv_udp_python(listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes] - Python native implementation of recv_udp\n- _recv_udp_socket_ext(listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes] - socket_ext implementation of recv_udp\n- _recv_udp_fallback(listener: socket.socket, bufsize: int) -> Tuple[Any, None, bytes] - Fallback implementation without ancillary data support\n- send_udp(sock: socket.socket, srcip: Optional[Tuple[str, int]], dstip: Tuple[str, int], data: bytes) -> None - Sends UDP data with specified source address\n- setup_udp_listener(udp_listener: socket.socket) -> None - Configures UDP listener socket with transparent proxying options",
    "Canonical_solution": "import struct\nimport socket\nfrom typing import Optional, Tuple, Any\n\nIP_TRANSPARENT = 19\nIP_ORIGDSTADDR = 20\nIP_RECVORIGDSTADDR = IP_ORIGDSTADDR\nSOL_IPV6 = 41\nIPV6_ORIGDSTADDR = 74\nIPV6_RECVORIGDSTADDR = IPV6_ORIGDSTADDR\n\nclass UDPSocketHandler:\n    def __init__(self):\n        self.recvmsg = self._detect_recvmsg_implementation()\n        \n    def _detect_recvmsg_implementation(self) -> Optional[str]:\n        try:\n            import socket as pythonsocket\n            getattr(pythonsocket.socket, \"recvmsg\")\n            return \"python\"\n        except AttributeError:\n            try:\n                import socket_ext\n                getattr(socket_ext.socket, \"recvmsg\")\n                return \"socket_ext\"\n            except ImportError:\n                return None\n\n    def recv_udp(self, listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes]:\n        if self.recvmsg == \"python\":\n            return self._recv_udp_python(listener, bufsize)\n        elif self.recvmsg == \"socket_ext\":\n            return self._recv_udp_socket_ext(listener, bufsize)\n        else:\n            return self._recv_udp_fallback(listener, bufsize)\n\n    def _recv_udp_python(self, listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes]:\n        data, ancdata, msg_flags, srcip = listener.recvmsg(4096, socket.CMSG_SPACE(24))\n        dstip = None\n        for cmsg_level, cmsg_type, cmsg_data in ancdata:\n            if cmsg_level == socket.SOL_IP and cmsg_type == IP_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET:\n                    start = 4\n                    length = 4\n                    ip = socket.inet_ntop(family, cmsg_data[start:start + length])\n                    dstip = (ip, port)\n                    break\n            elif cmsg_level == SOL_IPV6 and cmsg_type == IPV6_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET6:\n                    start = 8\n                    length = 16\n                    ip = socket.inet_ntop(family, cmsg_data[start:start + length])\n                    dstip = (ip, port)\n                    break\n        return (srcip, dstip, data)\n\n    def _recv_udp_socket_ext(self, listener: socket.socket, bufsize: int) -> Tuple[Any, Optional[Tuple[str, int]], bytes]:\n        srcip, data, adata, flags = listener.recvmsg((bufsize,), socket.CMSG_SPACE(24))\n        dstip = None\n        for a in adata:\n            if a.cmsg_level == socket.SOL_IP and a.cmsg_type == IP_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', a.cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET:\n                    start = 4\n                    length = 4\n                    ip = socket.inet_ntop(family, a.cmsg_data[start:start + length])\n                    dstip = (ip, port)\n                    break\n            elif a.cmsg_level == SOL_IPV6 and a.cmsg_type == IPV6_ORIGDSTADDR:\n                family, port = struct.unpack('=HH', a.cmsg_data[0:4])\n                port = socket.htons(port)\n                if family == socket.AF_INET6:\n                    start = 8\n                    length = 16\n                    ip = socket.inet_ntop(family, a.cmsg_data[start:start + length])\n                    dstip = (ip, port)\n                    break\n        return (srcip, dstip, data[0])\n\n    def _recv_udp_fallback(self, listener: socket.socket, bufsize: int) -> Tuple[Any, None, bytes]:\n        data, srcip = listener.recvfrom(bufsize)\n        return (srcip, None, data)\n\n    def send_udp(self, sock: socket.socket, srcip: Optional[Tuple[str, int]], dstip: Tuple[str, int], data: bytes) -> None:\n        if not srcip:\n            return\n        sender = socket.socket(sock.family, socket.SOCK_DGRAM)\n        sender.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        sender.setsockopt(socket.SOL_IP, IP_TRANSPARENT, 1)\n        sender.bind(srcip)\n        sender.sendto(data, dstip)\n        sender.close()\n\n    def setup_udp_listener(self, udp_listener: socket.socket) -> None:\n        udp_listener.setsockopt(socket.SOL_IP, IP_TRANSPARENT, 1)\n        if hasattr(udp_listener, 'v4') and udp_listener.v4 is not None:\n            udp_listener.v4.setsockopt(socket.SOL_IP, IP_RECVORIGDSTADDR, 1)\n        if hasattr(udp_listener, 'v6') and udp_listener.v6 is not None:\n            udp_listener.v6.setsockopt(SOL_IPV6, IPV6_RECVORIGDSTADDR, 1)",
    "Code_level": "Class-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\nimport sys\nimport struct\nimport errno\n\nclass UDPSocketHandler:\n    def __init__(self):\n        self.recvmsg = self._detect_recvmsg_implementation()\n\n    def _detect_recvmsg_implementation(self):\n        if hasattr(socket.socket, 'recvmsg'):\n            return 'python'\n        try:\n            import socket_ext\n            if hasattr(socket_ext, 'recvmsg'):\n                return 'socket_ext'\n        except ImportError:\n            pass\n        return None\n\n    def recv_udp(self, listener, bufsize):\n        if self.recvmsg == 'python':\n            return self._recv_udp_python(listener, bufsize)\n        elif self.recvmsg == 'socket_ext':\n            return self._recv_udp_socket_ext(listener, bufsize)\n        else:\n            return self._recv_udp_fallback(listener, bufsize)\n\n    def _recv_udp_python(self, listener, bufsize):\n        try:\n            data, ancdata, flags, srcaddr = listener.recvmsg(bufsize, socket.CMSG_SPACE(24))\n            dstaddr = None\n            for cmsg_level, cmsg_type, cmsg_data in ancdata:\n                if cmsg_level == socket.SOL_IP and cmsg_type == socket.IP_RECVORIGDSTADDR:\n                    port, addr = struct.unpack('!H4s', cmsg_data[2:8])\n                    dstaddr = (socket.inet_ntoa(addr), port)\n                elif cmsg_level == socket.SOL_IPV6 and cmsg_type == socket.IPV6_RECVORIGDSTADDR:\n                    port, flowinfo, addr, scope_id = struct.unpack('!HI16sI', cmsg_data)\n                    dstaddr = (socket.inet_ntop(socket.AF_INET6, addr), port)\n            return srcaddr, dstaddr, data\n        except (OSError, struct.error):\n            return listener.getsockname(), None, listener.recv(bufsize)\n\n    def _recv_udp_socket_ext(self, listener, bufsize):\n        try:\n            import socket_ext\n            data, srcaddr, dstaddr = socket_ext.recvmsg(listener, bufsize)\n            return srcaddr, dstaddr, data\n        except (ImportError, OSError):\n            return listener.getsockname(), None, listener.recv(bufsize)\n\n    def _recv_udp_fallback(self, listener, bufsize):\n        return listener.getsockname(), None, listener.recv(bufsize)\n\n    def send_udp(self, sock, srcip, dstip, data):\n        if srcip:\n            sock.bind(srcip)\n        sock.sendto(data, dstip)\n\n    def setup_udp_listener(self, udp_listener):\n        try:\n            udp_listener.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n        except (OSError, AttributeError):\n            pass\n\n        if hasattr(socket, 'SO_REUSEPORT'):\n            try:\n                udp_listener.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEPORT, 1)\n            except OSError:\n                pass\n\n        if hasattr(socket, 'IP_TRANSPARENT'):\n            try:\n                udp_listener.setsockopt(socket.SOL_IP, socket.IP_TRANSPARENT, 1)\n            except OSError:\n                pass\n\n        if hasattr(socket, 'IPV6_TRANSPARENT'):\n            try:\n                udp_listener.setsockopt(socket.SOL_IPV6, socket.IPV6_TRANSPARENT, 1)\n            except OSError:\n                pass\n\n        if hasattr(socket, 'IP_RECVORIGDSTADDR'):\n            try:\n                udp_listener.setsockopt(socket.SOL_IP, socket.IP_RECVORIGDSTADDR, 1)\n            except OSError:\n                pass\n\n        if hasattr(socket, 'IPV6_RECVORIGDSTADDR'):\n            try:\n                udp_listener.setsockopt(socket.SOL_IPV6, socket.IPV6_RECVORIGDSTADDR, 1)\n            except OSError:\n                pass"
  },
  {
    "Task_id": 433,
    "Instruction": "Task Description: Implement a UDP relay class that facilitates communication between local and remote servers using UDP sockets, handling DNS resolution and data forwarding.\n\nClass Description: The UDPRelay class manages UDP socket communication, including socket creation, binding, data transmission, and DNS resolution. It acts as a relay between local and remote endpoints, handling both incoming and outgoing UDP packets.\n\nAttributes:\n- _config: [dict] - Configuration parameters including addresses and ports\n- _dns_resolver: [object] - DNS resolver for handling domain name resolution\n- _is_local: [bool] - Flag indicating if the instance is local or remote\n- _sockets: [set] - Set of active client sockets\n- _server_socket: [socket.socket] - Main server socket for listening\n- _closed: [bool] - Flag indicating if the relay is closed\n- _listen_addr: [str] - Address to listen on\n- _listen_port: [int] - Port to listen on\n\nMethods:\n- __init__(config: dict, dns_resolver: object, is_local: bool) -> None - Initializes the UDP relay with configuration, DNS resolver, and local/remote mode\n- _handle_server() -> None - Handles incoming data from the server socket\n- _handle_server_dns_resolved(error: str, remote_addr: tuple, server_addr: str, params: tuple) -> None - Handles DNS resolution results and forwards data\n- _handle_client(sock: socket.socket) -> None - Handles incoming data from client sockets\n- close() -> None - Closes all sockets and cleans up resources",
    "Canonical_solution": "import socket\nimport errno\nimport logging\nfrom typing import Tuple, Optional\n\nclass UDPRelay:\n    def __init__(self, config, dns_resolver, is_local):\n        self._config = config\n        self._dns_resolver = dns_resolver\n        self._is_local = is_local\n        self._sockets = set()\n        self._server_socket = None\n        self._closed = False\n\n        if is_local:\n            self._listen_addr = config['local_address']\n            self._listen_port = config['local_port']\n        else:\n            self._listen_addr = config['server']\n            self._listen_port = config['server_port']\n\n        addrs = socket.getaddrinfo(self._listen_addr, self._listen_port, 0,\n                                 socket.SOCK_DGRAM, socket.SOL_UDP)\n        af, socktype, proto, canonname, sa = addrs[0]\n        self._server_socket = socket.socket(af, socktype, proto)\n        self._server_socket.bind((self._listen_addr, self._listen_port))\n        self._server_socket.setblocking(False)\n\n    def _handle_server(self):\n        data, r_addr = self._server_socket.recvfrom(65536)\n        if not data:\n            logging.debug('UDP handle_server: data is empty')\n            return\n\n        try:\n            header_result = parse_header(data)\n            if header_result is None:\n                return\n            connecttype, addrtype, dest_addr, dest_port, header_length = header_result\n\n            if self._is_local:\n                server_addr, server_port = self._get_a_server()\n            else:\n                server_addr, server_port = dest_addr, dest_port\n\n            if (addrtype & 7) == 3:\n                af = common.is_ip(server_addr)\n                if not af:\n                    handler = common.UDPAsyncDNSHandler((data, r_addr, None, header_length))\n                    handler.resolve(self._dns_resolver, (server_addr, server_port), \n                                   self._handle_server_dns_resolved)\n                else:\n                    self._handle_server_dns_resolved(\"\", (server_addr, server_port), \n                                                   server_addr, (data, r_addr, None, header_length))\n            else:\n                self._handle_server_dns_resolved(\"\", (server_addr, server_port), \n                                               server_addr, (data, r_addr, None, header_length))\n        except Exception as e:\n            logging.error(f\"Error handling server data: {e}\")\n\n    def _handle_server_dns_resolved(self, error, remote_addr, server_addr, params):\n        if error:\n            return\n        data, r_addr, uid, header_length = params\n        server_port = remote_addr[1]\n\n        addrs = socket.getaddrinfo(server_addr, server_port, 0,\n                                 socket.SOCK_DGRAM, socket.SOL_UDP)\n        af, socktype, proto, canonname, sa = addrs[0]\n        client = socket.socket(af, socktype, proto)\n        client.setblocking(False)\n\n        try:\n            client.sendto(data[header_length:], (server_addr, server_port))\n            self._sockets.add(client.fileno())\n        except IOError as e:\n            err = eventloop.errno_from_exception(e)\n            if err not in (errno.EINPROGRESS, errno.EAGAIN):\n                logging.error(f\"Error sending UDP data: {e}\")\n\n    def _handle_client(self, sock):\n        data, r_addr = sock.recvfrom(65536)\n        if not data:\n            return\n\n        if not self._is_local:\n            data = pack_addr(r_addr[0]) + struct.pack('>H', r_addr[1]) + data\n\n        try:\n            self._server_socket.sendto(data, r_addr)\n        except (OSError, IOError) as e:\n            error_no = eventloop.errno_from_exception(e)\n            if error_no not in (errno.EWOULDBLOCK,):\n                logging.error(f\"Error sending UDP response: {e}\")\n\n    def close(self):\n        self._closed = True\n        if self._server_socket:\n            self._server_socket.close()\n        for sock in self._sockets:\n            try:\n                sock.close()\n            except:\n                pass",
    "Code_level": "Class-level",
    "FSMID_for_test": 71,
    "wrong_result": "import socket\nimport errno\nimport struct\nfrom collections import namedtuple\n\nclass UDPRelay:\n    def __init__(self, config, dns_resolver, is_local):\n        self._config = config\n        self._dns_resolver = dns_resolver\n        self._is_local = is_local\n        self._sockets = set()\n        self._server_socket = None\n        self._closed = False\n        self._listen_addr = config['listen_addr']\n        self._listen_port = config['listen_port']\n\n    def _handle_server(self):\n        try:\n            data, addr = self._server_socket.recvfrom(65535)\n            if not data:\n                return\n            if self._is_local:\n                if len(data) < 3:\n                    return\n                atyp = data[0]\n                if atyp == 1:\n                    remote_addr = socket.inet_ntoa(data[1:5])\n                    remote_port = struct.unpack('>H', data[5:7])[0]\n                    payload = data[7:]\n                elif atyp == 3:\n                    domain_length = data[1]\n                    remote_addr = data[2:2+domain_length].decode('ascii')\n                    remote_port = struct.unpack('>H', data[2+domain_length:4+domain_length])[0]\n                    payload = data[4+domain_length:]\n                else:\n                    return\n                self._dns_resolver.resolve(remote_addr, lambda *args: self._handle_server_dns_resolved(*args, (addr, payload)))\n            else:\n                sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n                sock.setblocking(False)\n                self._sockets.add(sock)\n                sock.sendto(data, addr)\n        except socket.error as e:\n            if e.errno not in (errno.EAGAIN, errno.EWOULDBLOCK):\n                self.close()\n\n    def _handle_server_dns_resolved(self, error, remote_addr, server_addr, params):\n        if error:\n            return\n        client_addr, payload = params\n        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)\n        sock.setblocking(False)\n        self._sockets.add(sock)\n        sock.sendto(payload, remote_addr)\n\n    def _handle_client(self, sock):\n        try:\n            data, addr = sock.recvfrom(65535)\n            if not data:\n                return\n            if self._is_local:\n                self._server_socket.sendto(data, addr)\n            else:\n                header = b'\\x01\\x00\\x00' + socket.inet_aton(addr[0]) + struct.pack('>H', addr[1])\n                self._server_socket.sendto(header + data, (self._listen_addr, self._listen_port))\n        except socket.error as e:\n            if e.errno not in (errno.EAGAIN, errno.EWOULDBLOCK):\n                self._sockets.discard(sock)\n                sock.close()\n\n    def close(self):\n        if self._closed:\n            return\n        self._closed = True\n        if self._server_socket:\n            self._server_socket.close()\n        for sock in self._sockets:\n            sock.close()\n        self._sockets.clear()"
  },
  {
    "Task_id": 434,
    "Instruction": "Task Description: Implement a WebSocket client connection using Tornado in Python, which handles WebSocket protocol communication including connection establishment, message exchange, and connection closure.\n\nClass Description: WebSocketClientConnection is a Tornado-based WebSocket client implementation that manages the lifecycle of a WebSocket connection, including handling messages, pings, and connection events.\n\nAttributes:\n- connect_future: Future - A future representing the connection attempt\n- read_queue: Queue - A queue for incoming messages\n- key: bytes - The WebSocket handshake key\n- _on_message_callback: Optional[Callable] - Callback for received messages\n- close_code: Optional[int] - WebSocket close code\n- close_reason: Optional[str] - WebSocket close reason\n- params: _WebSocketParams - WebSocket connection parameters\n- tcp_client: TCPClient - Underlying TCP client\n- protocol: Optional[WebSocketProtocol] - WebSocket protocol handler\n\nMethods:\n- __init__(request: HTTPRequest, on_message_callback: Optional[Callable], compression_options: Optional[Dict], ping_interval: Optional[float], ping_timeout: Optional[float], max_message_size: int, subprotocols: Optional[List[str]], resolver: Optional[Resolver]) -> None - Initializes the WebSocket client connection\n- close(code: Optional[int], reason: Optional[str]) -> None - Closes the WebSocket connection\n- on_connection_close() -> None - Handles connection closure\n- on_ws_connection_close(close_code: Optional[int], close_reason: Optional[str]) -> None - Handles WebSocket-specific connection closure\n- _on_http_response(response: HTTPResponse) -> None - Handles HTTP response during connection\n- headers_received(start_line: Union[RequestStartLine, ResponseStartLine], headers: HTTPHeaders) -> Awaitable[None] - Processes received headers\n- write_message(message: Union[str, bytes, Dict[str, Any]], binary: bool) -> Future[None] - Sends a WebSocket message\n- read_message(callback: Optional[Callable]) -> Awaitable[Union[None, str, bytes]] - Reads a message from the queue\n- _on_message(message: Union[None, str, bytes]) -> Optional[Awaitable[None]] - Internal message handler\n- ping(data: bytes) -> None - Sends a ping frame\n- get_websocket_protocol() -> WebSocketProtocol - Creates the WebSocket protocol handler",
    "Canonical_solution": "import abc\nimport asyncio\nimport base64\nimport hashlib\nimport os\nimport struct\nimport tornado\nfrom urllib.parse import urlparse\nimport zlib\nfrom tornado.concurrent import Future\nfrom tornado.escape import utf8, native_str, to_unicode\nfrom tornado import gen, httpclient, httputil\nfrom tornado.ioloop import IOLoop, PeriodicCallback\nfrom tornado.iostream import StreamClosedError, IOStream\nfrom tornado.log import app_log\nfrom tornado.netutil import Resolver\nfrom tornado import simple_httpclient\nfrom tornado.queues import Queue\nfrom tornado.tcpclient import TCPClient\nfrom tornado.util import _websocket_mask\nfrom typing import Optional, Dict, Union, List, Awaitable, Callable, Tuple, Type, Any, cast\n\n_default_max_message_size = 10 * 1024 * 1024\n\nclass WebSocketClientConnection(simple_httpclient._HTTPConnection):\n    def __init__(\n        self,\n        request: httpclient.HTTPRequest,\n        on_message_callback: Optional[Callable[[Union[None, str, bytes]], None]] = None,\n        compression_options: Optional[Dict[str, Any]] = None,\n        ping_interval: Optional[float] = None,\n        ping_timeout: Optional[float] = None,\n        max_message_size: int = _default_max_message_size,\n        subprotocols: Optional[List[str]] = None,\n        resolver: Optional[Resolver] = None,\n    ) -> None:\n        self.connect_future = Future()\n        self.read_queue = Queue(1)\n        self.key = base64.b64encode(os.urandom(16))\n        self._on_message_callback = on_message_callback\n        self.close_code = None\n        self.close_reason = None\n        self.params = _WebSocketParams(\n            ping_interval=ping_interval,\n            ping_timeout=ping_timeout,\n            max_message_size=max_message_size,\n            compression_options=compression_options,\n        )\n\n        scheme, sep, rest = request.url.partition(\":\")\n        scheme = {\"ws\": \"http\", \"wss\": \"https\"}[scheme]\n        request.url = scheme + sep + rest\n        request.headers.update(\n            {\n                \"Upgrade\": \"websocket\",\n                \"Connection\": \"Upgrade\",\n                \"Sec-WebSocket-Key\": to_unicode(self.key),\n                \"Sec-WebSocket-Version\": \"13\",\n            }\n        )\n        if subprotocols is not None:\n            request.headers[\"Sec-WebSocket-Protocol\"] = \",\".join(subprotocols)\n        if compression_options is not None:\n            request.headers[\"Sec-WebSocket-Extensions\"] = (\n                \"permessage-deflate; client_max_window_bits\"\n            )\n\n        request.follow_redirects = False\n\n        self.tcp_client = TCPClient(resolver=resolver)\n        super().__init__(\n            None,\n            request,\n            lambda: None,\n            self._on_http_response,\n            104857600,\n            self.tcp_client,\n            65536,\n            104857600,\n        )\n\n    def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n        if self.protocol is not None:\n            self.protocol.close(code, reason)\n            self.protocol = None\n\n    def on_connection_close(self) -> None:\n        if not self.connect_future.done():\n            self.connect_future.set_exception(StreamClosedError())\n        self._on_message(None)\n        self.tcp_client.close()\n        super().on_connection_close()\n\n    def on_ws_connection_close(\n        self, close_code: Optional[int] = None, close_reason: Optional[str] = None\n    ) -> None:\n        self.close_code = close_code\n        self.close_reason = close_reason\n        self.on_connection_close()\n\n    def _on_http_response(self, response: httpclient.HTTPResponse) -> None:\n        if not self.connect_future.done():\n            if response.error:\n                self.connect_future.set_exception(response.error)\n            else:\n                self.connect_future.set_exception(\n                    WebSocketError(\"Non-websocket response\")\n                )\n\n    async def headers_received(\n        self,\n        start_line: Union[httputil.RequestStartLine, httputil.ResponseStartLine],\n        headers: httputil.HTTPHeaders,\n    ) -> None:\n        assert isinstance(start_line, httputil.ResponseStartLine)\n        if start_line.code != 101:\n            await super().headers_received(start_line, headers)\n            return\n\n        if self._timeout is not None:\n            self.io_loop.remove_timeout(self._timeout)\n            self._timeout = None\n\n        self.headers = headers\n        self.protocol = self.get_websocket_protocol()\n        self.protocol._process_server_headers(self.key, self.headers)\n        self.protocol.stream = self.connection.detach()\n\n        IOLoop.current().add_callback(self.protocol._receive_frame_loop)\n        self.protocol.start_pinging()\n\n        self.final_callback = None\n        future_set_result_unless_cancelled(self.connect_future, self)\n\n    def write_message(\n        self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False\n    ) -> \"Future[None]\":\n        if self.protocol is None:\n            raise WebSocketClosedError(\"Client connection has been closed\")\n        return self.protocol.write_message(message, binary=binary)\n\n    def read_message(\n        self,\n        callback: Optional[Callable[[\"Future[Union[None, str, bytes]]\"], None]] = None,\n    ) -> Awaitable[Union[None, str, bytes]]:\n        awaitable = self.read_queue.get()\n        if callback is not None:\n            self.io_loop.add_future(asyncio.ensure_future(awaitable), callback)\n        return awaitable\n\n    def _on_message(\n        self, message: Union[None, str, bytes]\n    ) -> Optional[Awaitable[None]]:\n        if self._on_message_callback:\n            self._on_message_callback(message)\n            return None\n        else:\n            return self.read_queue.put(message)\n\n    def ping(self, data: bytes = b\"\") -> None:\n        data = utf8(data)\n        if self.protocol is None:\n            raise WebSocketClosedError()\n        self.protocol.write_ping(data)\n\n    def get_websocket_protocol(self) -> \"WebSocketProtocol\":\n        return WebSocketProtocol13(self, mask_outgoing=True, params=self.params)\n\n    @property\n    def selected_subprotocol(self) -> Optional[str]:\n        return self.protocol.selected_subprotocol\n\nclass _WebSocketParams:\n    def __init__(\n        self,\n        ping_interval: Optional[float] = None,\n        ping_timeout: Optional[float] = None,\n        max_message_size: int = _default_max_message_size,\n        compression_options: Optional[Dict[str, Any]] = None,\n    ) -> None:\n        self.ping_interval = ping_interval\n        self.ping_timeout = ping_timeout\n        self.max_message_size = max_message_size\n        self.compression_options = compression_options\n\nclass WebSocketProtocol(abc.ABC):\n    def __init__(self, handler: Any) -> None:\n        self.handler = handler\n        self.stream = None\n        self.client_terminated = False\n        self.server_terminated = False\n\n    def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n        raise NotImplementedError()\n\n    def is_closing(self) -> bool:\n        raise NotImplementedError()\n\n    async def accept_connection(self, handler: Any) -> None:\n        raise NotImplementedError()\n\n    def write_message(\n        self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False\n    ) -> \"Future[None]\":\n        raise NotImplementedError()\n\n    @property\n    def selected_subprotocol(self) -> Optional[str]:\n        raise NotImplementedError()\n\n    def write_ping(self, data: bytes) -> None:\n        raise NotImplementedError()\n\n    def _process_server_headers(\n        self, key: Union[str, bytes], headers: httputil.HTTPHeaders\n    ) -> None:\n        raise NotImplementedError()\n\n    def start_pinging(self) -> None:\n        raise NotImplementedError()\n\n    async def _receive_frame_loop(self) -> None:\n        raise NotImplementedError()\n\n    def set_nodelay(self, x: bool) -> None:\n        raise NotImplementedError()\n\nclass WebSocketProtocol13(WebSocketProtocol):\n    FIN = 0x80\n    RSV1 = 0x40\n    RSV2 = 0x20\n    RSV3 = 0x10\n    RSV_MASK = RSV1 | RSV2 | RSV3\n    OPCODE_MASK = 0x0F\n\n    def __init__(\n        self,\n        handler: Any,\n        mask_outgoing: bool,\n        params: _WebSocketParams,\n    ) -> None:\n        super().__init__(handler)\n        self.mask_outgoing = mask_outgoing\n        self.params = params\n        self._final_frame = False\n        self._frame_opcode = None\n        self._frame_mask = None\n        self._frame_length = None\n        self._fragmented_message_buffer = None\n        self._fragmented_message_opcode = None\n        self._waiting = None\n        self._compression_options = params.compression_options\n        self._decompressor = None\n        self._compressor = None\n        self._frame_compressed = None\n        self._message_bytes_in = 0\n        self._message_bytes_out = 0\n        self._wire_bytes_in = 0\n        self._wire_bytes_out = 0\n        self.ping_callback = None\n        self.last_ping = 0.0\n        self.last_pong = 0.0\n        self.close_code = None\n        self.close_reason = None\n\n    @property\n    def selected_subprotocol(self) -> Optional[str]:\n        return self._selected_subprotocol\n\n    @selected_subprotocol.setter\n    def selected_subprotocol(self, value: Optional[str]) -> None:\n        self._selected_subprotocol = value\n\n    def _process_server_headers(\n        self, key: Union[str, bytes], headers: httputil.HTTPHeaders\n    ) -> None:\n        assert headers[\"Upgrade\"].lower() == \"websocket\"\n        assert headers[\"Connection\"].lower() == \"upgrade\"\n        accept = self.compute_accept_value(key)\n        assert headers[\"Sec-Websocket-Accept\"] == accept\n\n        extensions = self._parse_extensions_header(headers)\n        for ext in extensions:\n            if ext[0] == \"permessage-deflate\" and self._compression_options is not None:\n                self._create_compressors(\"client\", ext[1])\n            else:\n                raise ValueError(\"unsupported extension %r\", ext)\n\n        self.selected_subprotocol = headers.get(\"Sec-WebSocket-Protocol\", None)\n\n    def write_message(\n        self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False\n    ) -> \"Future[None]\":\n        if binary:\n            opcode = 0x2\n        else:\n            opcode = 0x1\n        if isinstance(message, dict):\n            message = tornado.escape.json_encode(message)\n        message = tornado.escape.utf8(message)\n        assert isinstance(message, bytes)\n        self._message_bytes_out += len(message)\n        flags = 0\n        if self._compressor:\n            message = self._compressor.compress(message)\n            flags |= self.RSV1\n        try:\n            fut = self._write_frame(True, opcode, message, flags=flags)\n        except StreamClosedError:\n            raise WebSocketClosedError()\n\n        async def wrapper() -> None:\n            try:\n                await fut\n            except StreamClosedError:\n                raise WebSocketClosedError()\n\n        return asyncio.ensure_future(wrapper())\n\n    def write_ping(self, data: bytes) -> None:\n        assert isinstance(data, bytes)\n        self._write_frame(True, 0x9, data)\n\n    def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n        if not self.server_terminated:\n            if not self.stream.closed():\n                if code is None and reason is not None:\n                    code = 1000\n                if code is None:\n                    close_data = b\"\"\n                else:\n                    close_data = struct.pack(\">H\", code)\n                if reason is not None:\n                    close_data += utf8(reason)\n                try:\n                    self._write_frame(True, 0x8, close_data)\n                except StreamClosedError:\n                    self._abort()\n            self.server_terminated = True\n        if self.client_terminated:\n            if self._waiting is not None:\n                self.stream.io_loop.remove_timeout(self._waiting)\n                self._waiting = None\n            self.stream.close()\n        elif self._waiting is None:\n            self._waiting = self.stream.io_loop.add_timeout(\n                self.stream.io_loop.time() + 5, self._abort\n            )\n        if self.ping_callback:\n            self.ping_callback.stop()\n            self.ping_callback = None\n\n    def is_closing(self) -> bool:\n        return self.stream.closed() or self.client_terminated or self.server_terminated\n\n    def start_pinging(self) -> None:\n        if self.ping_interval is not None and self.ping_interval > 0:\n            self.last_ping = self.last_pong = IOLoop.current().time()\n            self.ping_callback = PeriodicCallback(\n                self.periodic_ping, self.ping_interval * 1000\n            )\n            self.ping_callback.start()\n\n    @staticmethod\n    def compute_accept_value(key: Union[str, bytes]) -> str:\n        sha1 = hashlib.sha1()\n        sha1.update(utf8(key))\n        sha1.update(b\"258EAFA5-E914-47DA-95CA-C5AB0DC85B11\")\n        return native_str(base64.b64encode(sha1.digest()))\n\n    def _parse_extensions_header(\n        self, headers: httputil.HTTPHeaders\n    ) -> List[Tuple[str, Dict[str, str]]]:\n        extensions = headers.get(\"Sec-WebSocket-Extensions\", \"\")\n        if extensions:\n            return [httputil._parse_header(e.strip()) for e in extensions.split(\",\")]\n        return []\n\n    def _write_frame(\n        self, fin: bool, opcode: int, data: bytes, flags: int = 0\n    ) -> \"Future[None]\":\n        data_len = len(data)\n        if opcode & 0x8:\n            if not fin:\n                raise ValueError(\"control frames may not be fragmented\")\n            if data_len > 125:\n                raise ValueError(\"control frame payloads may not exceed 125 bytes\")\n        if fin:\n            finbit = self.FIN\n        else:\n            finbit = 0\n        frame = struct.pack(\"B\", finbit | opcode | flags)\n        if self.mask_outgoing:\n            mask_bit = 0x80\n        else:\n            mask_bit = 0\n        if data_len < 126:\n            frame += struct.pack(\"B\", data_len | mask_bit)\n        elif data_len <= 0xFFFF:\n            frame += struct.pack(\"!BH\", 126 | mask_bit, data_len)\n        else:\n            frame += struct.pack(\"!BQ\", 127 | mask_bit, data_len)\n        if self.mask_outgoing:\n            mask = os.urandom(4)\n            data = mask + _websocket_mask(mask, data)\n        frame += data\n        self._wire_bytes_out += len(frame)\n        return self.stream.write(frame)\n\ndef websocket_connect(\n    url: Union[str, httpclient.HTTPRequest],\n    callback: Optional[Callable[[\"Future[WebSocketClientConnection]\"], None]] = None,\n    connect_timeout: Optional[float] = None,\n    on_message_callback: Optional[Callable[[Union[None, str, bytes]], None]] = None,\n    compression_options: Optional[Dict[str, Any]] = None,\n    ping_interval: Optional[float] = None,\n    ping_timeout: Optional[float] = None,\n    max_message_size: int = _default_max_message_size,\n    subprotocols: Optional[List[str]] = None,\n    resolver: Optional[Resolver] = None,\n) -> \"Awaitable[WebSocketClientConnection]\":\n    if isinstance(url, httpclient.HTTPRequest):\n        assert connect_timeout is None\n        request = url\n        request.headers = httputil.HTTPHeaders(request.headers)\n    else:\n        request = httpclient.HTTPRequest(url, connect_timeout=connect_timeout)\n    request = cast(\n        httpclient.HTTPRequest,\n        httpclient._RequestProxy(request, httpclient.HTTPRequest._DEFAULTS),\n    )\n    conn = WebSocketClientConnection(\n        request,\n        on_message_callback=on_message_callback,\n        compression_options=compression_options,\n        ping_interval=ping_interval,\n        ping_timeout=ping_timeout,\n        max_message_size=max_message_size,\n        subprotocols=subprotocols,\n        resolver=resolver,\n    )\n    if callback is not None:\n        IOLoop.current().add_future(conn.connect_future, callback)\n    return conn.connect_future",
    "Code_level": "Class-level",
    "FSMID_for_test": 51,
    "wrong_result": "import asyncio\nfrom typing import Optional, Union, Dict, List, Any, Callable, Awaitable\nfrom tornado.httpclient import HTTPRequest, HTTPResponse\nfrom tornado.httputil import HTTPHeaders, RequestStartLine, ResponseStartLine\nfrom tornado.iostream import StreamClosedError\nfrom tornado.tcpclient import TCPClient\nfrom tornado.queues import Queue\nfrom tornado.concurrent import Future\nfrom tornado.websocket import WebSocketProtocol, WebSocketHandler, _WebSocketParams\nfrom tornado.netutil import Resolver\n\nclass WebSocketClientConnection:\n    def __init__(\n        self,\n        request: HTTPRequest,\n        on_message_callback: Optional[Callable] = None,\n        compression_options: Optional[Dict] = None,\n        ping_interval: Optional[float] = None,\n        ping_timeout: Optional[float] = None,\n        max_message_size: int = 10 * 1024 * 1024,\n        subprotocols: Optional[List[str]] = None,\n        resolver: Optional[Resolver] = None,\n    ):\n        self.connect_future = Future()\n        self.read_queue = Queue()\n        self.key = WebSocketHandler._generate_websocket_key()\n        self._on_message_callback = on_message_callback\n        self.close_code = None\n        self.close_reason = None\n        self.params = _WebSocketParams(\n            compression_options=compression_options,\n            ping_interval=ping_interval,\n            ping_timeout=ping_timeout,\n            max_message_size=max_message_size,\n        )\n        self.tcp_client = TCPClient(resolver=resolver)\n        self.protocol = None\n\n        request.headers.update({\n            \"Upgrade\": \"websocket\",\n            \"Connection\": \"Upgrade\",\n            \"Sec-WebSocket-Key\": self.key,\n            \"Sec-WebSocket-Version\": \"13\",\n        })\n        if subprotocols:\n            request.headers[\"Sec-WebSocket-Protocol\"] = \",\".join(subprotocols)\n\n        self.tcp_client.connect(\n            request,\n            callback=self._on_http_response,\n            streaming_callback=self.headers_received,\n        )\n\n    def close(self, code: Optional[int] = None, reason: Optional[str] = None) -> None:\n        if self.protocol:\n            self.protocol.close(code, reason)\n        else:\n            self.on_ws_connection_close(code, reason)\n\n    def on_connection_close(self) -> None:\n        self.close()\n\n    def on_ws_connection_close(self, close_code: Optional[int], close_reason: Optional[str]) -> None:\n        self.close_code = close_code\n        self.close_reason = close_reason\n        if not self.connect_future.done():\n            self.connect_future.set_exception(StreamClosedError())\n        self.connect_future = None\n\n    def _on_http_response(self, response: HTTPResponse) -> None:\n        if response.error:\n            self.connect_future.set_exception(response.error)\n            return\n        self.protocol = self.get_websocket_protocol()\n        self.protocol._process_server_headers(response.headers)\n        self.protocol.start_client()\n        self.connect_future.set_result(self)\n\n    async def headers_received(self, start_line: Union[RequestStartLine, ResponseStartLine], headers: HTTPHeaders) -> Awaitable[None]:\n        pass\n\n    def write_message(self, message: Union[str, bytes, Dict[str, Any]], binary: bool = False) -> Future[None]:\n        if not self.protocol:\n            raise RuntimeError(\"WebSocket connection is closed\")\n        return self.protocol.write_message(message, binary)\n\n    async def read_message(self, callback: Optional[Callable] = None) -> Awaitable[Union[None, str, bytes]]:\n        if callback:\n            self._on_message_callback = callback\n        return await self.read_queue.get()\n\n    def _on_message(self, message: Union[None, str, bytes]) -> Optional[Awaitable[None]]:\n        if self._on_message_callback:\n            return self._on_message_callback(message)\n        else:\n            return self.read_queue.put(message)\n\n    def ping(self, data: bytes) -> None:\n        if self.protocol:\n            self.protocol.write_ping(data)\n\n    def get_websocket_protocol(self) -> WebSocketProtocol:\n        protocol = WebSocketProtocol(\n            self,\n            mask_outgoing=True,\n            compression_options=self.params.compression_options,\n            ping_interval=self.params.ping_interval,\n            ping_timeout=self.params.ping_timeout,\n            max_message_size=self.params.max_message_size,\n        )\n        protocol._on_message_callback = self._on_message\n        protocol._on_connection_close = self.on_ws_connection_close\n        return protocol"
  },
  {
    "Task_id": 439,
    "Instruction": "Task Description: Implement a WebSocket client class for streaming financial market data with authentication, subscription management, and message handling capabilities.\n\nClass Description: The _DataStream class establishes and maintains a WebSocket connection to a financial data streaming service. It handles authentication, manages subscriptions to different types of market data (trades, quotes, bars), processes incoming messages, and provides methods for controlling the connection lifecycle.\n\nAttributes:\n- _endpoint: str - WebSocket server URL\n- _key_id: str - API key for authentication\n- _secret_key: str - API secret for authentication\n- _ws: WebSocketClientProtocol - Active WebSocket connection\n- _running: bool - Connection status flag\n- _loop: asyncio.AbstractEventLoop - Event loop reference\n- _raw_data: bool - Flag for raw data processing\n- _stop_stream_queue: queue.Queue - Queue for stop signals\n- _handlers: Dict[str, Dict] - Dictionary of message handlers per data type\n- _name: str - Connection name identifier\n- _should_run: bool - Control flag for main loop\n- _max_frame_size: int - Maximum WebSocket frame size\n- _websocket_params: Dict - WebSocket connection parameters\n\nMethods:\n- __init__(endpoint: str, key_id: str, secret_key: str, raw_data: bool = False, websocket_params: Optional[Dict] = None) -> None - Initialize the data stream with connection parameters\n- _connect() -> None - Establish WebSocket connection\n- _auth() -> None - Authenticate with the server\n- _start_ws() -> None - Combined connection and authentication\n- close() -> None - Close the WebSocket connection\n- stop_ws() -> None - Signal to stop the WebSocket connection\n- _consume() -> None - Main message processing loop\n- _dispatch(msg: Dict) -> None - Route messages to appropriate handlers\n- _subscribe(handler: Callable, symbols: List[str], handlers: Dict) -> None - Internal subscription management\n- _subscribe_all() -> None - Send all active subscriptions to server\n- _run_forever() -> None - Main connection management loop\n- subscribe_trades(handler: Callable, *symbols: str) -> None - Subscribe to trade data\n- subscribe_quotes(handler: Callable, *symbols: str) -> None - Subscribe to quote data\n- subscribe_bars(handler: Callable, *symbols: str) -> None - Subscribe to bar data\n- subscribe_updated_bars(handler: Callable, *symbols: str) -> None - Subscribe to updated bar data\n- subscribe_daily_bars(handler: Callable, *symbols: str) -> None - Subscribe to daily bar data\n- stop() -> None - Stop the WebSocket connection and processing",
    "Canonical_solution": "import asyncio\nimport websockets\nimport msgpack\nimport queue\nfrom typing import Dict, Optional\n\nclass _DataStream:\n    def __init__(self,\n                 endpoint: str,\n                 key_id: str,\n                 secret_key: str,\n                 raw_data: bool = False,\n                 websocket_params: Optional[Dict] = None) -> None:\n        self._endpoint = endpoint\n        self._key_id = key_id\n        self._secret_key = secret_key\n        self._ws = None\n        self._running = False\n        self._loop = None\n        self._raw_data = raw_data\n        self._stop_stream_queue = queue.Queue()\n        self._handlers = {\n            'trades':      {},\n            'quotes':      {},\n            'bars':        {},\n            'updatedBars': {},\n            'dailyBars':   {},\n        }\n        self._name = 'data'\n        self._should_run = True\n        self._max_frame_size = 32768\n        self._websocket_params = websocket_params or {\n            \"ping_interval\": 10,\n            \"ping_timeout\": 180,\n            \"max_queue\": 1024,\n        }\n\n    async def _connect(self):\n        self._ws = await websockets.connect(\n            self._endpoint,\n            extra_headers={'Content-Type': 'application/msgpack'},\n            **self._websocket_params\n        )\n        r = await self._ws.recv()\n        msg = msgpack.unpackb(r)\n        if msg[0]['T'] != 'success' or msg[0]['msg'] != 'connected':\n            raise ValueError('connected message not received')\n\n    async def _auth(self):\n        await self._ws.send(\n            msgpack.packb({\n                'action': 'auth',\n                'key':    self._key_id,\n                'secret': self._secret_key,\n            }))\n        r = await self._ws.recv()\n        msg = msgpack.unpackb(r)\n        if msg[0]['T'] == 'error':\n            raise ValueError(msg[0].get('msg', 'auth failed'))\n        if msg[0]['T'] != 'success' or msg[0]['msg'] != 'authenticated':\n            raise ValueError('failed to authenticate')\n\n    async def _start_ws(self):\n        await self._connect()\n        await self._auth()\n\n    async def close(self):\n        if self._ws:\n            await self._ws.close()\n            self._ws = None\n            self._running = False\n\n    async def stop_ws(self):\n        self._should_run = False\n        if self._stop_stream_queue.empty():\n            self._stop_stream_queue.put_nowait({\"should_stop\": True})\n\n    async def _consume(self):\n        while True:\n            if not self._stop_stream_queue.empty():\n                self._stop_stream_queue.get(timeout=1)\n                await self.close()\n                break\n            else:\n                try:\n                    r = await asyncio.wait_for(self._ws.recv(), 5)\n                    msgs = msgpack.unpackb(r)\n                    for msg in msgs:\n                        await self._dispatch(msg)\n                except asyncio.TimeoutError:\n                    pass\n\n    async def _dispatch(self, msg):\n        msg_type = msg.get('T')\n        symbol = msg.get('S')\n        if msg_type == 't':\n            handler = self._handlers['trades'].get(\n                symbol, self._handlers['trades'].get('*', None))\n            if handler:\n                await handler(self._cast(msg_type, msg))\n        elif msg_type == 'q':\n            handler = self._handlers['quotes'].get(\n                symbol, self._handlers['quotes'].get('*', None))\n            if handler:\n                await handler(self._cast(msg_type, msg))\n        elif msg_type == 'b':\n            handler = self._handlers['bars'].get(\n                symbol, self._handlers['bars'].get('*', None))\n            if handler:\n                await handler(self._cast(msg_type, msg))\n        elif msg_type == 'u':\n            handler = self._handlers['updatedBars'].get(\n                symbol, self._handlers['updatedBars'].get('*', None))\n            if handler:\n                await handler(self._cast(msg_type, msg))\n        elif msg_type == 'd':\n            handler = self._handlers['dailyBars'].get(\n                symbol, self._handlers['dailyBars'].get('*', None))\n            if handler:\n                await handler(self._cast(msg_type, msg))\n        elif msg_type == 'error':\n            log.error(f'error: {msg.get(\"msg\")} ({msg.get(\"code\")})')\n\n    def _subscribe(self, handler, symbols, handlers):\n        for symbol in symbols:\n            handlers[symbol] = handler\n        if self._running:\n            asyncio.run_coroutine_threadsafe(\n                self._subscribe_all(), self._loop\n            ).result()\n\n    async def _subscribe_all(self):\n        msg = defaultdict(list)\n        for k, v in self._handlers.items():\n            if v:\n                for s in v.keys():\n                    msg[k].append(s)\n        msg['action'] = 'subscribe'\n        bs = msgpack.packb(msg)\n        frames = (bs[i:i+self._max_frame_size]\n                  for i in range(0, len(bs), self._max_frame_size))\n        await self._ws.send(frames)\n\n    async def _run_forever(self):\n        self._loop = asyncio.get_running_loop()\n        while not any(v for k, v in self._handlers.items()):\n            if not self._stop_stream_queue.empty():\n                self._stop_stream_queue.get(timeout=1)\n                return\n            await asyncio.sleep(0.1)\n        self._should_run = True\n        self._running = False\n        while True:\n            try:\n                if not self._should_run:\n                    return\n                if not self._running:\n                    await self._start_ws()\n                    await self._subscribe_all()\n                    self._running = True\n                await self._consume()\n            except websockets.WebSocketException as wse:\n                await self.close()\n                self._running = False\n            finally:\n                await asyncio.sleep(0.01)\n\n    def subscribe_trades(self, handler, *symbols):\n        self._subscribe(handler, symbols, self._handlers['trades'])\n\n    def subscribe_quotes(self, handler, *symbols):\n        self._subscribe(handler, symbols, self._handlers['quotes'])\n\n    def subscribe_bars(self, handler, *symbols):\n        self._subscribe(handler, symbols, self._handlers['bars'])\n\n    def subscribe_updated_bars(self, handler, *symbols):\n        self._subscribe(handler, symbols, self._handlers['updatedBars'])\n\n    def subscribe_daily_bars(self, handler, *symbols):\n        self._subscribe(handler, symbols, self._handlers['dailyBars'])\n\n    def stop(self):\n        if self._loop.is_running():\n            asyncio.run_coroutine_threadsafe(\n                self.stop_ws(),\n                self._loop).result()",
    "Code_level": "Class-level",
    "FSMID_for_test": 46,
    "wrong_result": "import asyncio\nimport json\nimport queue\nfrom typing import Dict, List, Optional, Callable\nfrom websockets.client import WebSocketClientProtocol, connect\n\nclass _DataStream:\n    def __init__(\n        self,\n        endpoint: str,\n        key_id: str,\n        secret_key: str,\n        raw_data: bool = False,\n        websocket_params: Optional[Dict] = None,\n    ) -> None:\n        self._endpoint = endpoint\n        self._key_id = key_id\n        self._secret_key = secret_key\n        self._ws = None\n        self._running = False\n        self._loop = asyncio.get_event_loop()\n        self._raw_data = raw_data\n        self._stop_stream_queue = queue.Queue()\n        self._handlers = {\n            \"trades\": {},\n            \"quotes\": {},\n            \"bars\": {},\n            \"updated_bars\": {},\n            \"daily_bars\": {},\n        }\n        self._name = \"alpaca_websocket\"\n        self._should_run = False\n        self._max_frame_size = 2**25\n        self._websocket_params = websocket_params or {}\n\n    async def _connect(self) -> None:\n        self._ws = await connect(\n            self._endpoint,\n            max_size=self._max_frame_size,\n            **self._websocket_params,\n        )\n        self._running = True\n\n    async def _auth(self) -> None:\n        auth_msg = {\n            \"action\": \"authenticate\",\n            \"data\": {\n                \"key_id\": self._key_id,\n                \"secret_key\": self._secret_key,\n            },\n        }\n        await self._ws.send(json.dumps(auth_msg))\n\n    async def _start_ws(self) -> None:\n        await self._connect()\n        await self._auth()\n\n    async def close(self) -> None:\n        if self._ws:\n            await self._ws.close()\n            self._ws = None\n            self._running = False\n\n    async def stop_ws(self) -> None:\n        self._should_run = False\n        await self.close()\n\n    async def _consume(self) -> None:\n        while self._running:\n            try:\n                msg = await self._ws.recv()\n                if not self._raw_data:\n                    msg = json.loads(msg)\n                await self._dispatch(msg)\n            except Exception as e:\n                await self.stop_ws()\n                raise e\n\n    async def _dispatch(self, msg: Dict) -> None:\n        if \"stream\" in msg:\n            stream = msg[\"stream\"]\n            for handler_type, handlers in self._handlers.items():\n                if stream.endswith(handler_type):\n                    for symbol, handler in handlers.items():\n                        if symbol in stream:\n                            await handler(msg)\n\n    def _subscribe(self, handler: Callable, symbols: List[str], handlers: Dict) -> None:\n        for symbol in symbols:\n            handlers[symbol.upper()] = handler\n\n    async def _subscribe_all(self) -> None:\n        subscriptions = []\n        for handler_type, handlers in self._handlers.items():\n            if handlers:\n                subscriptions.append(handler_type)\n        if subscriptions:\n            sub_msg = {\n                \"action\": \"listen\",\n                \"data\": {\n                    \"streams\": subscriptions,\n                },\n            }\n            await self._ws.send(json.dumps(sub_msg))\n\n    async def _run_forever(self) -> None:\n        self._should_run = True\n        while self._should_run:\n            try:\n                if not self._running:\n                    await self._start_ws()\n                    await self._subscribe_all()\n                await self._consume()\n            except Exception as e:\n                await self.stop_ws()\n                raise e\n\n    def subscribe_trades(self, handler: Callable, *symbols: str) -> None:\n        self._subscribe(handler, symbols, self._handlers[\"trades\"])\n\n    def subscribe_quotes(self, handler: Callable, *symbols: str) -> None:\n        self._subscribe(handler, symbols, self._handlers[\"quotes\"])\n\n    def subscribe_bars(self, handler: Callable, *symbols: str) -> None:\n        self._subscribe(handler, symbols, self._handlers[\"bars\"])\n\n    def subscribe_updated_bars(self, handler: Callable, *symbols: str) -> None:\n        self._subscribe(handler, symbols, self._handlers[\"updated_bars\"])\n\n    def subscribe_daily_bars(self, handler: Callable, *symbols: str) -> None:\n        self._subscribe(handler, symbols, self._handlers[\"daily_bars\"])\n\n    async def stop(self) -> None:\n        await self.stop_ws()"
  },
  {
    "Task_id": 440,
    "Instruction": "Task Description: Create a WebSocket client in Python using asyncio and websockets libraries to connect to a WebSocket server, subscribe to a channel, and receive messages, optionally decompressing them if they are gzipped.\n\nFunction Description: The function establishes a WebSocket connection to a specified URI, sends a subscription message, and receives a specified number of messages from the server. It can handle gzip-compressed messages by decompressing them before display.\n\nInput:\n- uri (str): The WebSocket server URI to connect to (e.g., 'wss://api.huobi.pro/ws').\n- sub (str): The subscription message to send to the server (e.g., '{\"sub\": \"market.btcusdt.trade.detail\", \"id\": 4}').\n- is_gzip (bool): A flag indicating whether the received messages are gzip-compressed.\n- count (int): The number of messages to receive before exiting.\n\nOutput: None (The function prints received messages to the console).",
    "Canonical_solution": "import argparse\nimport asyncio\nimport zlib\nimport websockets\n\nasync def websocket_client(uri, sub, is_gzip, count):\n    async with websockets.connect(uri) as websocket:\n        await websocket.send(sub)\n        print(f\"> {sub}\")\n\n        for i in range(count):\n            response = await websocket.recv()\n            if not is_gzip:\n                print(f\"< {response}\")\n            else:\n                print(f\"< {zlib.decompress(response, 16 + zlib.MAX_WBITS)}\")\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--uri', default='wss://api.huobi.pro/ws', help='URI to connect to')\n    parser.add_argument('--sub', default='{\"sub\": \"market.btcusdt.trade.detail\", \"id\": 4}', help='Subscription string')\n    parser.add_argument('--count', default=3, type=int, help='Number of messages to receive before exiting')\n    parser.add_argument('-z', action='store_true', help='Use gzip on messages')\n    args = parser.parse_args()\n\n    asyncio.get_event_loop().run_until_complete(\n        websocket_client(args.uri, args.sub, args.z, args.count)\n    )",
    "Code_level": "Function-level",
    "FSMID_for_test": 46,
    "wrong_result": "import asyncio\nimport websockets\nimport gzip\n\nasync def websocket_client(uri, sub, is_gzip, count):\n    async with websockets.connect(uri) as websocket:\n        await websocket.send(sub)\n        received = 0\n        while received < count:\n            message = await websocket.recv()\n            if is_gzip:\n                message = gzip.decompress(message).decode('utf-8')\n            print(message)\n            received += 1"
  },
  {
    "Task_id": 445,
    "Instruction": "Task Description: Implement a WebSocket server using FastAPI that validates user sessions by performing a bitwise operation on received user and session IDs.\n\nFunction Description: The function `websocket_endpoint` establishes a WebSocket connection, receives binary data containing user and session IDs, validates them using a specific bitwise operation, and sends back the validation result as binary data.\n\nInput: \n- websocket: WebSocket - The WebSocket connection object provided by FastAPI\n- data: bytes - Binary data received from the client containing packed user_id and session_id (two unsigned 32-bit integers in little-endian format)\n\nOutput: \n- None (The function sends binary data back through the WebSocket connection but doesn't return anything)\n- The sent data contains a packed unsigned 32-bit integer (little-endian) representing the validation result (1 for valid, 0 for invalid)",
    "Canonical_solution": "from fastapi import FastAPI, WebSocket\nimport struct\n\napp = FastAPI()\n\n@app.websocket('/validate_session_ws')\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    while True:\n        data = await websocket.receive_bytes()\n        user_id, session_id = struct.unpack('<II', data)\n        result = (user_id ^ session_id) % 23 == 0\n        await websocket.send_bytes(struct.pack('<I', result))",
    "Code_level": "Function-level",
    "FSMID_for_test": 48,
    "wrong_result": "from fastapi import WebSocket\nimport struct\n\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    try:\n        while True:\n            data = await websocket.receive_bytes()\n            if len(data) != 8:\n                await websocket.send_bytes(struct.pack('<I', 0))\n                continue\n            user_id, session_id = struct.unpack('<II', data)\n            validation_result = (user_id & session_id) == (user_id ^ session_id)\n            await websocket.send_bytes(struct.pack('<I', 1 if validation_result else 0))\n    except Exception:\n        await websocket.close()"
  },
  {
    "Task_id": 446,
    "Instruction": "Task Description: Create a WebSocket server using FastAPI that accepts client connections, sends a sequence of numbers at a specified interval, and then closes the connection.\n\nFunction Description: The function `websocket_endpoint` establishes a WebSocket connection with a client, sends a sequence of numbers (0 to 4) at a specified time interval, and then closes the connection. It handles any exceptions that may occur during the process.\n\nInput:  \n- `websocket`: WebSocket - The WebSocket connection instance provided by FastAPI.  \n- `interval`: int (default: 1) - The time interval (in seconds) between sending consecutive numbers.  \n\nOutput: None (The function sends messages over the WebSocket but does not return any value.)",
    "Canonical_solution": "import asyncio\nfrom fastapi import FastAPI, WebSocket\n\napp = FastAPI()\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket, interval: int = 1):\n    await websocket.accept()\n    try:\n        for i in range(5):\n            await websocket.send_text(str(i))\n            await asyncio.sleep(interval)\n        await websocket.close()\n    except Exception as e:\n        print(e)",
    "Code_level": "Function-level",
    "FSMID_for_test": 48,
    "wrong_result": "from fastapi import WebSocket\nimport asyncio\n\nasync def websocket_endpoint(websocket: WebSocket, interval: int = 1):\n    await websocket.accept()\n    try:\n        for i in range(5):\n            await websocket.send_text(str(i))\n            await asyncio.sleep(interval)\n        await websocket.close()\n    except Exception as e:\n        await websocket.close(code=1011)"
  },
  {
    "Task_id": 447,
    "Instruction": "Task Description: Implement a WebSocket connection manager class for handling multiple client connections, supporting different subprotocols (JSON and BYTES), and managing message sending/receiving.\n\nClass Description: The ConnectionManager class manages active WebSocket connections, handles different WebSocket subprotocols, and provides methods for sending/receiving messages in the appropriate format.\n\nAttributes:\n- active_connections: List[WebSocket] - Maintains a list of currently active WebSocket connections\n- protocol_dict: Dict[str, WebsocketSubProtocols] - Maps client addresses to their chosen subprotocols\n\nMethods:\n- get_client(websocket: WebSocket) -> str - Returns a string identifier for the client (host:port)\n- get_subprotocol(headers: Dict) -> WebsocketSubProtocols - Determines the subprotocol from request headers\n- connect(websocket: WebSocket) -> None - Accepts a new WebSocket connection and stores its subprotocol\n- disconnect(websocket: WebSocket) -> None - Removes a disconnected WebSocket from active connections\n- receive(websocket: WebSocket) -> Any - Receives a message from the WebSocket using the appropriate subprotocol\n- iter(websocket: WebSocket) -> AsyncIterator[Any] - Provides an async iterator for continuously receiving messages\n- send(websocket: WebSocket, data: Union[DataRequest, StatusMessage]) -> None - Sends data through the WebSocket using the appropriate subprotocol",
    "Canonical_solution": "from typing import Any, AsyncIterator, Dict, List, Optional, Union\nfrom fastapi import FastAPI, Response, WebSocket, WebSocketDisconnect, status\nfrom jina.enums import WebsocketSubProtocols\nfrom jina.excepts import InternalNetworkError\nfrom jina.types.request.data import DataRequest\nfrom jina.types.request.status import StatusMessage\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        self.protocol_dict: Dict[str, WebsocketSubProtocols] = {}\n\n    def get_client(self, websocket: WebSocket) -> str:\n        return f'{websocket.client.host}:{websocket.client.port}'\n\n    def get_subprotocol(self, headers: Dict):\n        try:\n            if 'sec-websocket-protocol' in headers:\n                subprotocol = WebsocketSubProtocols(headers['sec-websocket-protocol'])\n            elif b'sec-websocket-protocol' in headers:\n                subprotocol = WebsocketSubProtocols(headers[b'sec-websocket-protocol'].decode())\n            else:\n                subprotocol = WebsocketSubProtocols.JSON\n        except Exception:\n            subprotocol = WebsocketSubProtocols.JSON\n        return subprotocol\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        subprotocol = self.get_subprotocol(dict(websocket.scope['headers']))\n        self.active_connections.append(websocket)\n        self.protocol_dict[self.get_client(websocket)] = subprotocol\n\n    def disconnect(self, websocket: WebSocket):\n        self.protocol_dict.pop(self.get_client(websocket))\n        self.active_connections.remove(websocket)\n\n    async def receive(self, websocket: WebSocket) -> Any:\n        subprotocol = self.protocol_dict[self.get_client(websocket)]\n        if subprotocol == WebsocketSubProtocols.JSON:\n            return await websocket.receive_json(mode='text')\n        elif subprotocol == WebsocketSubProtocols.BYTES:\n            return await websocket.receive_bytes()\n\n    async def iter(self, websocket: WebSocket) -> AsyncIterator[Any]:\n        try:\n            while True:\n                yield await self.receive(websocket)\n        except WebSocketDisconnect:\n            pass\n\n    async def send(self, websocket: WebSocket, data: Union[DataRequest, StatusMessage]) -> None:\n        subprotocol = self.protocol_dict[self.get_client(websocket)]\n        if subprotocol == WebsocketSubProtocols.JSON:\n            return await websocket.send_json(data.to_dict(), mode='text')\n        elif subprotocol == WebsocketSubProtocols.BYTES:\n            return await websocket.send_bytes(data.to_bytes())",
    "Code_level": "Class-level",
    "FSMID_for_test": 48,
    "wrong_result": "import asyncio\nfrom typing import Any, Dict, List, Union, AsyncIterator\nfrom websockets import WebSocket\nfrom enum import Enum\n\nclass WebsocketSubProtocols(Enum):\n    JSON = \"json\"\n    BYTES = \"bytes\"\n\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n        self.protocol_dict: Dict[str, WebsocketSubProtocols] = {}\n\n    def get_client(self, websocket: WebSocket) -> str:\n        return f\"{websocket.remote_address[0]}:{websocket.remote_address[1]}\"\n\n    def get_subprotocol(self, headers: Dict) -> WebsocketSubProtocols:\n        subprotocol = headers.get(\"sec-websocket-protocol\", \"\").lower()\n        if subprotocol == \"bytes\":\n            return WebsocketSubProtocols.BYTES\n        return WebsocketSubProtocols.JSON\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        client = self.get_client(websocket)\n        self.protocol_dict[client] = self.get_subprotocol(websocket.request_headers)\n        self.active_connections.append(websocket)\n\n    async def disconnect(self, websocket: WebSocket):\n        client = self.get_client(websocket)\n        self.protocol_dict.pop(client, None)\n        self.active_connections.remove(websocket)\n\n    async def receive(self, websocket: WebSocket) -> Any:\n        client = self.get_client(websocket)\n        protocol = self.protocol_dict[client]\n        if protocol == WebsocketSubProtocols.JSON:\n            return await websocket.receive_json()\n        return await websocket.receive_bytes()\n\n    async def iter(self, websocket: WebSocket) -> AsyncIterator[Any]:\n        try:\n            while True:\n                yield await self.receive(websocket)\n        except Exception:\n            await self.disconnect(websocket)\n\n    async def send(self, websocket: WebSocket, data: Union[Dict, bytes]):\n        client = self.get_client(websocket)\n        protocol = self.protocol_dict[client]\n        if protocol == WebsocketSubProtocols.JSON:\n            await websocket.send_json(data)\n        else:\n            await websocket.send_bytes(data)"
  },
  {
    "Task_id": 448,
    "Instruction": "Task Description: Implement a WebSocket server using Tornado in Python that handles WebSocket connections, processes incoming messages, and manages connection lifecycle events.\n\nClass Description: WebSocketServer is a class that sets up a Tornado WebSocket server with an echo handler that sends received messages back to the client.\n\nAttributes:\n- close_future: [Future] - A Future object used to track when the WebSocket connection is closed.\n- app: [Application] - The Tornado web application instance that routes WebSocket connections.\n\nMethods:\n- __init__: [__init__]() -> [None] - Initializes the WebSocket server, creates the Application instance, and sets up the echo handler route.\n- get_app: [get_app]() -> [Application] - Returns the Tornado web application instance.\n- ws_connect: [ws_connect](path: str, **kwargs: dict) -> [WebSocketClientConnection] - Establishes a WebSocket connection to the specified path with optional connection parameters.\n\nClass Description: TestWebSocketHandler is a base WebSocket handler class that provides common functionality for WebSocket connections.\n\nAttributes:\n- close_future: [Future] - A Future object used to signal when the connection is closed.\n- compression_options: [dict] - Options for WebSocket compression.\n\nMethods:\n- initialize: [initialize](close_future: Future = None, compression_options: dict = None) -> [None] - Initializes the handler with optional close_future and compression_options.\n- get_compression_options: [get_compression_options]() -> [dict] - Returns the compression options for the WebSocket connection.\n- on_close: [on_close]() -> [None] - Called when the WebSocket connection is closed; resolves the close_future if it exists.\n\nClass Description: EchoHandler is a WebSocket handler that echoes received messages back to the client.\n\nMethods:\n- on_message: [on_message](message: str or bytes) -> [None] - Handles incoming WebSocket messages and echoes them back to the client. Handles cancellation and closed connection errors gracefully.",
    "Canonical_solution": "from tornado.websocket import WebSocketHandler\nfrom tornado.web import Application\nfrom tornado.concurrent import Future\nfrom tornado import gen\n\nclass TestWebSocketHandler(WebSocketHandler):\n    def initialize(self, close_future=None, compression_options=None):\n        self.close_future = close_future\n        self.compression_options = compression_options\n\n    def get_compression_options(self):\n        return self.compression_options\n\n    def on_close(self):\n        if self.close_future is not None:\n            self.close_future.set_result((self.close_code, self.close_reason))\n\nclass EchoHandler(TestWebSocketHandler):\n    @gen.coroutine\n    def on_message(self, message):\n        try:\n            yield self.write_message(message, isinstance(message, bytes))\n        except asyncio.CancelledError:\n            pass\n        except WebSocketClosedError:\n            pass\n\nclass WebSocketServer:\n    def __init__(self):\n        self.close_future = Future()\n        self.app = Application([\n            (\"/echo\", EchoHandler, dict(close_future=self.close_future))\n        ])\n\n    def get_app(self):\n        return self.app\n\n    @gen.coroutine\n    def ws_connect(self, path, **kwargs):\n        ws = yield websocket_connect(\n            \"ws://127.0.0.1:%d%s\" % (self.get_http_port(), path), **kwargs\n        )\n        raise gen.Return(ws)",
    "Code_level": "Class-level",
    "FSMID_for_test": 50,
    "wrong_result": "import asyncio\nfrom tornado import web, websocket, ioloop, httpclient\n\nclass WebSocketServer:\n    def __init__(self):\n        self.close_future = asyncio.Future()\n        self.app = web.Application([\n            (r\"/echo\", EchoHandler, dict(close_future=self.close_future)),\n        ])\n\n    def get_app(self):\n        return self.app\n\n    async def ws_connect(self, path, **kwargs):\n        ws_url = f\"ws://localhost:8888{path}\"\n        client = httpclient.AsyncHTTPClient()\n        request = httpclient.HTTPRequest(ws_url, **kwargs)\n        ws_conn = await websocket.websocket_connect(request)\n        return ws_conn\n\nclass TestWebSocketHandler(websocket.WebSocketHandler):\n    def initialize(self, close_future=None, compression_options=None):\n        self.close_future = close_future\n        self.compression_options = compression_options\n\n    def get_compression_options(self):\n        return self.compression_options\n\n    def on_close(self):\n        if self.close_future and not self.close_future.done():\n            self.close_future.set_result(None)\n\nclass EchoHandler(TestWebSocketHandler):\n    def on_message(self, message):\n        try:\n            if not self.ws_connection or self.ws_connection.is_closing():\n                return\n            self.write_message(message)\n        except (websocket.WebSocketClosedError, asyncio.CancelledError):\n            pass"
  },
  {
    "Task_id": 45,
    "Instruction": "Task Description: Create an asynchronous function to fetch GitHub contributor activity data for a specific repository, filtering by time period and excluding certain users.\n\nFunction Description: This function retrieves GitHub events for a repository and processes them to track contributor activity, including created issues, pull requests, and merged commits. It filters events by date and excludes specified users, using GitHub's API with authentication.\n\nInput:\n- owner: str - GitHub repository owner username\n- repo: str - GitHub repository name\n- since: datetime - Minimum datetime for events to include\n- max: int (default=100) - Maximum number of events to fetch\n- excluded_users: set[str] | None (default=None) - Set of usernames to exclude\n- token: str | None (default=None) - GitHub API token (falls back to Marvin settings if None)\n\nOutput:\n- dict[str, dict[str, str | list]] - Dictionary mapping contributor usernames to their activity data, where each value is a dictionary containing:\n  - \"created_issues\": list - Opened issues by the contributor\n  - \"created_pull_requests\": list - Opened PRs by the contributor\n  - \"merged_commits\": list - Merged commits by the contributor (excluding merge commits)",
    "Canonical_solution": "import httpx\nimport marvin\nfrom datetime import datetime\n\nasync def fetch_contributor_data(\n    owner: str,\n    repo: str,\n    since: datetime,\n    max: int = 100,\n    excluded_users: set[str] | None = None,\n    token: str | None = None,\n) -> dict[str, dict[str, str | list]]:\n    if not excluded_users:\n        excluded_users = {}\n\n    if not token:\n        token = marvin.settings.github_token.get_secret_value()\n\n    events_url = f\"https://api.github.com/repos/{owner}/{repo}/events?per_page={max}\"\n\n    contributors_activity = {}\n\n    async with httpx.AsyncClient(\n        headers={\n            \"Accept\": \"application/vnd.github.v3+json\",\n            \"Authorization\": f\"token {token}\",\n        }\n    ) as client:\n        events = (await client.get(events_url)).json()\n\n        for event in events:\n            if (actor := event.get(\"actor\")) and actor[\"login\"] in excluded_users:\n                continue\n            created_at = datetime.fromisoformat(event[\"created_at\"].rstrip(\"Z\"))\n            if created_at < since:\n                continue\n\n            contributor_username = actor[\"login\"] if actor else \"unknown\"\n\n            if contributor_username not in contributors_activity:\n                contributors_activity[contributor_username] = {\n                    \"created_issues\": [],\n                    \"created_pull_requests\": [],\n                    \"merged_commits\": [],\n                }\n\n            if (\n                event[\"type\"] == \"IssuesEvent\"\n                and event[\"payload\"][\"action\"] == \"opened\"\n            ):\n                contributors_activity[contributor_username][\"created_issues\"].append(\n                    event[\"payload\"][\"issue\"]\n                )\n\n            elif (\n                event[\"type\"] == \"PullRequestEvent\"\n                and event[\"payload\"][\"action\"] == \"opened\"\n            ):\n                contributors_activity[contributor_username][\n                    \"created_pull_requests\"\n                ].append(event[\"payload\"][\"pull_request\"])\n\n            elif event[\"type\"] == \"PushEvent\":\n                for commit_data in event[\"payload\"][\"commits\"]:\n                    commit = (await client.get(commit_data[\"url\"])).json()\n                    commit_message = commit[\"commit\"][\"message\"].split(\"\\n\")\n                    cleaned_commit_message = \"\\n\".join(\n                        line\n                        for line in commit_message\n                        if not line.strip().lower().startswith(\"co-authored-by:\")\n                    )\n                    commit_msg = commit[\"commit\"][\"message\"] = cleaned_commit_message\n\n                    if (\n                        \"Merge remote-tracking branch\" not in commit_msg\n                        and \"Merge branch\" not in commit_msg\n                    ):\n                        contributors_activity[contributor_username][\n                            \"merged_commits\"\n                        ].append(commit)\n\n    return contributors_activity",
    "Code_level": "Function-level",
    "FSMID_for_test": 59,
    "wrong_result": "import asyncio\nimport aiohttp\nfrom datetime import datetime\nfrom typing import Optional, Set, Dict, List\n\nasync def fetch_github_contributor_activity(\n    owner: str,\n    repo: str,\n    since: datetime,\n    max: int = 100,\n    excluded_users: Optional[Set[str]] = None,\n    token: Optional[str] = None,\n) -> Dict[str, Dict[str, List[str]]]:\n    if excluded_users is None:\n        excluded_users = set()\n    \n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    if token:\n        headers[\"Authorization\"] = f\"token {token}\"\n    \n    url = f\"https://api.github.com/repos/{owner}/{repo}/events\"\n    params = {\"per_page\": max}\n    \n    activity_data = {}\n    \n    async with aiohttp.ClientSession(headers=headers) as session:\n        async with session.get(url, params=params) as response:\n            if response.status != 200:\n                raise Exception(f\"GitHub API error: {response.status}\")\n            \n            events = await response.json()\n            \n            for event in events:\n                created_at = datetime.strptime(event[\"created_at\"], \"%Y-%m-%dT%H:%M:%SZ\")\n                if created_at < since:\n                    continue\n                \n                actor = event.get(\"actor\", {}).get(\"login\")\n                if not actor or actor in excluded_users:\n                    continue\n                \n                if actor not in activity_data:\n                    activity_data[actor] = {\n                        \"created_issues\": [],\n                        \"created_pull_requests\": [],\n                        \"merged_commits\": []\n                    }\n                \n                event_type = event[\"type\"]\n                payload = event[\"payload\"]\n                \n                if event_type == \"IssuesEvent\" and payload.get(\"action\") == \"opened\":\n                    issue = payload.get(\"issue\", {})\n                    if issue:\n                        activity_data[actor][\"created_issues\"].append(issue.get(\"html_url\", \"\"))\n                \n                elif event_type == \"PullRequestEvent\" and payload.get(\"action\") == \"opened\":\n                    pr = payload.get(\"pull_request\", {})\n                    if pr:\n                        activity_data[actor][\"created_pull_requests\"].append(pr.get(\"html_url\", \"\"))\n                \n                elif event_type == \"PushEvent\":\n                    commits = payload.get(\"commits\", [])\n                    for commit in commits:\n                        if \"merge\" not in commit.get(\"message\", \"\").lower():\n                            activity_data[actor][\"merged_commits\"].append(commit.get(\"url\", \"\"))\n    \n    return activity_data"
  },
  {
    "Task_id": 451,
    "Instruction": "Task Description: Implement a WebSocket server using Tornado in Python to handle real-time communication with clients, including processing video frames and managing participant data.\n\nClass Description: WebSocketHandler is a Tornado WebSocket handler class that manages WebSocket connections, processes incoming messages, and handles the lifecycle of WebSocket connections.\n\nAttributes:\n- None explicitly defined in the class (uses global variables for state management)\n\nMethods:\n- open() -> None - Called when a new WebSocket connection is established. Initializes participant position and calls newParticipant.\n- on_message(message: str) -> None - Handles incoming WebSocket messages. Processes different message types (identified by 'msgID') to manage video frame extraction, data writing to CSV, and frame navigation.\n- on_close() -> None - Called when the WebSocket connection is closed (currently empty implementation).\n- on_error() -> None - Called when an error occurs (not shown in the reference code but typically part of WebSocketHandler).\n\nClass Description: Application is a Tornado web application class that routes WebSocket and static file requests.\n\nAttributes:\n- None explicitly defined (inherits from tornado.web.Application)\n\nMethods:\n- __init__() -> None - Initializes the application with URL handlers for WebSocket connections and static files, and configures template paths.\n\nNote: The reference code also uses several global variables and helper functions (newParticipant, sendVideoEnd, sendVideoFrame, writeDataToCSV, writeScreenCapOutputFrames, closeScreenCapOutVideo) which are not part of the class definitions but are used by the WebSocketHandler methods.",
    "Canonical_solution": "import tornado.websocket\nimport tornado.web\nimport tornado.escape\nimport json\nimport csv\nimport os\nimport glob\nimport numpy as np\nimport subprocess\nimport re\nfrom itertools import chain\n\nclass WebSocketHandler(tornado.websocket.WebSocketHandler):\n    def open(self):\n        global_variables.participantPos = -1\n        newParticipant(self)\n\n    def on_message(self, message):\n        msg = tornado.escape.json_decode(message)\n        \n        if msg['msgID'] == '1':\n            global_variables.participant.videosPos += 1\n            pv = global_variables.participant.videos[global_variables.participant.videosPos]\n            video = global_variables.participant.directory + '/' + pv.filename\n            \n            outDir = outputPrefix + video + \"_frames\" + '/'\n            if not os.path.isdir(outDir):\n                os.makedirs(outDir)\n\n            gpCSVDone = outputPrefix + global_variables.participant.directory + '_' + pv.filename + '_' + csvDoneName\n            gpCSV = outputPrefix + global_variables.participant.directory + '_' + pv.filename + '_' + csvTempName\n            \n            if os.path.isfile(gpCSVDone):\n                sendVideoEnd(self)\n                return\n            elif os.path.isfile(gpCSV):\n                os.remove(gpCSV)\n                if writeCSV:\n                    with open(gpCSV, 'w', newline='') as csvfile:\n                        writer = csv.DictWriter(csvfile, fieldnames=fieldnames, delimiter=',', quoting=csv.QUOTE_ALL)\n                        writer.writeheader()\n\n            framesDoneFile = outDir + '/' + \"framesExtracted.txt\"\n            if not os.path.isfile(framesDoneFile):\n                completedProcess = subprocess.run('ffmpeg -i \"./' + video + '\" -vf showinfo \"' + outDir + 'frame_%08d.png\"',\n                    stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True, shell=True)\n\n                nFrames = len(glob.glob(outDir + '*.png'))\n                if nFrames == 0:\n                    sendVideoEnd(self)\n                    return\n\n                allPts = np.ones(nFrames, dtype=np.int) * -1\n                ptsTimebase = -1\n                framerate = -1\n                lines = completedProcess.stderr.splitlines()\n                for l in lines:\n                    if l.startswith(\"[Parsed_showinfo_0 @\"):\n                        timebase = l.find(\"config in time_base:\")\n                        fr = l.find(\", frame_rate:\")\n                        nStart = l.find(\"n:\")\n                        ptsStart = l.find(\"pts:\")\n                        pts_timeStart = l.find(\"pts_time:\")\n                        if nStart >= 0 and ptsStart >= 0:\n                            frameNum = int(l[nStart+2:ptsStart-1].strip())\n                            pts = int(l[ptsStart+4:pts_timeStart].strip())\n                            allPts[frameNum] = pts\n                        elif timebase >= 0:\n                            ptsTimebase = l[timebase+20:fr].strip()\n                            framerate = l[fr+13:].strip()\n                            sl = framerate.find(\"/\")\n                            if sl > 0:\n                                frPre = framerate[0:sl]\n                                frPost = framerate[sl+1:]\n                                framerate = float(frPre) / float(frPost)\n                            else:\n                                framerate = float(framerate)\n\n                prev = 0\n                for i in range(0, nFrames):\n                    if allPts[i] == -1:\n                        allPts[i] = prev + int(1000/framerate)\n                    prev = allPts[i]\n\n                for i in range(0, nFrames):\n                    inputFile = outDir + frameExtractFormat.format(i+1)\n                    outputFile = outDir + frameOutFormat.format(i, allPts[i])\n                    os.rename(inputFile, outputFile)\n\n                with open(framesDoneFile, 'w') as f:\n                    f.write(\"Done.\")\n\n            pv.frameFilesList = sorted(glob.glob(outDir + '*.png'))\n            pv.frameFilesPos = 0\n            sendVideoFrame(self, pv.frameFilesList[pv.frameFilesPos], pv)\n\n        elif msg['msgID'] == '3':\n            frameTimeEpoch = writeDataToCSV(global_variables.participant, msg)\n\n            if global_variables.writeScreenCapVideo:\n                writeScreenCapOutputFrames(global_variables.participant, frameTimeEpoch)\n\n            pv = global_variables.participant.videos[global_variables.participant.videosPos]\n            pv.frameFilesPos += 1\n\n            if pv.frameFilesPos >= len(pv.frameFilesList):\n                if global_variables.writeScreenCapVideo:\n                    closeScreenCapOutVideo(global_variables.participant)\n\n                gpCSV = outputPrefix + global_variables.participant.directory + '_' + pv.filename + '_' + csvTempName\n                gpCSVDone = outputPrefix + global_variables.participant.directory + '_' + pv.filename + '_' + csvDoneName\n                if os.path.isfile(gpCSV):\n                    os.rename(gpCSV, gpCSVDone)\n\n                sendVideoEnd(self)\n            else:\n                sendVideoFrame(self, pv.frameFilesList[pv.frameFilesPos], pv)\n\n    def on_close(self):\n        pass\n\nclass Application(tornado.web.Application):\n    def __init__(self):\n        handlers = [\n            (r'/websocket', WebSocketHandler),\n            (r'/(.*)', tornado.web.StaticFileHandler, {'path': '.', 'default_filename': ''}),\n        ]\n        settings = {\n            'template_path': 'templates'\n        }\n        tornado.web.Application.__init__(self, handlers, **settings)",
    "Code_level": "Class-level",
    "FSMID_for_test": 50,
    "wrong_result": "import tornado.ioloop\nimport tornado.web\nimport tornado.websocket\nimport json\nimport os\nimport csv\nimport cv2\nimport base64\n\nclass WebSocketHandler(tornado.websocket.WebSocketHandler):\n    def open(self):\n        global participant_position\n        participant_position = 0\n        newParticipant()\n\n    def on_message(self, message):\n        global participant_position, video_capture, video_fps, video_frame_count, video_width, video_height, video_frame_position, screen_cap_out_video, screen_cap_out_frames, screen_cap_out_frames_dir, screen_cap_out_frames_count, data_file, data_file_name, data_file_fieldnames, video_file_name, video_file_path, video_file_dir, video_file_ext, video_file_base_name, video_file_full_path, video_file_full_path_no_ext, video_file_full_path_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext, video_file_full_path_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no_ext_no_dir_no"
  },
  {
    "Task_id": 459,
    "Instruction": "Task Description: Create a function that sets up and runs a WebSocket server in a separate thread, allowing for asynchronous communication between clients and the server.\n\nFunction Description: The function `run_websocket_server` initializes and starts a WebSocket server on a specified port. The server runs in a separate thread to avoid blocking the main program. It handles incoming messages by echoing them back to the client. The function returns the event loop and the thread object for potential external control.\n\nInput:  \n- `port` [int] - The port number on which the WebSocket server will listen for incoming connections.\n\nOutput:  \n- `event_loop` [asyncio.AbstractEventLoop] - The event loop associated with the WebSocket server, which can be used to manage or stop the server.  \n- `thd` [threading.Thread] - The thread object running the WebSocket server, which can be used to join or terminate the thread.",
    "Canonical_solution": "import asyncio\nimport websockets\nfrom threading import Thread\n\ndef run_websocket_server(port):\n    event_loop = asyncio.new_event_loop()\n\n    async def empty_server(websocket, path):\n        data = await websocket.recv()\n        await asyncio.sleep(0.02)\n        await websocket.send(data)\n\n    def run_server():\n        asyncio.set_event_loop(event_loop)\n        server = websockets.serve(empty_server, \"127.0.0.1\", port)\n        event_loop.run_until_complete(server)\n        event_loop.run_forever()\n\n    thd = Thread(target=run_server)\n    thd.start()\n    return event_loop, thd",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport threading\nimport websockets\n\ndef run_websocket_server(port):\n    async def echo(websocket, path):\n        async for message in websocket:\n            await websocket.send(message)\n\n    async def server():\n        async with websockets.serve(echo, \"localhost\", port):\n            await asyncio.Future()\n\n    event_loop = asyncio.new_event_loop()\n    thd = threading.Thread(target=event_loop.run_forever)\n    thd.start()\n    asyncio.run_coroutine_threadsafe(server(), event_loop)\n    return event_loop, thd"
  },
  {
    "Task_id": 46,
    "Instruction": "Task Description: Create an asynchronous HTTP client function that handles SQS records, makes HTTP requests, and processes responses in an AWS Lambda environment.\n\nFunction Description: The function processes SQS records asynchronously, makes HTTP GET requests to a specified endpoint using httpx, and returns the HTTP status code. It's designed to work with AWS Lambda Powertools for batch processing.\n\nInput: \n- record: SQSRecord - A single SQS record containing message data\n- (Implicit) client: httpx.AsyncClient - An asynchronous HTTP client created within the function\n\nOutput: \n- int - HTTP status code from the GET request (e.g., 200, 404, etc.)\n- (Through processor) Processed batch response for the Lambda handler",
    "Canonical_solution": "import httpx\nfrom aws_lambda_powertools.utilities.batch import AsyncBatchProcessor, EventType, async_process_partial_response\nfrom aws_lambda_powertools.utilities.data_classes.sqs_event import SQSRecord\nfrom aws_lambda_powertools.utilities.typing import LambdaContext\n\nprocessor = AsyncBatchProcessor(event_type=EventType.SQS)\n\nasync def async_record_handler(record: SQSRecord):\n    async with httpx.AsyncClient() as client:\n        ret = await client.get(\"https://httpbin.org/get\")\n    return ret.status_code\n\ndef lambda_handler(event, context: LambdaContext):\n    return async_process_partial_response(\n        event=event,\n        record_handler=async_record_handler,\n        processor=processor,\n        context=context,\n    )",
    "Code_level": "Function-level",
    "FSMID_for_test": 59,
    "wrong_result": "import asyncio\nimport json\nimport httpx\nfrom typing import Dict, Any\n\nasync def process_record(record: Dict[str, Any], client: httpx.AsyncClient) -> int:\n    try:\n        message_body = json.loads(record['body'])\n        url = message_body.get('url')\n        if not url:\n            return 400\n        \n        response = await client.get(url)\n        return response.status_code\n    \n    except json.JSONDecodeError:\n        return 400\n    except httpx.RequestError:\n        return 500"
  },
  {
    "Task_id": 461,
    "Instruction": "Task Description: Implement a WebSocket server interface that manages client connections and facilitates message broadcasting in a synchronous manner using asyncio and threading.\n\nClass Description: SyncWebSocketInterface is a class that provides a synchronous interface to an asynchronous WebSocket server. It manages client connections, handles message broadcasting to all connected clients, and runs the asyncio event loop in a separate thread to maintain responsiveness.\n\nAttributes:\n\nclients: [set] - A set of connected WebSocket clients.\nloop: [asyncio.AbstractEventLoop] - The asyncio event loop used for WebSocket operations.\nthread: [threading.Thread] - The thread running the asyncio event loop.\n\nMethods:\n\n__init__: [Name]() -> [None] - Initializes the WebSocket interface, creates a new event loop, and starts it in a separate thread.\n_run_event_loop: [Name]() -> [None] - Internal method that runs the asyncio event loop in the dedicated thread.\n_run_async: [Name](coroutine) -> [None] - Runs a coroutine in the asyncio event loop from another thread.\n_send_to_all_clients: [Name](clients, msg) -> [None] - Internal async method to send a message to all specified clients.\nregister_client: [Name](websocket) -> [None] - Registers a new WebSocket client to the set of connected clients.\nunregister_client: [Name](websocket) -> [None] - Removes a WebSocket client from the set of connected clients.\ninternal_monologue: [Name](msg) -> [None] - Prints and broadcasts an internal monologue message to all clients.\nassistant_message: [Name](msg) -> [None] - Prints and broadcasts an assistant message to all clients.\nfunction_message: [Name](msg) -> [None] - Prints and broadcasts a function message to all clients.\nclose: [Name]() -> [None] - Stops the event loop and joins the thread, cleaning up resources.",
    "Canonical_solution": "import asyncio\nimport threading\nimport letta.server.ws_api.protocol as protocol\n\nclass SyncWebSocketInterface:\n    def __init__(self):\n        self.clients = set()\n        self.loop = asyncio.new_event_loop()\n        self.thread = threading.Thread(target=self._run_event_loop, daemon=True)\n        self.thread.start()\n\n    def _run_event_loop(self):\n        asyncio.set_event_loop(self.loop)\n        try:\n            self.loop.run_forever()\n        finally:\n            self.loop.run_until_complete(self.loop.shutdown_asyncgens())\n            self.loop.close()\n\n    def _run_async(self, coroutine):\n        if not self.loop.is_closed():\n            asyncio.run_coroutine_threadsafe(coroutine, self.loop)\n\n    async def _send_to_all_clients(self, clients, msg):\n        if clients:\n            await asyncio.gather(*(client.send_text(msg) for client in clients))\n\n    def register_client(self, websocket):\n        self.clients.add(websocket)\n\n    def unregister_client(self, websocket):\n        self.clients.remove(websocket)\n\n    def internal_monologue(self, msg):\n        print(msg)\n        if self.clients:\n            self._run_async(self._send_to_all_clients(self.clients, protocol.server_agent_internal_monologue(msg)))\n\n    def assistant_message(self, msg):\n        print(msg)\n        if self.clients:\n            self._run_async(self._send_to_all_clients(self.clients, protocol.server_agent_assistant_message(msg)))\n\n    def function_message(self, msg):\n        print(msg)\n        if self.clients:\n            self._run_async(self._send_to_all_clients(self.clients, protocol.server_agent_function_message(msg)))\n\n    def close(self):\n        self.loop.call_soon_threadsafe(self.loop.stop)\n        self.thread.join()",
    "Code_level": "Class-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport threading\nimport websockets\n\nclass SyncWebSocketInterface:\n    def __init__(self):\n        self.clients = set()\n        self.loop = asyncio.new_event_loop()\n        self.thread = threading.Thread(target=self._run_event_loop, daemon=True)\n        self.thread.start()\n\n    def _run_event_loop(self):\n        asyncio.set_event_loop(self.loop)\n        self.loop.run_forever()\n\n    def _run_async(self, coroutine):\n        asyncio.run_coroutine_threadsafe(coroutine, self.loop)\n\n    async def _send_to_all_clients(self, clients, msg):\n        if clients:\n            await asyncio.gather(*[client.send(msg) for client in clients])\n\n    def register_client(self, websocket):\n        self._run_async(self._register_client(websocket))\n\n    async def _register_client(self, websocket):\n        self.clients.add(websocket)\n\n    def unregister_client(self, websocket):\n        self._run_async(self._unregister_client(websocket))\n\n    async def _unregister_client(self, websocket):\n        self.clients.remove(websocket)\n\n    def internal_monologue(self, msg):\n        print(f\"Internal Monologue: {msg}\")\n        self._run_async(self._send_to_all_clients(self.clients, f\"Internal Monologue: {msg}\"))\n\n    def assistant_message(self, msg):\n        print(f\"Assistant: {msg}\")\n        self._run_async(self._send_to_all_clients(self.clients, f\"Assistant: {msg}\"))\n\n    def function_message(self, msg):\n        print(f\"Function: {msg}\")\n        self._run_async(self._send_to_all_clients(self.clients, f\"Function: {msg}\"))\n\n    def close(self):\n        self.loop.call_soon_threadsafe(self.loop.stop)\n        self.thread.join()"
  },
  {
    "Task_id": 462,
    "Instruction": "Task Description: Implement a WebSocket server using Python's asyncio and websockets libraries to handle client connections, process incoming JSON messages, and send appropriate responses based on the requested function.\n\nFunction Description: The server function handles WebSocket connections, processes incoming JSON messages, and responds based on the specified function in the message. It supports three main operations: echoing back a value, prompting for user input, and throwing an error message. The server runs indefinitely, maintaining persistent connections with clients.\n\nInput: \n- websocket: WebSocket connection object (provided by websockets library)\n- path: str - The path requested by the client (provided by websockets library)\n\nOutput: None (The function runs indefinitely, sending responses back through the WebSocket connection)\n\nThe function processes incoming JSON messages with the following structure:\n{\n    \"function\": str - The operation to perform (\"echo\", \"ask\", or \"throw\"),\n    \"args\": list - Arguments for the specified function\n}\n\nThe function sends back JSON responses with either:\n- A \"return\" field containing the result for successful operations\n- A \"throw\" field containing error messages for failures or unknown functions",
    "Canonical_solution": "import asyncio\nimport websockets\nimport json\n\nasync def server(websocket, path):\n  while True:\n    # Get received data from websocket\n    data = await websocket.recv()\n    print (\"Received: \" + data)\n    json_data = json.loads(data)\n    if json_data.get('function') == 'echo':\n      #echo back the first argument\n      json_data['return'] = json_data['args'][0]\n    elif json_data.get('function') == 'ask':\n      #echo back the first argument\n      json_data['return'] = input(json_data['args'][0])\n    elif json_data.get('function') == 'throw':\n      #throw the first argument\n      json_data['throw'] = json_data['args'][0]\n    elif not json_data.get('function') is None:\n      json_data['throw'] = 'Unknown function : ' + json_data['function']\n\n    # Send response back to client to acknowledge receiving message\n    response = json.dumps(json_data)\n    print (\"Responding: \" + response)\n    await websocket.send(response)\n\ndef start_websocket_server():\n    # Create websocket server\n    start_server = websockets.serve(server, \"localhost\", 5150)\n    # Start and run websocket server forever\n    asyncio.get_event_loop().run_until_complete(start_server)\n    print(\"Starting loop\")\n    asyncio.get_event_loop().run_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport json\nimport websockets\n\nasync def handle_connection(websocket, path):\n    async for message in websocket:\n        try:\n            data = json.loads(message)\n            func = data.get(\"function\")\n            args = data.get(\"args\", [])\n\n            if func == \"echo\":\n                response = {\"return\": args[0] if args else None}\n            elif func == \"ask\":\n                response = {\"return\": input(args[0] if args else \"Enter input: \")}\n            elif func == \"throw\":\n                response = {\"throw\": args[0] if args else \"Error thrown\"}\n            else:\n                response = {\"throw\": \"Unknown function\"}\n\n            await websocket.send(json.dumps(response))\n        except Exception as e:\n            await websocket.send(json.dumps({\"throw\": str(e)}))\n\nasync def main():\n    async with websockets.serve(handle_connection, \"localhost\", 8765):\n        await asyncio.Future()\n\nasyncio.run(main())"
  },
  {
    "Task_id": 465,
    "Instruction": "Task Description: Create a WebSocket server using Python's asyncio and websockets libraries to handle client connections, process incoming JSON messages, and send responses back to clients.\n\nFunction Description: The server function handles WebSocket connections, processes incoming JSON data, performs specified operations (echo, ask, throw), and sends back appropriate responses. The start_websocket_server function initializes and runs the WebSocket server.\n\nInput:\n- websocket: WebSocket connection object (provided by websockets library)\n- path: str - The path requested by the client (provided by websockets library)\n\nOutput:\n- None (The function runs indefinitely, maintaining the WebSocket connection and processing messages)\n\nNote: The server expects JSON messages with the following structure:\n{\n  \"function\": str (one of 'echo', 'ask', 'throw', or other),\n  \"args\": list (arguments for the specified function),\n  \"return\": any (will be populated in response),\n  \"throw\": any (will be populated in case of errors)\n}",
    "Canonical_solution": "import asyncio\nimport websockets\nimport json\n\nasync def server(websocket, path):\n  while True:\n    # Get received data from websocket\n    data = await websocket.recv()\n    print (\"Received: \" + data)\n    json_data = json.loads(data)\n    if json_data.get('function') == 'echo':\n      #echo back the first argument\n      json_data['return'] = json_data['args'][0]\n    elif json_data.get('function') == 'ask':\n      #echo back the first argument\n      json_data['return'] = input(json_data['args'][0])\n    elif json_data.get('function') == 'throw':\n      #throw the first argument\n      json_data['throw'] = json_data['args'][0]\n    elif not json_data.get('function') is None:\n      json_data['throw'] = 'Unknown function : ' + json_data['function']\n\n    # Send response back to client to acknowledge receiving message\n    response = json.dumps(json_data)\n    print (\"Responding: \" + response)\n    await websocket.send(response)\n\ndef start_websocket_server():\n    # Create websocket server\n    start_server = websockets.serve(server, \"localhost\", 5150)\n    # Start and run websocket server forever\n    asyncio.get_event_loop().run_until_complete(start_server)\n    print(\"Starting loop\")\n    asyncio.get_event_loop().run_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport json\nimport websockets\n\nasync def handle_message(websocket, path):\n    async for message in websocket:\n        try:\n            data = json.loads(message)\n            function = data.get('function')\n            args = data.get('args', [])\n            response = {'function': function, 'args': args}\n\n            if function == 'echo':\n                response['return'] = args\n            elif function == 'ask':\n                response['return'] = input(args[0] if args else '') if args else input()\n            elif function == 'throw':\n                raise ValueError(args[0] if args else 'Error')\n            else:\n                response['throw'] = 'Unknown function'\n\n            await websocket.send(json.dumps(response))\n        except Exception as e:\n            error_response = {'throw': str(e)}\n            await websocket.send(json.dumps(error_response))\n\ndef start_websocket_server(host='localhost', port=8765):\n    loop = asyncio.get_event_loop()\n    server = websockets.serve(handle_message, host, port)\n    loop.run_until_complete(server)\n    loop.run_forever()"
  },
  {
    "Task_id": 466,
    "Instruction": "Task Description: Implement a WebSocket server using Python's asyncio and websockets libraries to handle client connections, process incoming JSON messages, and send responses back to clients.\n\nFunction Description: The server function handles WebSocket connections, processes incoming JSON messages, performs specified operations (echo, ask, throw), and sends back appropriate responses. The start_websocket_server function initializes and runs the WebSocket server.\n\nInput:\n- websocket: WebSocket connection object (provided by websockets library)\n- path: str - The path requested by the client (provided by websockets library)\n\nOutput:\n- None (The function runs indefinitely, processing messages and sending responses through the WebSocket connection)\n\nNote: The server expects JSON messages with the following structure:\n{\n  \"function\": \"echo|ask|throw\",  // Operation to perform\n  \"args\": [...]                // Arguments for the operation\n}",
    "Canonical_solution": "import asyncio\nimport websockets\nimport json\n\nasync def server(websocket, path):\n  while True:\n    # Get received data from websocket\n    data = await websocket.recv()\n    print (\"Received: \" + data)\n    json_data = json.loads(data)\n    if json_data.get('function') == 'echo':\n      #echo back the first argument\n      json_data['return'] = json_data['args'][0]\n    elif json_data.get('function') == 'ask':\n      #echo back the first argument\n      json_data['return'] = input(json_data['args'][0])\n    elif json_data.get('function') == 'throw':\n      #throw the first argument\n      json_data['throw'] = json_data['args'][0]\n    elif not json_data.get('function') is None:\n      json_data['throw'] = 'Unknown function : ' + json_data['function']\n\n    # Send response back to client to acknowledge receiving message\n    response = json.dumps(json_data)\n    print (\"Responding: \" + response)\n    await websocket.send(response)\n\ndef start_websocket_server():\n    # Create websocket server\n    start_server = websockets.serve(server, \"localhost\", 5150)\n    # Start and run websocket server forever\n    asyncio.get_event_loop().run_until_complete(start_server)\n    print(\"Starting loop\")\n    asyncio.get_event_loop().run_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport json\nimport websockets\n\nasync def handle_message(websocket, path):\n    async for message in websocket:\n        try:\n            data = json.loads(message)\n            func = data.get('function')\n            args = data.get('args', [])\n\n            if func == 'echo':\n                response = {'result': args}\n            elif func == 'ask':\n                response = {'result': 'This is the answer'}\n            elif func == 'throw':\n                response = {'error': 'Requested to throw error'}\n            else:\n                response = {'error': 'Unknown function'}\n\n            await websocket.send(json.dumps(response))\n        except json.JSONDecodeError:\n            await websocket.send(json.dumps({'error': 'Invalid JSON'}))\n        except Exception as e:\n            await websocket.send(json.dumps({'error': str(e)}))\n\nasync def start_websocket_server(host='localhost', port=8765):\n    async with websockets.serve(handle_message, host, port):\n        await asyncio.Future()"
  },
  {
    "Task_id": 467,
    "Instruction": "Task Description: Create a WebSocket server using Python's asyncio and websockets libraries to handle client connections, process incoming JSON messages, and send responses back to clients based on the requested function.\n\nFunction Description: The server function handles WebSocket connections, processes incoming JSON messages, performs specified operations (echo, ask, or throw), and returns appropriate responses. The start_websocket_server function initializes and starts the WebSocket server.\n\nInput:\n- websocket: WebSocket connection object (provided by websockets library)\n- path: str - The path requested by the client (provided by websockets library)\n\nOutput:\n- None (The function runs indefinitely, maintaining the WebSocket connection and processing messages)\n\nNote: The server expects JSON messages with the following structure:\n{\n  \"function\": str (required, specifies operation: \"echo\", \"ask\", or \"throw\"),\n  \"args\": list (arguments for the specified function)\n}\n\nThe server returns JSON responses with either:\n- A \"return\" field containing the result for successful operations\n- A \"throw\" field containing error messages for failures",
    "Canonical_solution": "import asyncio\nimport websockets\nimport json\n\nasync def server(websocket, path):\n  while True:\n    # Get received data from websocket\n    data = await websocket.recv()\n    print (\"Received: \" + data)\n    json_data = json.loads(data)\n    if json_data.get('function') == 'echo':\n      #echo back the first argument\n      json_data['return'] = json_data['args'][0]\n    elif json_data.get('function') == 'ask':\n      #echo back the first argument\n      json_data['return'] = input(json_data['args'][0])\n    elif json_data.get('function') == 'throw':\n      #throw the first argument\n      json_data['throw'] = json_data['args'][0]\n    elif not json_data.get('function') is None:\n      json_data['throw'] = 'Unknown function : ' + json_data['function']\n\n    # Send response back to client to acknowledge receiving message\n    response = json.dumps(json_data)\n    print (\"Responding: \" + response)\n    await websocket.send(response)\n\ndef start_websocket_server():\n    # Create websocket server\n    start_server = websockets.serve(server, \"localhost\", 5150)\n    # Start and run websocket server forever\n    asyncio.get_event_loop().run_until_complete(start_server)\n    print(\"Starting loop\")\n    asyncio.get_event_loop().run_forever()",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport json\nimport websockets\n\nasync def handle_message(websocket, path):\n    async for message in websocket:\n        try:\n            data = json.loads(message)\n            if not isinstance(data, dict) or 'function' not in data:\n                raise ValueError(\"Invalid message format\")\n\n            function = data['function']\n            args = data.get('args', [])\n\n            if function == 'echo':\n                response = {'return': args}\n            elif function == 'ask':\n                response = {'return': 'This is the answer'}\n            elif function == 'throw':\n                raise ValueError(\"Error as requested\")\n            else:\n                raise ValueError(f\"Unknown function: {function}\")\n\n            await websocket.send(json.dumps(response))\n\n        except Exception as e:\n            error_response = {'throw': str(e)}\n            await websocket.send(json.dumps(error_response))\n\nasync def start_websocket_server(host='localhost', port=8765):\n    async with websockets.serve(handle_message, host, port):\n        await asyncio.Future()"
  },
  {
    "Task_id": 469,
    "Instruction": "Task Description: Implement a WebSocket server using Python's asyncio and websockets libraries to handle bidirectional communication between clients and the server, with support for message production and consumption through multiprocessing queues.\n\nFunction Description: The SocketWorker function establishes a WebSocket server that manages client connections, processes incoming messages, and sends messages to specific clients based on their unique IDs. It uses asynchronous programming to handle multiple clients concurrently.\n\nInput:\n- queueX: multiprocessing.Queue - Queue for outgoing messages to clients (format: (client_id, message))\n- queueZ: multiprocessing.Queue - Queue for incoming messages from clients (format: (client_id, message))\n\nOutput: None (The function runs indefinitely as a server process)\n\nMethods (nested async functions within SocketWorker):\n\n1. producer() -> Union[Tuple[str, str], str] - Generates messages to be sent to clients, either from queueX or periodic ping messages.\n\n2. producer_handler(websocket: WebSocketServerProtocol, path: str) -> None - Handles sending messages to connected clients via the WebSocket connection.\n\n3. consumer(websocket: WebSocketServerProtocol, msg: str) -> None - Processes incoming messages from clients, handling pong responses and forwarding valid requests to queueZ.\n\n4. consumer_handler(websocket: WebSocketServerProtocol, path: str) -> None - Continuously receives messages from a client connection and processes them.\n\n5. server(websocket: WebSocketServerProtocol, path: str) -> None - Main connection handler that manages client lifecycle, assigning unique IDs and coordinating producer/consumer tasks.\n\n6. srv_exception(loop: asyncio.AbstractEventLoop, context: dict) -> None - Exception handler for the WebSocket server event loop.",
    "Canonical_solution": "import asyncio\nimport websockets\nimport signal\nimport json\nimport random\nimport time\nfrom multiprocessing import Queue\n\n_DEBUG_LEVEL_ = 2\nPORT_NUM = 8266\n\ndef SocketWorker(queueX, queueZ):\n    signal.signal(signal.SIGINT, signal.SIG_DFL)\n    USERS = set()\n\n    async def producer():\n        hasData = False\n        try:\n            K, out = queueX.get(timeout=0.05)\n            hasData = True\n        except:\n            pass\n        if hasData:\n            return (K, out)\n        else:\n            await asyncio.sleep(0.001)\n            if random.random() < -0.003:\n                return '[PING]'\n            else:\n                return ''\n\n    async def producer_handler(websocket, path):\n        while True:\n            msg = await producer()\n            if isinstance(msg, tuple):\n                K, msg = msg\n                for x in USERS:\n                    if x.client_id == K:\n                        await x.send(msg)\n                        break\n            elif msg != '':\n                await websocket.send(msg)\n\n    async def consumer(websocket, msg):\n        if msg == '[PONG]':\n            return\n        try:\n            msg = json.loads(msg)\n            if msg['op'].lower() == 'get':\n                queueZ.put((websocket.client_id, msg['txt']))\n        except Exception as e:\n            print(e)\n            pass\n\n    async def consumer_handler(websocket, path):\n        while True:\n            msg = await websocket.recv()\n            await consumer(websocket, msg)\n\n    async def server(websocket, path):\n        websocket.client_id = '%020x' % random.randrange(16**20)\n        USERS.add(websocket)\n        print(\"[ws connect]\", len(USERS), 'users @',\n              time.strftime(\"%Y %b %d %H:%M:%S\", time.localtime(time.time())))\n        try:\n            await websocket.send('id_' + websocket.client_id)\n            consumer_task = asyncio.ensure_future(\n                consumer_handler(websocket, path))\n            producer_task = asyncio.ensure_future(\n                producer_handler(websocket, path))\n            done, pending = await asyncio.wait(\n                [consumer_task, producer_task],\n                return_when=asyncio.FIRST_COMPLETED)\n            for task in pending:\n                task.cancel()\n        finally:\n            USERS.remove(websocket)\n            print(\"[ws disconnect]\", len(USERS))\n\n    def srv_exception(loop, context):\n        if _DEBUG_LEVEL_ > 1:\n            print('exception', loop, context)\n        pass\n\n    try:\n        start_server = websockets.serve(server, \"127.0.0.1\", PORT_NUM)\n        asyncio.get_event_loop().run_until_complete(start_server)\n        asyncio.get_event_loop().set_exception_handler(srv_exception)\n        asyncio.get_event_loop().run_forever()\n    except Exception as e:\n        print('[srv error]', e)",
    "Code_level": "Function-level",
    "FSMID_for_test": 45,
    "wrong_result": "import asyncio\nimport websockets\nimport uuid\nfrom typing import Union, Tuple\n\ndef SocketWorker(queueX, queueZ):\n    clients = {}\n\n    async def producer():\n        try:\n            if not queueX.empty():\n                return queueX.get_nowait()\n        except:\n            pass\n        return \"ping\"\n\n    async def producer_handler(websocket, path):\n        client_id = None\n        for ws, cid in clients.items():\n            if ws == websocket:\n                client_id = cid\n                break\n        if client_id is None:\n            return\n        while True:\n            message = await producer()\n            if isinstance(message, tuple) and message[0] == client_id:\n                await websocket.send(message[1])\n            elif message == \"ping\":\n                await websocket.send(message)\n            await asyncio.sleep(0.1)\n\n    async def consumer(websocket, msg):\n        if msg == \"pong\":\n            return\n        for ws, cid in clients.items():\n            if ws == websocket:\n                queueZ.put((cid, msg))\n                break\n\n    async def consumer_handler(websocket, path):\n        while True:\n            try:\n                msg = await websocket.recv()\n                await consumer(websocket, msg)\n            except websockets.exceptions.ConnectionClosed:\n                break\n\n    async def server(websocket, path):\n        client_id = str(uuid.uuid4())\n        clients[websocket] = client_id\n        try:\n            producer_task = asyncio.create_task(producer_handler(websocket, path))\n            consumer_task = asyncio.create_task(consumer_handler(websocket, path))\n            await asyncio.gather(producer_task, consumer_task)\n        finally:\n            del clients[websocket]\n\n    def srv_exception(loop, context):\n        pass\n\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    loop.set_exception_handler(srv_exception)\n    start_server = websockets.serve(server, \"0.0.0.0\", 8765)\n    loop.run_until_complete(start_server)\n    loop.run_forever()"
  },
  {
    "Task_id": 47,
    "Instruction": "Task Description: Create an asynchronous HTTP client function that can send various types of HTTP requests (GET, POST, PUT, DELETE, PATCH, HEAD, OPTIONS) and return the response.\n\nFunction Description: The function `make_http_request` is an asynchronous HTTP client that sends HTTP requests using the `httpx` library. It supports multiple HTTP methods and can handle both raw content and JSON data for POST requests. The function uses a context manager to ensure proper resource management and returns the HTTP response object.\n\nInput:\n- `url`: str - The target URL for the HTTP request.\n- `method`: str (optional, default=\"GET\") - The HTTP method to use (GET, POST, PUT, DELETE, PATCH, HEAD, OPTIONS).\n- `content`: typing.Optional[bytes] (optional) - Raw bytes content to send in the request body (for POST, PUT, PATCH).\n- `json`: typing.Optional[dict] (optional) - JSON data to send in the request body (for POST).\n- `headers`: typing.Optional[dict] (optional) - Custom headers to include in the request.\n\nOutput:\n- `httpx.Response` - The HTTP response object containing status code, headers, and response body.",
    "Canonical_solution": "import httpx\nimport typing\nfrom datetime import timedelta\n\nasync def make_http_request(url: str, method: str = \"GET\", content: typing.Optional[bytes] = None, json: typing.Optional[dict] = None, headers: typing.Optional[dict] = None) -> httpx.Response:\n    async with httpx.AsyncClient() as client:\n        if method == \"GET\":\n            response = await client.get(url)\n        elif method == \"POST\":\n            if content:\n                response = await client.post(url, content=content)\n            elif json:\n                response = await client.post(url, json=json)\n        elif method == \"PUT\":\n            response = await client.put(url, content=content)\n        elif method == \"DELETE\":\n            response = await client.delete(url)\n        elif method == \"PATCH\":\n            response = await client.patch(url, content=content)\n        elif method == \"HEAD\":\n            response = await client.head(url)\n        elif method == \"OPTIONS\":\n            response = await client.options(url)\n        \n        return response",
    "Code_level": "Function-level",
    "FSMID_for_test": 59,
    "wrong_result": "import httpx\nimport typing\n\nasync def make_http_request(\n    url: str,\n    method: str = \"GET\",\n    content: typing.Optional[bytes] = None,\n    json: typing.Optional[dict] = None,\n    headers: typing.Optional[dict] = None,\n) -> httpx.Response:\n    async with httpx.AsyncClient() as client:\n        request_kwargs = {\"url\": url, \"method\": method}\n        if headers:\n            request_kwargs[\"headers\"] = headers\n        if content:\n            request_kwargs[\"content\"] = content\n        if json:\n            request_kwargs[\"json\"] = json\n        response = await client.request(**request_kwargs)\n        return response"
  },
  {
    "Task_id": 48,
    "Instruction": "Task Description: Implement an asynchronous function to search GitHub issues using the GitHub API, handling authentication and response processing.\n\nFunction Description: The function `search_github_issues` asynchronously searches for GitHub issues based on a query string, with options to specify the repository, number of results, and an optional API token. It handles authentication, sends an HTTP GET request to the GitHub API, processes the response, and returns a formatted summary of the issues found.\n\nInput:\n- `query`: str - The search query string for GitHub issues.\n- `repo`: str (optional, default=\"prefecthq/prefect\") - The repository to search in, formatted as \"owner/repo\".\n- `n`: int (optional, default=3) - The maximum number of issues to return.\n- `api_token`: Optional[str] (optional, default=None) - The GitHub API token for authentication. If not provided, the function attempts to retrieve it from various sources.\n\nOutput:\n- str - A formatted string summarizing the found GitHub issues, including their titles, URLs, and truncated bodies. Returns \"No issues found.\" if no issues match the query.",
    "Canonical_solution": "import os\nimport httpx\nfrom typing import Optional\n\nasync def get_token() -> str:\n    try:\n        from prefect.blocks.system import Secret\n        return (await Secret.load(name=\"github-token\")).get()  # type: ignore\n    except (ImportError, ValueError) as exc:\n        getattr(get_logger(\"marvin\"), \"debug_kv\")(\n            (\n                \"Prefect Secret for GitHub token not retrieved. \"\n                f\"{exc.__class__.__name__}: {exc}\"\n                \"red\"\n            ),\n        )\n\n    try:\n        return getattr(marvin.settings, \"github_token\")\n    except AttributeError:\n        pass\n\n    if token := os.environ.get(\"MARVIN_GITHUB_TOKEN\", \"\"):\n        return token\n\n    raise RuntimeError(\"GitHub token not found\")\n\nasync def search_github_issues(\n    query: str,\n    repo: str = \"prefecthq/prefect\",\n    n: int = 3,\n    api_token: Optional[str] = None,\n) -> str:\n    headers = {\"Accept\": \"application/vnd.github.v3+json\"}\n    headers[\"Authorization\"] = f\"Bearer {api_token or await get_token()}\"\n\n    async with httpx.AsyncClient() as client:\n        response = await client.get(\n            \"https://api.github.com/search/issues\",\n            headers=headers,\n            params={\n                \"q\": query if \"repo:\" in query else f\"repo:{repo} {query}\",\n                \"order\": \"desc\",\n                \"per_page\": n,\n            },\n        )\n        response.raise_for_status()\n\n    issues_data = response.json()[\"items\"]\n\n    for issue in issues_data:\n        if not issue[\"body\"]:\n            continue\n        issue[\"body\"] = slice_tokens(issue[\"body\"], 1000)\n\n    issues = [GitHubIssue(**issue) for issue in issues_data]\n\n    summary = \"\\n\\n\".join(\n        f\"{issue.title} ({issue.html_url}):\\n{issue.body}\" for issue in issues\n    )\n    if not summary.strip():\n        return \"No issues found.\"\n    return summary",
    "Code_level": "Function-level",
    "FSMID_for_test": 59,
    "wrong_result": "import aiohttp\nimport os\nfrom typing import Optional\n\nasync def search_github_issues(\n    query: str,\n    repo: str = \"prefecthq/prefect\",\n    n: int = 3,\n    api_token: Optional[str] = None\n) -> str:\n    if api_token is None:\n        api_token = os.getenv(\"GITHUB_TOKEN\")\n    \n    headers = {}\n    if api_token:\n        headers[\"Authorization\"] = f\"token {api_token}\"\n    \n    url = f\"https://api.github.com/search/issues?q={query}+repo:{repo}+type:issue\"\n    \n    async with aiohttp.ClientSession() as session:\n        async with session.get(url, headers=headers) as response:\n            if response.status != 200:\n                return f\"Error: {response.status} - {await response.text()}\"\n            \n            data = await response.json()\n            issues = data.get(\"items\", [])[:n]\n            \n            if not issues:\n                return \"No issues found.\"\n            \n            result = []\n            for issue in issues:\n                title = issue.get(\"title\", \"No title\")\n                url = issue.get(\"html_url\", \"No URL\")\n                body = issue.get(\"body\", \"No description\")\n                truncated_body = (body[:100] + \"...\") if len(body) > 100 else body\n                result.append(f\"Title: {title}\\nURL: {url}\\nDescription: {truncated_body}\\n\")\n            \n            return \"\\n\".join(result)"
  },
  {
    "Task_id": 481,
    "Instruction": "Task Description: Implement a multi-process data processing system using ZeroMQ for inter-process communication, where worker processes apply a mapping function to incoming data points and return the results.\n\nClass Description: MultiProcessMapDataZMQ is a class that manages multiple worker processes for parallel data processing using ZeroMQ's REQ/REP pattern. It handles process creation, communication, and data serialization.\n\nAttributes:\n- ds: [Any] - The data source to be processed\n- num_proc: [int] - Number of worker processes to create\n- map_func: [callable] - Function to apply to each data point\n- _buffer_size: [int] - High water mark for ZeroMQ sockets\n- _strict: [bool] - Whether to enforce strict processing rules\n- _procs: [list] - List of worker processes\n- context: [zmq.Context] - ZeroMQ context\n- socket: [zmq.Socket] - Main DEALER socket for communication\n- _proc_ids: [list] - List of process identifiers\n\nMethods:\n- __init__(ds, num_proc=None, map_func=None, *, buffer_size=200, strict=False) -> None - Initializes the multi-process mapper with configuration parameters\n- reset_state() -> None - Sets up ZeroMQ context, sockets, and starts worker processes\n- _create_worker(id, pipename, hwm) -> _Worker - Creates a worker process instance\n- _send(dp) -> None - Sends a data point to workers\n- _recv() -> Any - Receives processed data from workers\n\nNested _Worker Class:\n- __init__(identity, map_func, pipename, hwm) -> None - Initializes a worker process\n- run() -> None - Worker's main loop that processes incoming data using REQ/REP pattern",
    "Canonical_solution": "import zmq\nimport multiprocessing as mp\nfrom ..utils.serialize import dumps_once as dumps, loads_once as loads\nfrom ..utils.concurrency import enable_death_signal\n\nclass MultiProcessMapDataZMQ:\n    class _Worker(mp.Process):\n        def __init__(self, identity, map_func, pipename, hwm):\n            super(MultiProcessMapDataZMQ._Worker, self).__init__()\n            self.identity = identity\n            self.map_func = map_func\n            self.pipename = pipename\n            self.hwm = hwm\n\n        def run(self):\n            enable_death_signal(_warn=self.identity == b'0')\n            ctx = zmq.Context()\n            socket = ctx.socket(zmq.REP)\n            socket.setsockopt(zmq.IDENTITY, self.identity)\n            socket.set_hwm(self.hwm)\n            socket.connect(self.pipename)\n\n            while True:\n                dp = loads(socket.recv(copy=False))\n                dp = self.map_func(dp)\n                socket.send(dumps(dp), copy=False)\n\n    def __init__(self, ds, num_proc=None, map_func=None, *, buffer_size=200, strict=False):\n        self.ds = ds\n        self.num_proc = num_proc\n        self.map_func = map_func\n        self._buffer_size = buffer_size\n        self._strict = strict\n        self._procs = []\n\n    def reset_state(self):\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.DEALER)\n        self.socket.set_hwm(self._buffer_size * 2)\n        pipename = _get_pipe_name('dataflow-map')\n        _bind_guard(self.socket, pipename)\n\n        self._proc_ids = [u'{}'.format(k).encode('utf-8') for k in range(self.num_proc)]\n        worker_hwm = int(self._buffer_size * 2 // self.num_proc)\n        self._procs = [self._create_worker(self._proc_ids[k], pipename, worker_hwm)\n                       for k in range(self.num_proc)]\n\n        self._start_processes()\n\n    def _create_worker(self, id, pipename, hwm):\n        return MultiProcessMapDataZMQ._Worker(id, self.map_func, pipename, hwm)\n\n    def _send(self, dp):\n        msg = [b\"\", dumps(dp)]\n        self.socket.send_multipart(msg, copy=False)\n\n    def _recv(self):\n        msg = self.socket.recv_multipart(copy=False)\n        dp = loads(msg[1])\n        return dp",
    "Code_level": "Class-level",
    "FSMID_for_test": 87,
    "wrong_result": "import os\nimport zmq\nimport pickle\nfrom multiprocessing import Process\n\nclass MultiProcessMapDataZMQ:\n    def __init__(self, ds, num_proc=None, map_func=None, *, buffer_size=200, strict=False):\n        self.ds = ds\n        self.num_proc = num_proc\n        self.map_func = map_func\n        self._buffer_size = buffer_size\n        self._strict = strict\n        self._procs = []\n        self.context = None\n        self.socket = None\n        self._proc_ids = []\n        self.reset_state()\n\n    def reset_state(self):\n        if self.context is not None:\n            self.context.destroy()\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.DEALER)\n        self.socket.set_hwm(self._buffer_size)\n        pipename = f\"ipc://tmp_pipe_{os.getpid()}\"\n        self.socket.bind(pipename)\n        self._proc_ids = [f\"worker-{i}\" for i in range(self.num_proc)]\n        self._procs = [self._create_worker(wid, pipename, self._buffer_size) for wid in self._proc_ids]\n        for p in self._procs:\n            p.start()\n\n    def _create_worker(self, id, pipename, hwm):\n        return self._Worker(id, self.map_func, pipename, hwm)\n\n    def _send(self, dp):\n        self.socket.send(pickle.dumps(dp), copy=False)\n\n    def _recv(self):\n        msg = self.socket.recv(copy=False)\n        return pickle.loads(msg.bytes)\n\n    class _Worker(Process):\n        def __init__(self, identity, map_func, pipename, hwm):\n            super().__init__()\n            self.identity = identity\n            self.map_func = map_func\n            self.pipename = pipename\n            self.hwm = hwm\n\n        def run(self):\n            context = zmq.Context()\n            socket = context.socket(zmq.REP)\n            socket.set_hwm(self.hwm)\n            socket.connect(self.pipename)\n            while True:\n                try:\n                    msg = socket.recv(copy=False)\n                    dp = pickle.loads(msg.bytes)\n                    res = self.map_func(dp)\n                    socket.send(pickle.dumps(res), copy=False)\n                except (zmq.ZMQError, KeyboardInterrupt, SystemExit):\n                    break\n            socket.close()\n            context.term()"
  },
  {
    "Task_id": 488,
    "Instruction": "Task Description: Implement a gRPC server class in Python that handles multiple types of RPC calls (Call, Train, Index, Search, StreamCall) and manages concurrent requests using a thread pool. The server should integrate with a ZeroMQ client for message passing and provide proper context management for resource cleanup.\n\nClass Description: FrontendService is a gRPC server implementation that processes incoming gRPC requests and forwards them to a backend service via ZeroMQ. It handles concurrent requests, manages server lifecycle, and provides streaming capabilities.\n\nAttributes:\n\n- server: [grpc.Server] - The gRPC server instance handling incoming requests\n- bind_address: [str] - The address and port where the server listens (format \"host:port\")\n- _stop_event: [threading.Event] - Event object used to signal server shutdown\n- args: [Namespace/object] - Configuration arguments for server setup (max_concurrency, max_message_size, etc.)\n\nMethods:\n\n- __init__(args) -> [None] - Initializes the gRPC server with thread pool executor and configures message size limits\n  Input: args - Configuration object containing server parameters\n- __enter__() -> [FrontendService] - Starts the gRPC server and clears stop event (context manager entry)\n- __exit__(exc_type, exc_val, exc_tb) -> [None] - Stops the gRPC server and triggers cleanup (context manager exit)\n  Input: exc_type, exc_val, exc_tb - Exception information if any occurred\n- stop() -> [None] - Signals the server to stop by setting the stop event\n- join() -> [None] - Waits for the server to complete shutdown\n- _Servicer: [Nested class] - Handles actual gRPC method implementations\n  Methods:\n    - Call(request, context) -> [gnes_pb2.Response] - Processes standard RPC calls\n      Input: request - gRPC request message, context - gRPC context object\n      Output: Processed response message\n    - Train(request, context) -> [gnes_pb2.Response] - Handles training requests\n    - Index(request, context) -> [gnes_pb2.Response] - Handles indexing requests\n    - Search(request, context) -> [gnes_pb2.Response] - Handles search requests\n    - StreamCall(request_iterator, context) -> [Iterator[gnes_pb2.Response]] - Handles streaming RPC calls\n      Input: request_iterator - Iterator of incoming streaming requests\n      Output: Iterator of response messages\n    - ZmqContext: [Nested class] - Manages ZeroMQ client lifecycle\n      Methods:\n        - __init__(args) -> [None] - Initializes thread-local storage for ZMQ client\n        - __enter__() -> [ZmqClient] - Creates and returns a ZMQ client instance\n        - __exit__(exc_type, exc_value, exc_traceback) -> [None] - Cleans up ZMQ client resources",
    "Canonical_solution": "import grpc\nfrom concurrent.futures import ThreadPoolExecutor\nfrom ..proto import gnes_pb2_grpc, gnes_pb2\nfrom ..client.base import ZmqClient\nimport threading\n\nclass FrontendService:\n    def __init__(self, args):\n        self.server = grpc.server(\n            ThreadPoolExecutor(max_workers=args.max_concurrency),\n            options=[('grpc.max_send_message_length', args.max_message_size),\n                     ('grpc.max_receive_message_length', args.max_message_size)])\n        gnes_pb2_grpc.add_GnesRPCServicer_to_server(self._Servicer(args), self.server)\n        self.bind_address = '{0}:{1}'.format(args.grpc_host, args.grpc_port)\n        self.server.add_insecure_port(self.bind_address)\n        self._stop_event = threading.Event()\n\n    def __enter__(self):\n        self.server.start()\n        self._stop_event.clear()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.server.stop(None)\n        self.stop()\n\n    def stop(self):\n        self._stop_event.set()\n\n    def join(self):\n        self._stop_event.wait()\n\n    class _Servicer(gnes_pb2_grpc.GnesRPCServicer):\n        def __init__(self, args):\n            self.args = args\n            self.zmq_context = self.ZmqContext(args)\n            self.request_id_cnt = 0\n            self.send_recv_kwargs = dict(\n                check_version=self.args.check_version,\n                timeout=self.args.timeout,\n                squeeze_pb=self.args.squeeze_pb)\n            self.pending_request = 0\n\n        def Call(self, request, context):\n            with self.zmq_context as zmq_client:\n                zmq_client.send_message(self.add_envelope(request, zmq_client), **self.send_recv_kwargs)\n                return self.remove_envelope(zmq_client.recv_message(**self.send_recv_kwargs))\n\n        def Train(self, request, context):\n            return self.Call(request, context)\n\n        def Index(self, request, context):\n            return self.Call(request, context)\n\n        def Search(self, request, context):\n            return self.Call(request, context)\n\n        def StreamCall(self, request_iterator, context):\n            self.pending_request = 0\n\n            def get_response(num_recv, blocked=False):\n                for _ in range(num_recv):\n                    if blocked or zmq_client.receiver.poll(1):\n                        msg = zmq_client.recv_message(**self.send_recv_kwargs)\n                        self.pending_request -= 1\n                        yield self.remove_envelope(msg)\n\n                while zmq_client.receiver.poll(1):\n                    msg = zmq_client.recv_message(**self.send_recv_kwargs)\n                    self.pending_request -= 1\n                    yield self.remove_envelope(msg)\n\n            with self.zmq_context as zmq_client:\n                for request in request_iterator:\n                    num_recv = max(self.pending_request - self.args.max_pending_request, 1)\n                    yield from get_response(num_recv, num_recv > 1)\n                    zmq_client.send_message(self.add_envelope(request, zmq_client), **self.send_recv_kwargs)\n                    self.pending_request += 1\n\n                yield from get_response(self.pending_request, blocked=True)\n\n        class ZmqContext:\n            def __init__(self, args):\n                self.args = args\n                self.tlocal = threading.local()\n                self.tlocal.client = None\n\n            def __enter__(self):\n                client = ZmqClient(self.args)\n                self.tlocal.client = client\n                return client\n\n            def __exit__(self, exc_type, exc_value, exc_traceback):\n                self.tlocal.client.close()\n                self.tlocal.client = None",
    "Code_level": "Class-level",
    "FSMID_for_test": 74,
    "wrong_result": "import threading\nimport grpc\nfrom concurrent import futures\nimport zmq\nfrom zmq.utils.monitor import recv_monitor_message\nfrom functools import partial\nfrom contextlib import contextmanager\n\nclass FrontendService:\n    def __init__(self, args):\n        self.bind_address = args.bind_address\n        self._stop_event = threading.Event()\n        self.args = args\n        self.server = grpc.server(\n            futures.ThreadPoolExecutor(max_workers=args.max_concurrency),\n            options=[\n                ('grpc.max_send_message_length', args.max_message_size),\n                ('grpc.max_receive_message_length', args.max_message_size),\n            ])\n\n    def __enter__(self):\n        self._stop_event.clear()\n        return self\n\n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.stop()\n        self.join()\n\n    def stop(self):\n        self._stop_event.set()\n\n    def join(self):\n        self.server.stop(None)\n\n    class _Servicer:\n        def __init__(self, args):\n            self.args = args\n\n        def Call(self, request, context):\n            with self.ZmqContext(self.args) as zmq_client:\n                return zmq_client.send_message(request)\n\n        def Train(self, request, context):\n            with self.ZmqContext(self.args) as zmq_client:\n                return zmq_client.send_message(request)\n\n        def Index(self, request, context):\n            with self.ZmqContext(self.args) as zmq_client:\n                return zmq_client.send_message(request)\n\n        def Search(self, request, context):\n            with self.ZmqContext(self.args) as zmq_client:\n                return zmq_client.send_message(request)\n\n        def StreamCall(self, request_iterator, context):\n            with self.ZmqContext(self.args) as zmq_client:\n                for request in request_iterator:\n                    yield zmq_client.send_message(request)\n\n        class ZmqContext:\n            def __init__(self, args):\n                self.args = args\n                self._local = threading.local()\n\n            def __enter__(self):\n                if not hasattr(self._local, 'zmq_client'):\n                    self._local.zmq_client = ZmqClient(self.args)\n                return self._local.zmq_client\n\n            def __exit__(self, exc_type, exc_value, exc_traceback):\n                if hasattr(self._local, 'zmq_client'):\n                    self._local.zmq_client.close()\n                    del self._local.zmq_client\n\nclass ZmqClient:\n    def __init__(self, args):\n        self.context = zmq.Context()\n        self.socket = self.context.socket(zmq.REQ)\n        self.socket.connect(args.zmq_addr)\n\n    def send_message(self, msg):\n        self.socket.send(msg.SerializeToString())\n        return self.socket.recv()\n\n    def close(self):\n        self.socket.close()\n        self.context.term()"
  },
  {
    "Task_id": 495,
    "Instruction": "Task Description: Create a function to set up and tear down a gRPC server with configurable interceptors and thread pool workers.\n\nFunction Description: The function `_set_up` initializes and starts a gRPC server with optional interceptors, while `_tear_down` gracefully stops the server. The server is configured to handle gRPC requests concurrently using a thread pool.\n\nInput:\n- `interceptors` (Optional[List[grpc.ServerInterceptor]]): A list of gRPC server interceptors for request/response processing. Defaults to None.\n- `server` (grpc.Server): The gRPC server instance to be stopped (for `_tear_down`).\n\nOutput:\n- `_set_up` returns the initialized and started `grpc.Server` instance.\n- `_tear_down` has no return value (stops the server).",
    "Canonical_solution": "import grpc\nfrom concurrent import futures\nfrom typing import List, Optional\n\ndef _set_up(interceptors: Optional[List[grpc.ServerInterceptor]] = None):\n    server = grpc.server(\n        futures.ThreadPoolExecutor(max_workers=2),\n        interceptors=interceptors,\n    )\n    add_gRPCTestServiceServicer_to_server(TestService(), server)\n    server.add_insecure_port(\"[::]:{}\".format(PORT))\n    server.start()\n    return server\n\ndef _tear_down(server: grpc.Server):\n    server.stop(None)",
    "Code_level": "Function-level",
    "FSMID_for_test": 74,
    "wrong_result": "import grpc\nfrom concurrent import futures\nfrom typing import List, Optional\n\ndef _set_up(interceptors: Optional[List[grpc.ServerInterceptor]] = None) -> grpc.Server:\n    server = grpc.server(\n        futures.ThreadPoolExecutor(max_workers=10),\n        interceptors=interceptors if interceptors else []\n    )\n    server.start()\n    return server\n\ndef _tear_down(server: grpc.Server) -> None:\n    server.stop(0)"
  },
  {
    "Task_id": 498,
    "Instruction": "Task Description: Implement a gRPC server for Optuna storage proxy service that handles study and trial management operations.\n\nClass Description: OptunaStorageProxyService is a gRPC servicer class that implements the StorageService interface, providing methods to manage Optuna studies and trials through a gRPC server. It acts as a proxy between gRPC clients and the underlying Optuna storage backend.\n\nAttributes:\n- _backend: BaseStorage - The underlying Optuna storage implementation\n- _lock: threading.Lock - A lock for thread-safe operations\n\nMethods:\n- CreateNewStudy(request: api_pb2.CreateNewStudyRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewStudyReply - Creates a new study with given directions and name\n- DeleteStudy(request: api_pb2.DeleteStudyRequest, context: grpc.ServicerContext) -> api_pb2.DeleteStudyReply - Deletes a study with the given ID\n- SetStudyUserAttribute(request: api_pb2.SetStudyUserAttributeRequest, context: grpc.ServicerContext) -> api_pb2.SetStudyUserAttributeReply - Sets a user attribute for a study\n- GetStudyIdFromName(request: api_pb2.GetStudyIdFromNameRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyIdFromNameReply - Gets study ID from study name\n- GetStudyNameFromId(request: api_pb2.GetStudyNameFromIdRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyNameFromIdReply - Gets study name from study ID\n- GetAllStudies(request: api_pb2.GetAllStudiesRequest, context: grpc.ServicerContext) -> api_pb2.GetAllStudiesReply - Gets all studies in the storage\n- CreateNewTrial(request: api_pb2.CreateNewTrialRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewTrialReply - Creates a new trial in a study\n- SetTrialParameter(request: api_pb2.SetTrialParameterRequest, context: grpc.ServicerContext) -> api_pb2.SetTrialParameterReply - Sets parameters for a trial\n- GetTrial(request: api_pb2.GetTrialRequest, context: grpc.ServicerContext) -> api_pb2.GetTrialReply - Gets trial details\n- GetAllTrials(request: api_pb2.GetAllTrialsRequest, context: grpc.ServicerContext) -> api_pb2.GetAllTrialsReply - Gets all trials for a study with optional state filtering",
    "Canonical_solution": "from concurrent.futures import ThreadPoolExecutor\nimport grpc\nfrom optuna.storages._grpc.grpc_imports import api_pb2_grpc\nfrom optuna.storages._grpc.grpc_imports import api_pb2\nfrom optuna.storages._grpc.grpc_imports import StorageServiceServicer\nfrom optuna.storages import BaseStorage\nfrom optuna._experimental import experimental_func\nimport logging\n\n_logger = logging.get_logger(__name__)\n\nclass OptunaStorageProxyService(StorageServiceServicer):\n    def __init__(self, storage: BaseStorage) -> None:\n        self._backend = storage\n        self._lock = threading.Lock()\n\n    def CreateNewStudy(self, request: api_pb2.CreateNewStudyRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewStudyReply:\n        directions = [\n            StudyDirection.MINIMIZE if d == api_pb2.MINIMIZE else StudyDirection.MAXIMIZE\n            for d in request.directions\n        ]\n        study_name = request.study_name\n\n        try:\n            study_id = self._backend.create_new_study(directions=directions, study_name=study_name)\n        except DuplicatedStudyError as e:\n            context.abort(code=grpc.StatusCode.ALREADY_EXISTS, details=str(e))\n        return api_pb2.CreateNewStudyReply(study_id=study_id)\n\n    def DeleteStudy(self, request: api_pb2.DeleteStudyRequest, context: grpc.ServicerContext) -> api_pb2.DeleteStudyReply:\n        study_id = request.study_id\n        try:\n            self._backend.delete_study(study_id)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.DeleteStudyReply()\n\n    def SetStudyUserAttribute(self, request: api_pb2.SetStudyUserAttributeRequest, context: grpc.ServicerContext) -> api_pb2.SetStudyUserAttributeReply:\n        try:\n            self._backend.set_study_user_attr(\n                request.study_id, request.key, json.loads(request.value)\n            )\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.SetStudyUserAttributeReply()\n\n    def GetStudyIdFromName(self, request: api_pb2.GetStudyIdFromNameRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyIdFromNameReply:\n        try:\n            study_id = self._backend.get_study_id_from_name(request.study_name)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.GetStudyIdFromNameReply(study_id=study_id)\n\n    def GetStudyNameFromId(self, request: api_pb2.GetStudyNameFromIdRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyNameFromIdReply:\n        study_id = request.study_id\n        try:\n            name = self._backend.get_study_name_from_id(study_id)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        assert name is not None\n        return api_pb2.GetStudyNameFromIdReply(study_name=name)\n\n    def GetAllStudies(self, request: api_pb2.GetAllStudiesRequest, context: grpc.ServicerContext) -> api_pb2.GetAllStudiesReply:\n        studies = self._backend.get_all_studies()\n        return api_pb2.GetAllStudiesReply(\n            studies=[\n                api_pb2.Study(\n                    study_id=study._study_id,\n                    study_name=study.study_name,\n                    directions=[\n                        api_pb2.MINIMIZE if d == StudyDirection.MINIMIZE else api_pb2.MAXIMIZE\n                        for d in study.directions\n                    ],\n                    user_attributes={\n                        key: json.dumps(value) for key, value in study.user_attrs.items()\n                    },\n                    system_attributes={\n                        key: json.dumps(value) for key, value in study.system_attrs.items()\n                    },\n                )\n                for study in studies\n            ]\n        )\n\n    def CreateNewTrial(self, request: api_pb2.CreateNewTrialRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewTrialReply:\n        study_id = request.study_id\n        template_trial = None\n        if not request.template_trial_is_none:\n            template_trial = _from_proto_trial(request.template_trial)\n\n        try:\n            trial_id = self._backend.create_new_trial(study_id, template_trial)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.CreateNewTrialReply(trial_id=trial_id)\n\n    def SetTrialParameter(self, request: api_pb2.SetTrialParameterRequest, context: grpc.ServicerContext) -> api_pb2.SetTrialParameterReply:\n        trial_id = request.trial_id\n        param_name = request.param_name\n        param_value_internal = request.param_value_internal\n        distribution = json_to_distribution(request.distribution)\n        try:\n            self._backend.set_trial_param(trial_id, param_name, param_value_internal, distribution)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        except RuntimeError as e:\n            context.abort(code=grpc.StatusCode.FAILED_PRECONDITION, details=str(e))\n        except ValueError as e:\n            context.abort(code=grpc.StatusCode.INVALID_ARGUMENT, details=str(e))\n        return api_pb2.SetTrialParameterReply()\n\n    def GetTrial(self, request: api_pb2.GetTrialRequest, context: grpc.ServicerContext) -> api_pb2.GetTrialReply:\n        trial_id = request.trial_id\n        try:\n            trial = self._backend.get_trial(trial_id)\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.GetTrialReply(trial=_to_proto_trial(trial))\n\n    def GetAllTrials(self, request: api_pb2.GetAllTrialsRequest, context: grpc.ServicerContext) -> api_pb2.GetAllTrialsReply:\n        study_id = request.study_id\n        states = [_from_proto_trial_state(state) for state in request.states]\n        try:\n            trials = self._backend.get_all_trials(\n                study_id,\n                deepcopy=False,\n                states=states,\n            )\n        except KeyError as e:\n            context.abort(code=grpc.StatusCode.NOT_FOUND, details=str(e))\n        return api_pb2.GetAllTrialsReply(trials=[_to_proto_trial(trial) for trial in trials])\n\n@experimental_func(\"4.2.0\")\ndef run_grpc_proxy_server(\n    storage: BaseStorage,\n    *,\n    host: str = \"localhost\",\n    port: int = 13000,\n    thread_pool: ThreadPoolExecutor | None = None,\n) -> None:\n    server = grpc.server(thread_pool or ThreadPoolExecutor(max_workers=10))\n    api_pb2_grpc.add_StorageServiceServicer_to_server(\n        OptunaStorageProxyService(storage), server\n    )\n    server.add_insecure_port(f\"{host}:{port}\")\n    server.start()\n    _logger.info(f\"Server started at {host}:{port}\")\n    _logger.info(\"Listening...\")\n    server.wait_for_termination()",
    "Code_level": "Class-level",
    "FSMID_for_test": 74,
    "wrong_result": "import threading\nimport grpc\nfrom optuna.storages import BaseStorage\nfrom optuna_dashboard.protobuf import api_pb2\nfrom optuna_dashboard.protobuf import api_pb2_grpc\n\nclass OptunaStorageProxyService(api_pb2_grpc.StorageServiceServicer):\n    def __init__(self, backend: BaseStorage) -> None:\n        self._backend = backend\n        self._lock = threading.Lock()\n\n    def CreateNewStudy(self, request: api_pb2.CreateNewStudyRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewStudyReply:\n        with self._lock:\n            study_id = self._backend.create_new_study(request.study_name, directions=request.directions)\n            return api_pb2.CreateNewStudyReply(study_id=study_id)\n\n    def DeleteStudy(self, request: api_pb2.DeleteStudyRequest, context: grpc.ServicerContext) -> api_pb2.DeleteStudyReply:\n        with self._lock:\n            self._backend.delete_study(request.study_id)\n            return api_pb2.DeleteStudyReply()\n\n    def SetStudyUserAttribute(self, request: api_pb2.SetStudyUserAttributeRequest, context: grpc.ServicerContext) -> api_pb2.SetStudyUserAttributeReply:\n        with self._lock:\n            self._backend.set_study_user_attr(request.study_id, request.key, request.value)\n            return api_pb2.SetStudyUserAttributeReply()\n\n    def GetStudyIdFromName(self, request: api_pb2.GetStudyIdFromNameRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyIdFromNameReply:\n        with self._lock:\n            study_id = self._backend.get_study_id_from_name(request.study_name)\n            return api_pb2.GetStudyIdFromNameReply(study_id=study_id)\n\n    def GetStudyNameFromId(self, request: api_pb2.GetStudyNameFromIdRequest, context: grpc.ServicerContext) -> api_pb2.GetStudyNameFromIdReply:\n        with self._lock:\n            study_name = self._backend.get_study_name_from_id(request.study_id)\n            return api_pb2.GetStudyNameFromIdReply(study_name=study_name)\n\n    def GetAllStudies(self, request: api_pb2.GetAllStudiesRequest, context: grpc.ServicerContext) -> api_pb2.GetAllStudiesReply:\n        with self._lock:\n            studies = self._backend.get_all_studies()\n            return api_pb2.GetAllStudiesReply(studies=studies)\n\n    def CreateNewTrial(self, request: api_pb2.CreateNewTrialRequest, context: grpc.ServicerContext) -> api_pb2.CreateNewTrialReply:\n        with self._lock:\n            trial_id = self._backend.create_new_trial(request.study_id)\n            return api_pb2.CreateNewTrialReply(trial_id=trial_id)\n\n    def SetTrialParameter(self, request: api_pb2.SetTrialParameterRequest, context: grpc.ServicerContext) -> api_pb2.SetTrialParameterReply:\n        with self._lock:\n            self._backend.set_trial_param(request.trial_id, request.param_name, request.param_value, request.distribution)\n            return api_pb2.SetTrialParameterReply()\n\n    def GetTrial(self, request: api_pb2.GetTrialRequest, context: grpc.ServicerContext) -> api_pb2.GetTrialReply:\n        with self._lock:\n            trial = self._backend.get_trial(request.trial_id)\n            return api_pb2.GetTrialReply(trial=trial)\n\n    def GetAllTrials(self, request: api_pb2.GetAllTrialsRequest, context: grpc.ServicerContext) -> api_pb2.GetAllTrialsReply:\n        with self._lock:\n            trials = self._backend.get_all_trials(request.study_id, states=request.states)\n            return api_pb2.GetAllTrialsReply(trials=trials)"
  },
  {
    "Task_id": 5,
    "Instruction": "Task Description: Create a Python function that interacts with the Pithos music player via D-Bus to retrieve and format the currently playing song's artist and title.\n\nFunction Description: The function connects to the user's D-Bus session bus, accesses the Pithos music player service, retrieves the current song information, and returns it in a formatted string combining the artist and title.\n\nInput: None\n\nOutput: str - A string in the format \"Artist - Title\" representing the currently playing song in Pithos, or None if the operation fails.",
    "Canonical_solution": "import dbus\n\ndef get_pithos_current_song():\n    pithos_bus = dbus.SessionBus()\n    pithos = pithos_bus.get_object(\"net.kevinmehall.Pithos\", \"/net/kevinmehall/Pithos\")\n    props = pithos.get_dbus_method(\"GetCurrentSong\", \"net.kevinmehall.Pithos\")\n    return props()[\"artist\"] + \" - \" + props()[\"title\"]",
    "Code_level": "Function-level",
    "FSMID_for_test": 75,
    "wrong_result": "import dbus\n\ndef get_current_song():\n    try:\n        session_bus = dbus.SessionBus()\n        pithos = session_bus.get_object('net.kevinmehall.Pithos', '/net/kevinmehall/Pithos')\n        iface = dbus.Interface(pithos, 'org.freedesktop.DBus.Properties')\n        props = iface.GetAll('net.kevinmehall.Pithos')\n        artist = props['Artist']\n        title = props['Title']\n        return f\"{artist} - {title}\"\n    except Exception:\n        return None"
  },
  {
    "Task_id": 55,
    "Instruction": "Task Description: Create an asynchronous function to check if the ngrok API is online and wait for it to become available if it's not.\n\nFunction Description: The function continuously attempts to connect to the ngrok API at a specified URL (constructed from environment variables) until it receives a successful response. It uses exponential backoff (fixed interval retries) between attempts and logs the connection status. The function is designed to be used in scenarios where you need to ensure the ngrok API is available before proceeding with operations that depend on it.\n\nInput: \n- None (uses environment variables NGROK_HOST and NGROK_PORT to construct the API URL)\n  \nOutput: \n- Returns True when the ngrok API is successfully reached\n- Does not return until a successful connection is established (infinite retry loop)",
    "Canonical_solution": "import asyncio\nimport httpx\nimport logging\n\nlogger = logging.getLogger(__name__)\n\nasync def wait_for_ngrok_api():\n    NGROK_API_URL = f\"http://{os.getenv('NGROK_HOST', 'ngrok')}:{os.getenv('NGROK_PORT', 4040)}\"\n    RETRY_INTERVAL = 15\n\n    while True:\n        try:\n            async with httpx.AsyncClient() as client:\n                response = await client.get(f\"{NGROK_API_URL}/api/tunnels\")\n                response.raise_for_status()\n                logger.debug(\"ngrok API is online.\")\n                return True\n        except httpx.RequestError:\n            logger.debug(\"ngrok API is offline. Waiting...\")\n            await asyncio.sleep(RETRY_INTERVAL)",
    "Code_level": "Function-level",
    "FSMID_for_test": 59,
    "wrong_result": "import os\nimport asyncio\nimport aiohttp\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def wait_for_ngrok():\n    host = os.getenv('NGROK_HOST', 'localhost')\n    port = os.getenv('NGROK_PORT', '4040')\n    api_url = f\"http://{host}:{port}/api/tunnels\"\n    backoff = 1\n    \n    while True:\n        try:\n            async with aiohttp.ClientSession() as session:\n                async with session.get(api_url) as response:\n                    if response.status == 200:\n                        logger.info(\"Successfully connected to ngrok API\")\n                        return True\n                    else:\n                        logger.warning(f\"Ngrok API returned status {response.status}\")\n        except Exception as e:\n            logger.warning(f\"Failed to connect to ngrok API: {str(e)}\")\n        \n        logger.info(f\"Retrying in {backoff} seconds...\")\n        await asyncio.sleep(backoff)\n        backoff = min(backoff * 2, 60)"
  },
  {
    "Task_id": 58,
    "Instruction": "Task Description: Create a Python class that interacts with the Play.ht API to synthesize text into speech and return the audio as an AudioSegment object.\n\nClass Description: The PlayHtSynthesizer class provides functionality to convert text to speech using the Play.ht API. It handles API authentication, request construction, and response processing to return audio in a usable format.\n\nAttributes:\n- voice: str - The voice identifier to be used for speech synthesis\n- sample_rate: int - The audio sample rate (default: 24000)\n- speed: Optional[float] - The playback speed of the synthesized speech\n- preset: Optional[str] - The voice preset to use\n- api_key: Optional[str] - The API key for Play.ht authentication\n- user_id: Optional[str] - The user ID for Play.ht authentication\n- tts_endpoint: str - The API endpoint for text-to-speech synthesis (default: \"https://play.ht/api/v2/tts/stream\")\n\nMethods:\n- __init__(voice: str, sample_rate: int = 24000, speed: Optional[float] = None, preset: Optional[str] = None, api_key: Optional[str] = None, user_id: Optional[str] = None) -> None - Initializes the synthesizer with configuration parameters\n- synthesize(text: str) -> AudioSegment - Converts the input text to speech using the Play.ht API and returns the audio as an AudioSegment object",
    "Canonical_solution": "import io\nfrom typing import Optional\nimport requests\nfrom pydub import AudioSegment\n\nclass PlayHtSynthesizer:\n    def __init__(\n        self,\n        voice: str,\n        sample_rate: int = 24000,\n        speed: Optional[float] = None,\n        preset: Optional[str] = None,\n        api_key: Optional[str] = None,\n        user_id: Optional[str] = None,\n    ):\n        self.voice = voice\n        self.sample_rate = sample_rate\n        self.speed = speed\n        self.preset = preset\n        self.api_key = api_key\n        self.user_id = user_id\n        self.tts_endpoint = \"https://play.ht/api/v2/tts/stream\"\n\n    def synthesize(self, text: str) -> AudioSegment:\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"X-User-ID\": self.user_id,\n            \"Accept\": \"audio/mpeg\",\n            \"Content-Type\": \"application/json\",\n        }\n        body = {\n            \"voice\": self.voice,\n            \"text\": text,\n            \"sample_rate\": self.sample_rate,\n        }\n        if self.speed is not None:\n            body[\"speed\"] = self.speed\n        if self.preset is not None:\n            body[\"preset\"] = self.preset\n\n        response = requests.post(self.tts_endpoint, headers=headers, json=body, timeout=5)\n        if not response.ok:\n            raise Exception(f\"Play.ht API error: {response.status_code}, {response.text}\")\n\n        return AudioSegment.from_mp3(io.BytesIO(response.content))",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom pydub import AudioSegment\nimport io\nimport json\nimport os\n\nclass PlayHtSynthesizer:\n    def __init__(self, voice, sample_rate=24000, speed=None, preset=None, api_key=None, user_id=None):\n        self.voice = voice\n        self.sample_rate = sample_rate\n        self.speed = speed\n        self.preset = preset\n        self.api_key = api_key or os.getenv('PLAY_HT_API_KEY')\n        self.user_id = user_id or os.getenv('PLAY_HT_USER_ID')\n        self.tts_endpoint = \"https://play.ht/api/v2/tts/stream\"\n\n    def synthesize(self, text):\n        headers = {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"X-User-ID\": self.user_id,\n            \"Content-Type\": \"application/json\"\n        }\n\n        payload = {\n            \"text\": text,\n            \"voice\": self.voice,\n            \"sample_rate\": self.sample_rate\n        }\n\n        if self.speed is not None:\n            payload[\"speed\"] = self.speed\n        if self.preset is not None:\n            payload[\"preset\"] = self.preset\n\n        response = requests.post(self.tts_endpoint, headers=headers, json=payload, stream=True)\n        response.raise_for_status()\n\n        audio_data = io.BytesIO()\n        for chunk in response.iter_content(chunk_size=1024):\n            audio_data.write(chunk)\n\n        audio_data.seek(0)\n        return AudioSegment.from_mp3(audio_data)"
  },
  {
    "Task_id": 59,
    "Instruction": "Task Description: Create a Python class that interacts with a REST API, handling both synchronous and asynchronous task creation, with proper error handling and API availability checking.\n\nClass Description: The `Api` class provides an interface to interact with a REST API, allowing for synchronous and asynchronous task creation. It includes methods to verify API availability, create tasks, and handle API responses and errors.\n\nAttributes:\n- _api_url: str - The base URL of the API endpoint, defaults to \"http://127.0.0.1:8000\" if not provided\n- _create_response_files: bool - Flag indicating whether to create response files (unused in current implementation)\n\nMethods:\n- __init__(api_url: Optional[str] = None, create_response_files: bool = True) -> None - Initializes the API client with optional custom URL and response file setting\n- is_api_running() -> bool - Checks if the API is available by making a GET request to the base endpoint\n- create_async_task(data, scraper_name=None) -> dict - Creates an asynchronous task by sending a POST request with the provided data\n- create_sync_task(data, scraper_name=None) -> dict - Creates a synchronous task by sending a POST request with the provided data\n- _make_api_url(path) -> str - Helper method to construct full API URLs by combining base URL with endpoint path\n- _raise_for_status(response) -> None - Internal method to handle HTTP response status codes and raise appropriate exceptions",
    "Canonical_solution": "import requests\nfrom requests.exceptions import ConnectionError\nfrom typing import Optional\n\nclass ApiException(Exception):\n    pass\n\nclass Api:\n    def __init__(self, api_url: Optional[str] = None, create_response_files: bool = True) -> None:\n        DEFAULT_API_URL = \"http://127.0.0.1:8000\"\n        self._api_url = api_url if api_url else DEFAULT_API_URL\n        self._create_response_files = create_response_files\n        if not self.is_api_running():\n            raise ApiException(f\"API at {self._api_url} is not running. Please check if the API is up and running.\")\n\n    def _make_api_url(self, path):\n        return f\"{self._api_url}/{path}\"\n\n    def is_api_running(self) -> bool:\n        try:\n            response = requests.get(self._make_api_url(\"api\"))\n            return response.status_code == 200\n        except ConnectionError:\n            raise ApiException(f\"API at {self._api_url} is not running. Check the network connection or verify if the API is running on a different endpoint.\")\n\n    def create_async_task(self, data, scraper_name=None):\n        url = self._make_api_url(\"api/tasks/create-task-async\")\n        payload = {\n            \"data\": data,\n            \"scraper_name\": scraper_name,\n        }\n        response = requests.post(url, json=payload)\n        self._raise_for_status(response)\n        return response.json()\n\n    def create_sync_task(self, data, scraper_name=None):\n        url = self._make_api_url(\"api/tasks/create-task-sync\")\n        payload = {\n            \"data\": data,\n            \"scraper_name\": scraper_name,\n        }\n        response = requests.post(url, json=payload)\n        self._raise_for_status(response)\n        return response.json()\n\n    def _raise_for_status(self, response):\n        if 400 <= response.status_code < 500:\n            data = response.json()\n            message = data.get(\"message\")\n            if message:\n                raise ApiException(message)\n            else:\n                response.raise_for_status()    \n        elif 500 <= response.status_code < 600:\n            response.raise_for_status()",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom typing import Optional, Dict, Any\n\nclass Api:\n    def __init__(self, api_url: Optional[str] = None, create_response_files: bool = True) -> None:\n        self._api_url = api_url if api_url is not None else \"http://127.0.0.1:8000\"\n        self._create_response_files = create_response_files\n\n    def is_api_running(self) -> bool:\n        try:\n            response = requests.get(self._api_url)\n            self._raise_for_status(response)\n            return True\n        except requests.exceptions.RequestException:\n            return False\n\n    def create_async_task(self, data: Dict[str, Any], scraper_name: Optional[str] = None) -> Dict[str, Any]:\n        url = self._make_api_url(\"/async_task\")\n        if scraper_name:\n            url = f\"{url}?scraper_name={scraper_name}\"\n        response = requests.post(url, json=data)\n        self._raise_for_status(response)\n        return response.json()\n\n    def create_sync_task(self, data: Dict[str, Any], scraper_name: Optional[str] = None) -> Dict[str, Any]:\n        url = self._make_api_url(\"/sync_task\")\n        if scraper_name:\n            url = f\"{url}?scraper_name={scraper_name}\"\n        response = requests.post(url, json=data)\n        self._raise_for_status(response)\n        return response.json()\n\n    def _make_api_url(self, path: str) -> str:\n        return f\"{self._api_url}{path}\"\n\n    def _raise_for_status(self, response: requests.Response) -> None:\n        try:\n            response.raise_for_status()\n        except requests.exceptions.HTTPError as http_err:\n            error_msg = f\"HTTP error occurred: {http_err}\"\n            if response.text:\n                error_msg += f\" - Response: {response.text}\"\n            raise requests.exceptions.HTTPError(error_msg) from http_err\n        except requests.exceptions.RequestException as req_err:\n            raise requests.exceptions.RequestException(f\"Request failed: {req_err}\") from req_err"
  },
  {
    "Task_id": 60,
    "Instruction": "Task Description: Create a Python module for handling HTTP client operations including login, check-in, and logout functionalities with support for retries and optional proxy usage.\n\nFunction Description: The module provides three main functions for HTTP interactions:\n1. `login`: Performs a POST request to authenticate with a service\n2. `checkin`: Performs a POST request to check in/register with a service\n3. `logout`: Performs a GET request to terminate a session\n\nInput:\n- For `login`:\n  - url: str - The endpoint URL for login\n  - params: dict - The login credentials/data to send\n  - headers: dict - HTTP headers for the request\n  - retry: int - Number of retry attempts\n  - proxy: bool - Whether to use proxy (default False)\n\n- For `checkin`:\n  - url: str - The endpoint URL for check-in\n  - headers: dict - HTTP headers for the request\n  - retry: int - Number of retry attempts\n  - proxy: bool - Whether to use proxy (default False)\n\n- For `logout`:\n  - url: str - The endpoint URL for logout\n  - headers: dict - HTTP headers for the request\n\nOutput:\n- For `login`: dict - Returns response headers (lowercased keys) if successful, empty dict otherwise\n- For `checkin`: None - Logs success/failure messages\n- For `logout`: int - Returns 0 on success, -3 on failure",
    "Canonical_solution": "import logging\nimport requests\nimport simplejson as json\nfrom requests.exceptions import RequestException\nfrom simplejson.decoder import JSONDecodeError\n\nHEADER = {\n    \"user-agent\": \"Mozilla/5.0 (Linux; Android 6.0; Nexus 5 Build/MRA58N) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/76.0.3803.0 Mobile Safari/537.36\",\n    \"accept\": \"application/json, text/javascript, */*; q=0.01\",\n    \"accept-language\": \"zh-CN,zh;q=0.9\",\n    \"dnt\": \"1\",\n    \"Connection\": \"keep-alive\",\n    \"content-type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n    \"x-requested-with\": \"XMLHttpRequest\",\n}\n\nPROXY = {\"http\": \"http://127.0.0.1:1080\", \"https\": \"http://127.0.0.1:1080\"}\n\ndef login(url, params, headers, retry, proxy=False) -> dict:\n    try:\n        if proxy:\n            response = requests.post(\n                url,\n                data=params,\n                headers=headers,\n                allow_redirects=True,\n                proxies=PROXY,\n                verify=False,\n            )\n        else:\n            response = requests.post(url, data=params, headers=headers, allow_redirects=True)\n\n        if response.status_code == 200:\n            return {str(key).lower(): value for key, value in response.headers.items()}\n        return {}\n\n    except RequestException as e:\n        logging.error(str(e))\n        retry -= 1\n\n        if retry > 0:\n            time.sleep(get_randint(30 * 60, 90 * 60))\n            return login(url, params, headers, retry, proxy)\n\n        logging.error(\"\u767b\u5f55\u5931\u8d25 URL: {}\".format(extract_domain(url)))\n        return {}\n\ndef checkin(url, headers, retry, proxy=False) -> None:\n    try:\n        response = (\n            requests.post(url, headers=headers, proxies=PROXY, verify=False)\n            if proxy\n            else requests.post(url, headers=headers)\n        )\n\n        if response.status_code == 200:\n            key = \"Content-Encoding\"\n            try:\n                data = (\n                    json.loads(brotli.decompress(response.content).decode(\"utf-8\"))\n                    if key in response.headers and response.headers[\"Content-Encoding\"] == \"br\"\n                    else response.json()\n                )\n\n                logging.info(\"\u7b7e\u5230\u6210\u529f URL: {} {}\".format(extract_domain(url), data[\"msg\"]))\n            except JSONDecodeError:\n                logging.error(\"\u7b7e\u5230\u5931\u8d25 URL: {}\".format(extract_domain(url)))\n\n    except RequestException as e:\n        logging.error(str(e))\n        retry -= 1\n\n        if retry > 0:\n            time.sleep(get_randint(30, 60 * 60))\n            return checkin(url, headers, retry, proxy)\n\n        logging.error(\"\u7b7e\u5230\u5931\u8d25 URL: {}\".format(extract_domain(url)))\n\ndef logout(url, headers) -> int:\n    try:\n        response = requests.get(url, headers=headers)\n        if response.status_code == 200:\n            return 0\n        else:\n            logging.info(\"\u9000\u51fa\u5931\u8d25 URL: {}\".format(extract_domain(url)))\n            return -3\n    except RequestException:\n        return -3",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport time\n\nclass HTTPClient:\n    def __init__(self):\n        self.session = requests.Session()\n        self.proxies = None\n\n    def _make_request(self, method, url, params=None, headers=None, retry=3, proxy=False):\n        if proxy:\n            self.proxies = {\n                'http': 'http://proxy.example.com:8080',\n                'https': 'http://proxy.example.com:8080'\n            }\n        else:\n            self.proxies = None\n\n        for attempt in range(retry):\n            try:\n                if method == 'POST':\n                    response = self.session.post(url, data=params, headers=headers, proxies=self.proxies)\n                else:\n                    response = self.session.get(url, headers=headers, proxies=self.proxies)\n                \n                if response.status_code == 200:\n                    return response\n                else:\n                    raise requests.exceptions.RequestException(f\"Status code: {response.status_code}\")\n            except requests.exceptions.RequestException as e:\n                if attempt == retry - 1:\n                    raise\n                time.sleep(1)\n        return None\n\n    def login(self, url, params, headers, retry=3, proxy=False):\n        try:\n            response = self._make_request('POST', url, params=params, headers=headers, retry=retry, proxy=proxy)\n            if response:\n                return {k.lower(): v for k, v in response.headers.items()}\n            return {}\n        except requests.exceptions.RequestException:\n            return {}\n\n    def checkin(self, url, headers, retry=3, proxy=False):\n        try:\n            self._make_request('POST', url, headers=headers, retry=retry, proxy=proxy)\n            print(\"Check-in successful\")\n        except requests.exceptions.RequestException:\n            print(\"Check-in failed\")\n\n    def logout(self, url, headers):\n        try:\n            self._make_request('GET', url, headers=headers)\n            return 0\n        except requests.exceptions.RequestException:\n            return -3"
  },
  {
    "Task_id": 61,
    "Instruction": "Task Description: Implement a set of functions for interacting with an OCR (Optical Character Recognition) service through HTTP requests, including uploading files for OCR processing, making OCR requests, and retrieving results.\n\nFunction Description: The functions provide a client-side interface to communicate with an OCR service API, handling file uploads, processing requests, and result retrieval.\n\nInput:\n- ocr_upload():\n  - file_path: str - Path to the file to be processed\n  - ocr_cache: bool - Whether to cache the OCR results\n  - prompt: str - Optional prompt text for the OCR processing\n  - prompt_file: str - Optional path to a file containing the prompt\n  - model: str - Model to use for OCR (default: 'llama3.1')\n  - strategy: str - Processing strategy (default: 'llama_vision')\n  - storage_profile: str - Storage profile to use (default: 'default')\n  - storage_filename: str - Optional custom filename for storage\n  - language: str - Language for OCR processing (default: 'en')\n\n- ocr_request():\n  - file_path: str - Path to the file to be processed\n  - ocr_cache: bool - Whether to cache the OCR results\n  - prompt: str - Optional prompt text for the OCR processing\n  - prompt_file: str - Optional path to a file containing the prompt\n  - model: str - Model to use for OCR (default: 'llama3.1')\n  - strategy: str - Processing strategy (default: 'llama_vision')\n  - storage_profile: str - Storage profile to use (default: 'default')\n  - storage_filename: str - Optional custom filename for storage\n  - language: str - Language for OCR processing (default: 'en')\n\n- get_result():\n  - task_id: str - The task ID returned from upload/request\n  - print_progress: bool - Whether to print progress updates (default: False)\n\nOutput:\n- ocr_upload() and ocr_request():\n  - Returns: dict - Either contains 'task_id' for async processing or 'text' for immediate results\n  - Returns None on failure\n\n- get_result():\n  - Returns: dict - The final OCR result when state is 'SUCCESS'\n  - Returns None if task failed or encountered errors",
    "Canonical_solution": "import os\nimport requests\nimport base64\n\ndef ocr_upload(file_path, ocr_cache, prompt, prompt_file=None, model='llama3.1', strategy='llama_vision', storage_profile='default', storage_filename=None, language='en'):\n    ocr_url = os.getenv('OCR_UPLOAD_URL', 'http://localhost:8000/ocr/upload')\n    files = {'file': open(file_path, 'rb')}\n    data = {'ocr_cache': ocr_cache, 'model': model, 'strategy': strategy, 'storage_profile': storage_profile, 'language': language}\n\n    if storage_filename:\n        data['storage_filename'] = storage_filename\n    \n    try:\n        if prompt_file:\n            prompt = open(prompt_file, 'r').read()\n    except FileNotFoundError:\n        print(f\"Prompt file not found: {prompt_file}\")\n        return None\n    \n    if prompt:\n        data['prompt'] = prompt\n\n    response = requests.post(ocr_url, files=files, data=data)\n    if response.status_code == 200:\n        respObject = response.json()\n        if respObject.get('task_id'):\n            return {\n                \"task_id\": respObject.get('task_id')\n            }\n        else:\n            return {\n                \"text\": respObject.get('text')\n            }\n    else:\n        print(f\"Failed to upload file: {response.text}\")\n        return None\n\ndef ocr_request(file_path, ocr_cache, prompt, prompt_file=None, model='llama3.1', strategy='llama_vision', storage_profile='default', storage_filename=None, language='en'):\n    ocr_url = os.getenv('OCR_REQUEST_URL', 'http://localhost:8000/ocr/request')\n    with open(file_path, 'rb') as f:\n        file_content = base64.b64encode(f.read()).decode('utf-8')\n    \n    data = {\n        'ocr_cache': ocr_cache,\n        'model': model,\n        'strategy': strategy,\n        'storage_profile': storage_profile,\n        'file': file_content,\n        'language': language\n    }\n\n    if storage_filename:\n        data['storage_filename'] = storage_filename\n    \n    if prompt_file:\n        try:\n            prompt = open(prompt_file, 'r').read()\n        except FileNotFoundError:\n            print(f\"Prompt file not found: {prompt_file}\")\n            return None\n    \n    if prompt:\n        data['prompt'] = prompt\n    \n    response = requests.post(ocr_url, json=data)\n    if response.status_code == 200:\n        respObject = response.json()\n        if respObject.get('task_id'):\n            return {\n                \"task_id\": respObject.get('task_id')\n            }\n        else:\n            return {\n                \"text\": respObject.get('text')\n            }\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n\ndef get_result(task_id, print_progress = False):\n    extracted_text_printed_once = False\n    result_url = os.getenv('RESULT_URL', f'http://localhost:8000/ocr/result/')\n    while True:\n        response = requests.get(result_url + task_id)\n        result = response.json()\n        if result['state'] != 'SUCCESS' and print_progress:\n            task_info = result.get('info')\n            if task_info is not None:\n                if task_info.get('extracted_text'):\n                    if not extracted_text_printed_once:\n                        extracted_text_printed_once = True\n                        print(\"Extracted text: \" + task_info.get('extracted_text'))\n                    else:\n                        del task_info['extracted_text']\n                del task_info['start_time']\n            print(result)\n        if response.status_code == 200:\n            if result['state'] == 'SUCCESS':\n                return result['result']\n            elif result['state'] == 'FAILURE':\n                print(\"OCR task failed.\")\n                return None\n        time.sleep(2)",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport os\n\ndef ocr_upload(file_path, ocr_cache=False, prompt=None, prompt_file=None, model='llama3.1', strategy='llama_vision', storage_profile='default', storage_filename=None, language='en'):\n    try:\n        url = \"https://api.ocr-service.com/upload\"\n        files = {'file': open(file_path, 'rb')}\n        data = {\n            'ocr_cache': ocr_cache,\n            'model': model,\n            'strategy': strategy,\n            'storage_profile': storage_profile,\n            'language': language\n        }\n        if prompt:\n            data['prompt'] = prompt\n        if prompt_file:\n            with open(prompt_file, 'r') as pf:\n                data['prompt'] = pf.read()\n        if storage_filename:\n            data['storage_filename'] = storage_filename\n        \n        response = requests.post(url, files=files, data=data)\n        response.raise_for_status()\n        return response.json()\n    except Exception as e:\n        return None\n\ndef ocr_request(file_path, ocr_cache=False, prompt=None, prompt_file=None, model='llama3.1', strategy='llama_vision', storage_profile='default', storage_filename=None, language='en'):\n    try:\n        url = \"https://api.ocr-service.com/request\"\n        files = {'file': open(file_path, 'rb')}\n        data = {\n            'ocr_cache': ocr_cache,\n            'model': model,\n            'strategy': strategy,\n            'storage_profile': storage_profile,\n            'language': language\n        }\n        if prompt:\n            data['prompt'] = prompt\n        if prompt_file:\n            with open(prompt_file, 'r') as pf:\n                data['prompt'] = pf.read()\n        if storage_filename:\n            data['storage_filename'] = storage_filename\n        \n        response = requests.post(url, files=files, data=data)\n        response.raise_for_status()\n        return response.json()\n    except Exception as e:\n        return None\n\ndef get_result(task_id, print_progress=False):\n    try:\n        url = f\"https://api.ocr-service.com/result/{task_id}\"\n        response = requests.get(url)\n        response.raise_for_status()\n        result = response.json()\n        \n        if print_progress:\n            print(f\"Task {task_id} status: {result.get('state')}\")\n        \n        if result.get('state') == 'SUCCESS':\n            return result\n        return None\n    except Exception as e:\n        return None"
  },
  {
    "Task_id": 62,
    "Instruction": "Task Description: Create a function to download a file from a given URL and save it to a specified local path.\n\nFunction Description: The function uses the Python requests library to make an HTTP GET request to download a file from a provided URL. It checks the response status code to ensure the request was successful, creates necessary directories for the local path if they don't exist, and saves the file content to the specified location.\n\nInput:\n- url: str - The URL of the file to download\n- local_path: str - The local file path where the downloaded content should be saved\n\nOutput:\n- None - The function doesn't return any value but prints status messages indicating success or failure of the download operation",
    "Canonical_solution": "import os\nimport requests\nfrom urllib.parse import urljoin\n\ndef download_file(url, local_path):\n    response = requests.get(url)\n    if response.status_code == 200:\n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        with open(local_path, 'wb') as f:\n            f.write(response.content)\n        print(f\"Downloaded: {local_path}\")\n    else:\n        print(response.status_code)\n        print(f\"Failed to download: {url}\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport os\n\ndef download_file(url, local_path):\n    try:\n        response = requests.get(url, stream=True)\n        response.raise_for_status()\n        \n        os.makedirs(os.path.dirname(local_path), exist_ok=True)\n        \n        with open(local_path, 'wb') as f:\n            for chunk in response.iter_content(chunk_size=8192):\n                if chunk:\n                    f.write(chunk)\n        print(f\"File downloaded successfully to {local_path}\")\n    except requests.exceptions.RequestException as e:\n        print(f\"Failed to download file: {e}\")\n    except Exception as e:\n        print(f\"An error occurred: {e}\")"
  },
  {
    "Task_id": 63,
    "Instruction": "Task Description: Create a function to download a file from a given URL and save it to a specified local folder.\n\nFunction Description: The function downloads a file from the provided URL using HTTP GET request and saves it to the specified folder. It handles folder creation if it doesn't exist and provides feedback on the download status.\n\nInput:\n- url: str - The URL of the file to download\n- folder_path: str - The local directory path where the file should be saved\n\nOutput:\n- None - The function doesn't return any value but saves the file to disk and prints status messages",
    "Canonical_solution": "import os\nimport requests\n\ndef download_file(url: str, folder_path: str) -> None:\n    # Create the folder if it doesn't exist\n    if not os.path.exists(folder_path):\n        os.makedirs(folder_path)\n\n    # Extract filename from URL\n    filename = url.split(\"/\")[-1]\n\n    # Define path to save the file\n    file_path = os.path.join(folder_path, filename)\n\n    # Send a GET request to the URL\n    response = requests.get(url)\n\n    # Check if request was successful (status code 200)\n    if response.status_code == 200:\n        # Save the file to the specified folder\n        with open(file_path, \"wb\") as f:\n            f.write(response.content)\n        print(f\"File downloaded successfully to {file_path}\")\n    else:\n        print(f\"Failed to download file. Status code: {response.status_code}\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport requests\n\ndef download_file(url, folder_path):\n    try:\n        if not os.path.exists(folder_path):\n            os.makedirs(folder_path)\n        \n        local_filename = url.split('/')[-1]\n        file_path = os.path.join(folder_path, local_filename)\n        \n        with requests.get(url, stream=True) as r:\n            r.raise_for_status()\n            with open(file_path, 'wb') as f:\n                for chunk in r.iter_content(chunk_size=8192): \n                    f.write(chunk)\n        print(f\"File downloaded successfully to {file_path}\")\n    except Exception as e:\n        print(f\"Error downloading file: {e}\")"
  },
  {
    "Task_id": 64,
    "Instruction": "Task Description: Create a Python class that serves as an HTTP client for interacting with Kibana and Elasticsearch APIs, handling authentication, requests, and responses for various operations.\n\nClass Description: The KibanaHTTPClient class provides methods to interact with Kibana's saved objects and Elasticsearch's templates through HTTP requests. It handles authentication, SSL verification, and response parsing.\n\nAttributes:\n- username: str - The username for authentication (default: \"redelk\")\n- password: str - The password for authentication (default: None)\n- verify_ssl: bool - Flag to enable/disable SSL verification (default: False)\n- scheme: str - The URL scheme (default: \"https\")\n- kibana_url: str - The base URL for Kibana API (default: \"https://localhost:5601\")\n- es_url: str - The base URL for Elasticsearch API (default: \"https://localhost:9200\")\n- headers: dict - HTTP headers including \"kbn-xsrf\" for Kibana requests\n\nMethods:\n- fetch_kibana_object(obj_type: str) -> list - Fetches Kibana saved objects of specified type by making a POST request to Kibana's export API. Returns parsed NDJSON response.\n- fetch_es_template(template_name: str) -> dict - Retrieves an Elasticsearch template by name via GET request. Returns parsed JSON response.\n- fetch_es_component_templates() -> dict - Gets all Elasticsearch component templates via GET request. Returns parsed JSON response.\n- fetch_es_index_templates() -> dict - Retrieves all Elasticsearch index templates via GET request. Returns parsed JSON response.",
    "Canonical_solution": "import requests\nimport json\nimport ndjson\nimport os\nimport re\n\nclass KibanaHTTPClient:\n    def __init__(self, username=\"redelk\", password=None, verify_ssl=False):\n        self.username = username\n        self.password = password\n        self.verify_ssl = verify_ssl\n        self.scheme = \"https\"\n        self.kibana_url = f\"{self.scheme}://localhost:5601\"\n        self.es_url = f\"{self.scheme}://localhost:9200\"\n        self.headers = {\"kbn-xsrf\": \"true\"}\n\n    def fetch_kibana_object(self, obj_type):\n        try:\n            response = requests.post(\n                f\"{self.kibana_url}/api/saved_objects/_export\",\n                json={\"type\": obj_type},\n                verify=self.verify_ssl,\n                auth=(self.username, self.password),\n                headers=self.headers,\n            )\n            if response.status_code != 200:\n                raise Exception(f\"HTTP status code {response.status_code}\")\n            return ndjson.loads(response.text.encode(\"utf-8\"))\n        except Exception as error:\n            raise Exception(f\"Error fetching kibana object {obj_type}: {error}\")\n\n    def fetch_es_template(self, template_name):\n        try:\n            response = requests.get(\n                f\"{self.es_url}/_template/{template_name}\",\n                verify=self.verify_ssl,\n                auth=(self.username, self.password),\n            )\n            if response.status_code != 200:\n                raise Exception(f\"HTTP status code {response.status_code}\")\n            return json.loads(response.text.encode(\"utf-8\"))\n        except Exception as error:\n            raise Exception(f\"Error fetching ES template {template_name}: {error}\")\n\n    def fetch_es_component_templates(self):\n        try:\n            response = requests.get(\n                f\"{self.es_url}/_component_template\",\n                verify=self.verify_ssl,\n                auth=(self.username, self.password),\n            )\n            if response.status_code != 200:\n                raise Exception(f\"HTTP status code {response.status_code}\")\n            return response.json()\n        except Exception as error:\n            raise Exception(f\"Error fetching ES component templates: {error}\")\n\n    def fetch_es_index_templates(self):\n        try:\n            response = requests.get(\n                f\"{self.es_url}/_index_template\",\n                verify=self.verify_ssl,\n                auth=(self.username, self.password),\n            )\n            if response.status_code != 200:\n                raise Exception(f\"HTTP status code {response.status_code}\")\n            return response.json()\n        except Exception as error:\n            raise Exception(f\"Error fetching ES index templates: {error}\")",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport json\nimport ndjson\n\nclass KibanaHTTPClient:\n    def __init__(self, username=\"redelk\", password=None, verify_ssl=False, scheme=\"https\", kibana_url=\"https://localhost:5601\", es_url=\"https://localhost:9200\"):\n        self.username = username\n        self.password = password\n        self.verify_ssl = verify_ssl\n        self.scheme = scheme\n        self.kibana_url = kibana_url\n        self.es_url = es_url\n        self.headers = {\n            \"kbn-xsrf\": \"true\",\n            \"Content-Type\": \"application/json\"\n        }\n\n    def _make_kibana_request(self, method, endpoint, data=None):\n        url = f\"{self.kibana_url}{endpoint}\"\n        auth = (self.username, self.password) if self.password else None\n        response = requests.request(\n            method,\n            url,\n            headers=self.headers,\n            json=data,\n            auth=auth,\n            verify=self.verify_ssl\n        )\n        response.raise_for_status()\n        return response\n\n    def _make_es_request(self, method, endpoint, data=None):\n        url = f\"{self.es_url}{endpoint}\"\n        auth = (self.username, self.password) if self.password else None\n        response = requests.request(\n            method,\n            url,\n            headers=self.headers,\n            json=data,\n            auth=auth,\n            verify=self.verify_ssl\n        )\n        response.raise_for_status()\n        return response\n\n    def fetch_kibana_object(self, obj_type):\n        endpoint = \"/api/saved_objects/_export\"\n        data = {\n            \"type\": obj_type,\n            \"includeReferencesDeep\": True\n        }\n        response = self._make_kibana_request(\"POST\", endpoint, data)\n        return ndjson.loads(response.text)\n\n    def fetch_es_template(self, template_name):\n        endpoint = f\"/_template/{template_name}\"\n        response = self._make_es_request(\"GET\", endpoint)\n        return response.json()\n\n    def fetch_es_component_templates(self):\n        endpoint = \"/_component_template\"\n        response = self._make_es_request(\"GET\", endpoint)\n        return response.json()\n\n    def fetch_es_index_templates(self):\n        endpoint = \"/_index_template\"\n        response = self._make_es_request(\"GET\", endpoint)\n        return response.json()"
  },
  {
    "Task_id": 66,
    "Instruction": "Task Description: Retrieve information about a specific GitHub project using the GitHub GraphQL API.\n\nFunction Description: This function queries the GitHub GraphQL API to fetch project details (id, title, and number) for a given organization and project title. It authenticates using a GitHub token and returns the project information if found.\n\nInput:\n- github_token: [str] - GitHub personal access token for authentication\n- github_org: [str] - GitHub organization name\n- github_project: [str] - Title of the project to search for\n\nOutput:\n- [dict/None] - Returns a dictionary containing the project's id, title, and number if found, otherwise None. Raises an HTTPError if the request fails.",
    "Canonical_solution": "import requests\nimport os\nimport sys\nfrom datetime import datetime, timedelta\n\n\nGITHUB_GRAPHQL_URL = \"https://api.github.com/graphql\"\n\n\ndef get_github_project_info(github_token, github_org, github_project):\n    headers = {\n        \"Authorization\": f\"Bearer {github_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    query = '''\n    {\n      organization(login: \"%s\") {\n        projectsV2(first: 20) {\n          nodes {\n            id\n            title\n            number\n          }\n        }\n      }\n    }\n    ''' % (github_org)\n    payload = {\n        \"query\": query\n    }\n\n    response = requests.post(GITHUB_GRAPHQL_URL, headers=headers, json=payload)\n    if response.status_code == 200:\n        # fine project by title\n        nodes = response.json().get(\"data\").get(\"organization\").get(\"projectsV2\").get(\"nodes\")\n        for node in nodes:\n            if node.get(\"title\") == github_project:\n                return node\n    else:\n        response.raise_for_status()",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\n\ndef get_github_project(github_token, github_org, github_project):\n    query = \"\"\"\n    query($org: String!, $projectTitle: String!) {\n        organization(login: $org) {\n            projectsV2(first: 1, query: $projectTitle) {\n                nodes {\n                    id\n                    title\n                    number\n                }\n            }\n        }\n    }\n    \"\"\"\n    variables = {\n        \"org\": github_org,\n        \"projectTitle\": github_project\n    }\n    headers = {\n        \"Authorization\": f\"Bearer {github_token}\",\n        \"Content-Type\": \"application/json\"\n    }\n    response = requests.post(\n        \"https://api.github.com/graphql\",\n        json={\"query\": query, \"variables\": variables},\n        headers=headers\n    )\n    response.raise_for_status()\n    data = response.json()\n    projects = data.get(\"data\", {}).get(\"organization\", {}).get(\"projectsV2\", {}).get(\"nodes\", [])\n    if projects:\n        return {\n            \"id\": projects[0][\"id\"],\n            \"title\": projects[0][\"title\"],\n            \"number\": projects[0][\"number\"]\n        }\n    return None"
  },
  {
    "Task_id": 67,
    "Instruction": "Task Description: Create a function to download financial market data from a specific API endpoint, handling authentication, progress tracking, and error checking during the download process.\n\nFunction Description: The function downloads compressed CSV data from a REST API endpoint for a given exchange, data type, symbol, and date. It streams the download to efficiently handle large files, shows progress using a progress bar, verifies the download completeness, and saves the data to a specified file path. The function includes proper error handling for failed requests or incomplete downloads.\n\nInput:\n- exchange: [str] - The financial exchange identifier (e.g., 'binance')\n- data_type: [str] - The type of market data to download (e.g., 'trades')\n- symbol: [str] - The trading symbol/pair (e.g., 'BTCUSDT')\n- yyyymmdd: [int/str] - The date in YYYYMMDD format\n- filepath: [str] - The local path where the downloaded file should be saved\n- key: [str] - The API authorization key (Bearer token)\n\nOutput:\n- None - The function doesn't return anything but saves the downloaded data to the specified file path\n- Raises RuntimeError if the download fails (either due to HTTP error or incomplete download)",
    "Canonical_solution": "import json\nimport os.path\nimport requests\nfrom datetime import datetime, timedelta\nfrom tqdm import tqdm\n\ndef download(exchange, data_type, symbol, yyyymmdd, filepath, key):\n    yyyymmdd = str(yyyymmdd)\n    url = f'https://datasets.tardis.dev/v1/{exchange}/{data_type}/{yyyymmdd[:4]}/{yyyymmdd[4:6]}/{yyyymmdd[6:]}/{symbol}.csv.gz'\n\n    response = requests.get(url, stream=True, headers={'Authorization': f'Bearer {key}'})\n    if response.status_code != 200:\n        raise RuntimeError('Could not download file')\n\n    total_size = int(response.headers.get('content-length', 0))\n    block_size = 1024\n\n    with tqdm(total=total_size, unit='B', unit_scale=True) as progress_bar:\n        with open(filepath, 'wb') as file:\n            for data in response.iter_content(block_size):\n                progress_bar.update(len(data))\n                file.write(data)\n\n    if total_size != 0 and progress_bar.n != total_size:\n        raise RuntimeError('Could not download file')",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport os\nfrom tqdm import tqdm\n\ndef download_market_data(exchange, data_type, symbol, yyyymmdd, filepath, key):\n    url = f\"https://api.marketdata.com/v1/{exchange}/{data_type}/{symbol}/{yyyymmdd}.csv.gz\"\n    headers = {\"Authorization\": f\"Bearer {key}\"}\n    \n    try:\n        with requests.get(url, headers=headers, stream=True) as response:\n            response.raise_for_status()\n            total_size = int(response.headers.get('content-length', 0))\n            \n            with open(filepath, 'wb') as f, tqdm(\n                desc=f\"Downloading {os.path.basename(filepath)}\",\n                total=total_size,\n                unit='B',\n                unit_scale=True,\n                unit_divisor=1024,\n            ) as progress_bar:\n                for chunk in response.iter_content(chunk_size=8192):\n                    if chunk:\n                        f.write(chunk)\n                        progress_bar.update(len(chunk))\n            \n            actual_size = os.path.getsize(filepath)\n            if total_size != 0 and actual_size != total_size:\n                os.remove(filepath)\n                raise RuntimeError(f\"Download incomplete. Expected {total_size} bytes, got {actual_size} bytes\")\n                \n    except requests.exceptions.RequestException as e:\n        raise RuntimeError(f\"Failed to download data: {str(e)}\")"
  },
  {
    "Task_id": 68,
    "Instruction": "Task Description: Download a comic image from the xkcd website given a specific issue number.\n\nFunction Description: The function downloads an image from the xkcd comic website by first fetching the webpage of the specified issue number, parsing the HTML to extract the image URL, and then downloading the image to the local filesystem. It handles HTTP requests and responses, checks for success status codes, and saves the image using streaming to avoid memory issues with large files.\n\nInput: \n- issue_number: [str] - The issue number of the comic to download (e.g., \"1234\"). This is appended to the base xkcd URL to form the full webpage URL.\n\nOutput: \n- [str] - A success message indicating the downloaded image filename if successful, or an error message if the request fails or the issue number is invalid.",
    "Canonical_solution": "import requests\nfrom bs4 import BeautifulSoup as bs\nimport shutil\n\ndef download_comic_image(issue_number):\n    url = \"https://xkcd.com/\"+ issue_number\n    response = requests.get(url)\n\n    if response.status_code == 200:\n        soup = bs(response.content, 'html.parser')\n        image_link = soup.find_all('img')[2]['src']\n        image_name = image_link.split('/')[-1]\n        image_url = \"https:\" + image_link\n        r = requests.get(image_url, stream=True)\n        \n        if r.status_code == 200:\n            r.raw.decode_content = True\n            with open(image_name, 'wb') as f:\n                shutil.copyfileobj(r.raw, f)\n            return f'Image successfully Downloaded: {image_name}'\n        else:\n            return 'Image Couldn\\'t be retrieved'\n    else:\n        return \"Issue number is invalid\"",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom bs4 import BeautifulSoup\nimport os\n\ndef download_xkcd_comic(issue_number):\n    base_url = f\"https://xkcd.com/{issue_number}/\"\n    try:\n        response = requests.get(base_url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        comic_div = soup.find(id='comic')\n        if not comic_div:\n            return \"Error: Could not find comic image on the page.\"\n        img_tag = comic_div.find('img')\n        if not img_tag:\n            return \"Error: Could not find image tag in the comic div.\"\n        img_url = img_tag.get('src')\n        if not img_url:\n            return \"Error: Image URL not found in the image tag.\"\n        if not img_url.startswith('http'):\n            img_url = f\"https:{img_url}\"\n        img_response = requests.get(img_url, stream=True)\n        img_response.raise_for_status()\n        filename = os.path.basename(img_url)\n        with open(filename, 'wb') as f:\n            for chunk in img_response.iter_content(chunk_size=8192):\n                f.write(chunk)\n        return f\"Successfully downloaded comic image to {filename}\"\n    except requests.exceptions.RequestException as e:\n        return f\"Error: Failed to download comic - {str(e)}\"\n    except Exception as e:\n        return f\"Error: An unexpected error occurred - {str(e)}\""
  },
  {
    "Task_id": 7,
    "Instruction": "Task Description: Create a Python function that makes an HTTP request to a specified API endpoint, handles the response, and manages errors appropriately. The function should support proxy configuration and include request signing.\n\nFunction Description: The `request` function sends an HTTP request to a predefined API endpoint. It constructs the request with proper parameters, handles the connection (including proxy support if configured), processes the response, and manages errors. The function includes logging for debugging and error tracking.\n\nInput:\n- `action`: [str] - The API endpoint action to be called (appended to '/api/')\n- `param`: [dict, optional] - A dictionary of parameters to be sent with the request\n- `**params`: [dict] - Additional parameters to be merged with `param`\n\nOutput:\n- Returns: [dict] - The 'data' field from the JSON response if successful\n- Raises:\n  - Exception: If the HTTP status code indicates an error (non-2xx)\n  - Exception: If the API response contains an error code\n  - Exception: If the response data is None\n\nThe function performs the following steps:\n1. Merges input parameters and signs them\n2. Establishes an HTTPS connection (with optional proxy support)\n3. Sends the request with proper headers\n4. Reads and decodes the response\n5. Validates the response status and content\n6. Returns the parsed data or raises appropriate exceptions",
    "Canonical_solution": "from hashlib import md5\nfrom json import loads as jsondecode\nfrom logging import debug, info, warning\nfrom time import mktime\nfrom datetime import datetime\nfrom http.client import HTTPSConnection\nfrom urllib.parse import urlencode\n\nclass Config:\n    ID = \"id\"\n    TOKEN = \"TOKEN\"\n    PROXY = None\n    TTL = None\n\nclass API:\n    SITE = \"www.dns.com\"\n    METHOD = \"POST\"\n\ndef request(action, param=None, **params):\n    if param:\n        params.update(param)\n    params = dict((k, params[k]) for k in params if params[k] is not None)\n    params = signature(params)\n    info(\"%s/api/%s/ : params:%s\", API.SITE, action, params)\n\n    if Config.PROXY:\n        conn = HTTPSConnection(Config.PROXY)\n        conn.set_tunnel(API.SITE, 443)\n    else:\n        conn = HTTPSConnection(API.SITE)\n\n    conn.request(API.METHOD, '/api/' + action + '/', urlencode(params),\n                 {\"Content-type\": \"application/x-www-form-urlencoded\"})\n    response = conn.getresponse()\n    result = response.read().decode('utf8')\n    conn.close()\n\n    if response.status < 200 or response.status >= 300:\n        warning('%s : error[%d]:%s', action, response.status, result)\n        raise Exception(result)\n    else:\n        data = jsondecode(result)\n        debug('%s : result:%s', action, data)\n        if data.get('code') != 0:\n            raise Exception(\"api error:\", data.get('message'))\n        data = data.get('data')\n        if data is None:\n            raise Exception('response data is none')\n        return data",
    "Code_level": "Function-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nimport json\nimport logging\nfrom urllib.parse import urlencode\nfrom hashlib import sha256\nimport hmac\nimport os\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\ndef request(action, param=None, **params):\n    if param is None:\n        param = {}\n    merged_params = {**param, **params}\n    \n    api_key = os.getenv('API_KEY', 'default_api_key')\n    api_secret = os.getenv('API_SECRET', 'default_api_secret')\n    base_url = os.getenv('API_BASE_URL', 'https://api.example.com')\n    \n    merged_params['api_key'] = api_key\n    merged_params['action'] = action\n    \n    sorted_params = sorted(merged_params.items(), key=lambda x: x[0])\n    query_string = urlencode(sorted_params)\n    signature = hmac.new(api_secret.encode(), query_string.encode(), sha256).hexdigest()\n    merged_params['sign'] = signature\n    \n    proxy_config = None\n    if os.getenv('HTTP_PROXY'):\n        proxy_config = {\n            'http': os.getenv('HTTP_PROXY'),\n            'https': os.getenv('HTTPS_PROXY')\n        }\n    \n    headers = {\n        'Content-Type': 'application/json',\n        'Accept': 'application/json'\n    }\n    \n    try:\n        response = requests.post(\n            f\"{base_url}/api/{action}\",\n            json=merged_params,\n            headers=headers,\n            proxies=proxy_config\n        )\n        \n        response.raise_for_status()\n        response_data = response.json()\n        \n        if response_data is None:\n            raise Exception(\"Empty response from server\")\n        \n        if 'error_code' in response_data and response_data['error_code'] != 0:\n            raise Exception(f\"API error: {response_data.get('error_msg', 'Unknown error')}\")\n        \n        if 'data' not in response_data:\n            raise Exception(\"Invalid response format: missing 'data' field\")\n        \n        return response_data['data']\n    \n    except requests.exceptions.RequestException as e:\n        logger.error(f\"Request failed: {str(e)}\")\n        raise Exception(f\"HTTP request failed: {str(e)}\")\n    except json.JSONDecodeError as e:\n        logger.error(f\"Failed to decode response: {str(e)}\")\n        raise Exception(\"Invalid JSON response\")\n    except Exception as e:\n        logger.error(f\"Unexpected error: {str(e)}\")\n        raise"
  },
  {
    "Task_id": 71,
    "Instruction": "Task Description: Create a function that scrapes and extracts text content from paragraphs of a given URL using HTTP requests and HTML parsing.\n\nFunction Description: The function should make an HTTP GET request to the specified URL, parse the HTML response to extract text from all paragraph elements (<p>), and return the combined text. It should handle potential errors in the HTTP request and log successful operations.\n\nInput: \n- url (str): The URL of the webpage to scrape text from.\n\nOutput: \n- str: The combined text extracted from all paragraph elements on the webpage, or an error message if the request fails.",
    "Canonical_solution": "import requests\nfrom bs4 import BeautifulSoup\nimport logging\n\ndef scrape_text_from_url(url):\n    response = requests.get(url)\n    if response.status_code != 200:\n        return \"Error: Could not retrieve content from URL.\"\n    soup = BeautifulSoup(response.text, \"html.parser\")\n    paragraphs = soup.find_all(\"p\")\n    text = \" \".join([p.get_text() for p in paragraphs])\n    logging.info(\"web scrape done\")\n    return text",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom bs4 import BeautifulSoup\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\n\ndef scrape_paragraphs(url):\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        soup = BeautifulSoup(response.text, 'html.parser')\n        paragraphs = soup.find_all('p')\n        text = ' '.join([p.get_text().strip() for p in paragraphs if p.get_text().strip()])\n        logging.info(f\"Successfully scraped paragraphs from {url}\")\n        return text\n    except requests.exceptions.RequestException as e:\n        logging.error(f\"Failed to scrape {url}: {e}\")\n        return f\"Error: Failed to scrape the webpage - {e}\""
  },
  {
    "Task_id": 72,
    "Instruction": "Task Description: Create a Python function that interacts with two different APIs: one for text-to-image generation (Hugging Face) and another for cloud storage (Firebase). The function should first generate an image from text using Hugging Face's API, then upload the resulting image to Firebase Storage.\n\nFunction Description: The function should take a text prompt as input, use it to generate an image via Hugging Face's API, and then upload that image to Firebase Storage. It should handle API errors gracefully and return the Firebase URL of the uploaded image.\n\nInput:\n- prompt (str): The text prompt to be converted into an image\n- destination_path (str): The path where the image should be stored in Firebase Storage\n\nOutput:\n- str: The Firebase Storage URL of the uploaded image, or None if either API call fails\n\nTask Description: Create a Python class that encapsulates the functionality for interacting with Hugging Face's text-to-image API and Firebase Storage API.\n\nClass Description: The class should manage API connections and provide methods for generating images from text and storing them in cloud storage. It should store necessary API credentials and configuration.\n\nAttributes:\n- HUGGING_FACE_API_URL (str): The endpoint URL for Hugging Face's text-to-image API\n- FIREBASE_API_URL (str): The base URL for Firebase Storage API\n- headers (dict): Authorization headers for Hugging Face API\n- API_KEY_PARAM (str): The API key parameter for Firebase requests\n- FIREBASE_STORAGE_BUCKET (str): The name of the Firebase Storage bucket\n\nMethods:\n- query(payload: dict) -> bytes: Sends a request to Hugging Face API with the given payload and returns the image data\n- upload_to_firebase_storage(image_data: bytes, destination_path: str) -> str: Uploads image data to Firebase Storage and returns the download URL\n- generate_and_store_image(prompt: str, destination_path: str) -> str: Combines both operations (generation and storage) into a single method",
    "Canonical_solution": "import requests\nfrom pydantic import Field\n\nHUGGING_FACE_API_URL = \"https://api-inference.huggingface.co/models/prompthero/openjourney-v4\"\nheaders = {\"Authorization\": f\"Bearer {HUGGING_FACE_TOKEN}\"}\nFIREBASE_API_URL = f\"https://firebasestorage.googleapis.com/v0/b/{FIREBASE_STORAGE_BUCKET}/o\"\nAPI_KEY_PARAM = f\"key={FIREBASE_API_KEY}\"\n\ndef query(payload):\n    \"\"\"\n    Queries the Hugging Face API to convert text to an image.\n    Args:\n        payload (dict): Input payload for the Hugging Face API.\n    Returns:\n        bytes: Image data.\n    \"\"\"\n    try:\n        response = requests.post(HUGGING_FACE_API_URL, headers=headers, json=payload, timeout=40)\n        return response.content\n    except requests.exceptions.RequestException as e:\n        print(f\"Error from Hugging Face API: {e}\")\n        return None\n\ndef upload_to_firebase_storage(image_data, destination_path):\n    \"\"\"\n    Uploads image data to Firebase Storage.\n    Args:\n        image_data (bytes): Image data to be uploaded.\n        destination_path (str): Path where the image will be stored.\n    Returns:\n        str: Firebase URL of the uploaded image.\n    \"\"\"\n    upload_url = f\"{FIREBASE_API_URL}/{destination_path}?{API_KEY_PARAM}\"\n    headers = {\"Content-Type\": \"image/png\"}\n\n    response = requests.post(upload_url, headers=headers, data=image_data)\n\n    if response.status_code == 200:\n        download_url = response.json().get(\"downloadTokens\", \"\")\n        firebase_url = f\"https://firebasestorage.googleapis.com/v0/b/{FIREBASE_STORAGE_BUCKET}/o/{destination_path}?alt=media&token={download_url}\"\n        return firebase_url\n    else:\n        print(\"Error uploading to Firebase Storage:\", response.content)\n        return None",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport json\nfrom firebase_admin import storage, initialize_app, credentials\n\nclass ImageGeneratorStorage:\n    def __init__(self, hugging_face_api_url, hugging_face_token, firebase_config, firebase_storage_bucket):\n        self.HUGGING_FACE_API_URL = hugging_face_api_url\n        self.headers = {\"Authorization\": f\"Bearer {hugging_face_token}\"}\n        cred = credentials.Certificate(firebase_config)\n        initialize_app(cred, {'storageBucket': firebase_storage_bucket})\n        self.bucket = storage.bucket()\n\n    def query(self, payload):\n        try:\n            response = requests.post(self.HUGGING_FACE_API_URL, headers=self.headers, json=payload)\n            response.raise_for_status()\n            return response.content\n        except requests.exceptions.RequestException:\n            return None\n\n    def upload_to_firebase_storage(self, image_data, destination_path):\n        try:\n            blob = self.bucket.blob(destination_path)\n            blob.upload_from_string(image_data, content_type='image/png')\n            blob.make_public()\n            return blob.public_url\n        except Exception:\n            return None\n\n    def generate_and_store_image(self, prompt, destination_path):\n        image_data = self.query({\"inputs\": prompt})\n        if not image_data:\n            return None\n        url = self.upload_to_firebase_storage(image_data, destination_path)\n        return url"
  },
  {
    "Task_id": 74,
    "Instruction": "Task Description: Implement a Python class that serves as a client for the Pushover notification service, capable of sending both text messages and file attachments through HTTP requests.\n\nClass Description: The PushoverClient class provides functionality to interact with the Pushover API, allowing users to send notifications with optional HTML formatting, titles, URLs, and file attachments. It handles HTTP communication and error logging.\n\nAttributes:\n- token: str - The API token for authenticating with Pushover\n- user: str - The user key identifying the recipient\n- device: str - The target device for the notification\n- pushover_url: str - The Pushover API endpoint URL\n\nMethods:\n- send_message(title: str, message: str, send_as_html: bool, additional_url: str) -> None - Sends a text notification with optional HTML formatting, title, and URL\n- send_file(file_name: str, contents: bytes, title: str, message: str, send_as_html: bool, additional_url: str) -> None - Sends a notification with a file attachment (converts SVG to PNG if needed) along with text content",
    "Canonical_solution": "import logging\nimport requests\n\nclass PushoverClient:\n    def __init__(self, token: str, user: str, device: str, pushover_url: str):\n        self.token = str(token)\n        self.user = str(user)\n        self.device = str(device)\n        self.pushover_url = str(pushover_url)\n\n    def send_message(self, title: str, message: str, send_as_html: bool, additional_url: str):\n        message_json = {\n            \"token\": self.token,\n            \"user\": self.user,\n            \"html\": 1 if send_as_html else \"\",\n            \"title\": title,\n            \"message\": message,\n            \"url\": additional_url,\n            \"device\": self.device\n        }\n\n        response = requests.post(self.pushover_url, json=message_json)\n\n        if response.status_code != 200:\n            logging.error(\n                f\"Failed to send pushover message with reason being: {response.reason} {response.text}\"\n            )\n\n    def send_file(self, file_name: str, contents: bytes, title: str, message: str, send_as_html: bool, additional_url: str):\n        if not is_image(file_name):\n            return\n            \n        if file_name.endswith(SVG_SUFFIX):\n            contents = convert_svg_to_png(contents)\n            file_name = file_name.replace(SVG_SUFFIX, PNG_SUFFIX)\n        files = {\"photo\": (file_name, contents.decode())}\n\n        message_json = {\n            \"token\": self.token,\n            \"user\": self.user,\n            \"html\": 1 if send_as_html else \"\",\n            \"title\": title,\n            \"message\": message,\n            \"url\": additional_url,\n            \"device\": self.device\n        }\n\n        response = requests.post(self.pushover_url, json=message_json, files=files)\n\n        if response.status_code != 200:\n            logging.error(\n                f\"Failed to send pushover message with reason being: {response.reason} {response.text}\"\n            )",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport logging\nfrom io import BytesIO\nfrom typing import Optional\n\nclass PushoverClient:\n    def __init__(self, token: str, user: str, device: Optional[str] = None):\n        self.token = token\n        self.user = user\n        self.device = device\n        self.pushover_url = \"https://api.pushover.net/1/messages.json\"\n        self.logger = logging.getLogger(__name__)\n\n    def send_message(self, title: str, message: str, send_as_html: bool = False, additional_url: Optional[str] = None) -> None:\n        data = {\n            \"token\": self.token,\n            \"user\": self.user,\n            \"title\": title,\n            \"message\": message,\n            \"html\": 1 if send_as_html else 0\n        }\n        if self.device:\n            data[\"device\"] = self.device\n        if additional_url:\n            data[\"url\"] = additional_url\n\n        try:\n            response = requests.post(self.pushover_url, data=data)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to send message: {e}\")\n\n    def send_file(self, file_name: str, contents: bytes, title: str, message: str, send_as_html: bool = False, additional_url: Optional[str] = None) -> None:\n        data = {\n            \"token\": self.token,\n            \"user\": self.user,\n            \"title\": title,\n            \"message\": message,\n            \"html\": 1 if send_as_html else 0\n        }\n        if self.device:\n            data[\"device\"] = self.device\n        if additional_url:\n            data[\"url\"] = additional_url\n\n        files = {\"attachment\": (file_name, BytesIO(contents))}\n\n        try:\n            response = requests.post(self.pushover_url, data=data, files=files)\n            response.raise_for_status()\n        except requests.exceptions.RequestException as e:\n            self.logger.error(f\"Failed to send file: {e}\")"
  },
  {
    "Task_id": 75,
    "Instruction": "Task Description: Create a Python class that interacts with the AccuWeather API to fetch weather information for a given city using an API key.\n\nClass Description: The AccuWeatherClient class provides functionality to retrieve weather data from AccuWeather's API. It handles API key management and makes HTTP requests to fetch location keys and weather information.\n\nAttributes:\napi_key: str - The AccuWeather API key required for authentication\n\nMethods:\n__init__: (api_key: str = \"\") -> None - Initializes the AccuWeatherClient with an optional API key\nget_weather: (city: str, extensions: str = \"all\") -> str - Fetches weather data for the specified city. The 'extensions' parameter can be \"all\" for current conditions or \"base\" for 5-day forecast. Returns formatted weather information or an error message if no API key is provided.",
    "Canonical_solution": "import json\nimport requests\n\nclass FreeApi:\n    def __init__(self, api_url: str):\n        self.api_url = api_url\n\n    def get_result(self) -> str:\n        response = requests.get(self.api_url)\n        if response.status_code == 200:\n            return response.text\n        else:\n            return None\n\nclass AccuWeatherClient:\n    def __init__(self, api_key: str = \"\"):\n        self.api_key = api_key\n\n    def get_weather(self, city: str, extensions: str = \"all\") -> str:\n        if not self.api_key:\n            return \"\u8bf7\u5148\u5728\u5de5\u5177\uff08tools\uff09\u4e2d\u8f93\u5165accuweather\u7684key\"\n\n        # Query city location key\n        location_api = FreeApi(\n            f\"http://dataservice.accuweather.com/locations/v1/cities/search?apikey={self.api_key}&q={city}&language=zh-CN&details=false\"\n        )\n        result = location_api.get_result()\n\n        if result:\n            data = json.loads(result)\n            city_key = data[0][\"Key\"]\n\n            # Query weather based on key\n            if extensions == \"all\":\n                weather_api = FreeApi(\n                    f\"http://dataservice.accuweather.com/currentconditions/v1/{city_key}?apikey={self.api_key}&language=zh-CN&details=false\"\n                )\n            elif extensions == \"base\":\n                weather_api = FreeApi(\n                    f\"http://dataservice.accuweather.com/forecasts/v1/daily/5day/{city_key}?apikey={self.api_key}&language=zh-CN&details=false\"\n                )\n\n            result = weather_api.get_result()\n            if result:\n                data = json.loads(result)\n                if extensions == \"all\":\n                    return f\"\u67e5\u8be2\u5230\u7684\u5929\u6c14\u4fe1\u606f\u5982\u4e0b\uff1a{str(data[0])}\u8bf7\u6839\u636e\u5929\u6c14\u4fe1\u606f\u56de\u7b54\u7528\u6237\u95ee\u9898\"\n                elif extensions == \"base\":\n                    return f\"\u67e5\u8be2\u5230\u7684\u5929\u6c14\u4fe1\u606f\u5982\u4e0b\uff1a{str(data['DailyForecasts'])}\u8bf7\u6839\u636e\u5929\u6c14\u4fe1\u606f\u56de\u7b54\u7528\u6237\u95ee\u9898\"\n        return None",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\n\nclass AccuWeatherClient:\n    def __init__(self, api_key=\"\"):\n        self.api_key = api_key\n\n    def get_weather(self, city, extensions=\"all\"):\n        if not self.api_key:\n            return \"Error: No API key provided\"\n\n        base_url = \"http://dataservice.accuweather.com\"\n        location_url = f\"{base_url}/locations/v1/cities/search\"\n        params = {\n            'apikey': self.api_key,\n            'q': city\n        }\n\n        try:\n            location_response = requests.get(location_url, params=params)\n            location_response.raise_for_status()\n            location_data = location_response.json()\n            if not location_data:\n                return f\"Weather data not found for city: {city}\"\n\n            location_key = location_data[0]['Key']\n            if extensions == \"all\":\n                weather_url = f\"{base_url}/currentconditions/v1/{location_key}\"\n            else:\n                weather_url = f\"{base_url}/forecasts/v1/daily/5day/{location_key}\"\n\n            weather_params = {'apikey': self.api_key}\n            weather_response = requests.get(weather_url, params=weather_params)\n            weather_response.raise_for_status()\n            weather_data = weather_response.json()\n\n            return weather_data\n        except requests.exceptions.RequestException as e:\n            return f\"Error fetching weather data: {str(e)}\""
  },
  {
    "Task_id": 76,
    "Instruction": "Task Description: Create a Python function that interacts with the Face++ API to perform face detection on an image, including face count verification and extraction of facial attributes.\n\nFunction Description: The function uses the Face++ API to detect faces in an image, verifies that exactly one face is present, and extracts facial rectangle coordinates and headpose information. It handles various API response status codes and raises appropriate exceptions for errors.\n\nInput:\n- ctx: A context object containing:\n  - origin_image: The original image to be processed (expected to be an image object)\n\nOutput:\n- Modifies the ctx object by adding:\n  - face[\"rectangle\"]: Tuple containing (left, top, width, height) coordinates of the detected face\n  - face[\"roll_angle\"]: The roll angle of the detected face (divided by 2)\n- Raises:\n  - FaceError: When zero or multiple faces are detected\n  - APIError: For various API-related errors (authentication, authorization, bad request, etc.)",
    "Canonical_solution": "import os\nimport requests\nfrom hivision.error import FaceError, APIError\nfrom hivision.utils import resize_image_to_kb_base64\n\ndef detect_face_face_plusplus(ctx):\n    \"\"\"\n    \u57fa\u4e8eFace++ API\u63a5\u53e3\u7684\u4eba\u8138\u68c0\u6d4b\u5904\u7406\u5668\uff0c\u53ea\u8fdb\u884c\u4eba\u8138\u6570\u91cf\u7684\u68c0\u6d4b\n    :param ctx: \u4e0a\u4e0b\u6587\uff0c\u6b64\u65f6\u5df2\u83b7\u53d6\u5230\u539f\u59cb\u56fe\u548c\u62a0\u56fe\u7ed3\u679c\uff0c\u4f46\u662f\u6211\u4eec\u53ea\u9700\u8981\u539f\u59cb\u56fe\n    :raise FaceError: \u4eba\u8138\u68c0\u6d4b\u9519\u8bef\uff0c\u591a\u4e2a\u4eba\u8138\u6216\u8005\u6ca1\u6709\u4eba\u8138\n    :raise APIError: API\u8c03\u7528\u9519\u8bef\n    \"\"\"\n    url = \"https://api-cn.faceplusplus.com/facepp/v3/detect\"\n    api_key = os.getenv(\"FACE_PLUS_API_KEY\")\n    api_secret = os.getenv(\"FACE_PLUS_API_SECRET\")\n\n    image = ctx.origin_image\n    image_base64 = resize_image_to_kb_base64(image, 2000, mode=\"max\")\n\n    files = {\n        \"api_key\": (None, api_key),\n        \"api_secret\": (None, api_secret),\n        \"image_base64\": (None, image_base64),\n        \"return_landmark\": (None, \"1\"),\n        \"return_attributes\": (None, \"headpose\"),\n    }\n\n    response = requests.post(url, files=files)\n    status_code = response.status_code\n    response_json = response.json()\n\n    if status_code == 200:\n        face_num = response_json[\"face_num\"]\n        if face_num == 1:\n            face_rectangle = response_json[\"faces\"][0][\"face_rectangle\"]\n            headpose = response_json[\"faces\"][0][\"attributes\"][\"headpose\"]\n            roll_angle = headpose[\"roll_angle\"] / 2\n\n            ctx.face[\"rectangle\"] = (\n                face_rectangle[\"left\"],\n                face_rectangle[\"top\"],\n                face_rectangle[\"width\"],\n                face_rectangle[\"height\"],\n            )\n            ctx.face[\"roll_angle\"] = roll_angle\n        else:\n            raise FaceError(\n                \"Expected 1 face, but got {}\".format(face_num), len(face_num)\n            )\n    elif status_code == 401:\n        raise APIError(\n            f\"Face++ Status code {status_code} Authentication error: API key and secret do not match.\",\n            status_code,\n        )\n    elif status_code == 403:\n        reason = response_json.get(\"error_message\", \"Unknown authorization error.\")\n        raise APIError(\n            f\"Authorization error: {reason}\",\n            status_code,\n        )\n    elif status_code == 400:\n        error_message = response_json.get(\"error_message\", \"Bad request.\")\n        raise APIError(\n            f\"Bad request error: {error_message}\",\n            status_code,\n        )\n    elif status_code == 413:\n        raise APIError(\n            f\"Face++ Status code {status_code} Request entity too large: The image exceeds the 2MB limit.\",\n            status_code,\n        )",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport base64\nfrom io import BytesIO\n\nclass FaceError(Exception):\n    pass\n\nclass APIError(Exception):\n    pass\n\ndef detect_face(ctx):\n    api_key = 'YOUR_API_KEY'\n    api_secret = 'YOUR_API_SECRET'\n    url = 'https://api-us.faceplusplus.com/facepp/v3/detect'\n\n    img_byte_arr = BytesIO()\n    ctx.origin_image.save(img_byte_arr, format='JPEG')\n    img_base64 = base64.b64encode(img_byte_arr.getvalue()).decode('utf-8')\n\n    params = {\n        'api_key': api_key,\n        'api_secret': api_secret,\n        'image_base64': img_base64,\n        'return_attributes': 'headpose'\n    }\n\n    try:\n        response = requests.post(url, data=params)\n        response.raise_for_status()\n        data = response.json()\n\n        if 'error_message' in data:\n            if 'AUTHENTICATION_ERROR' in data['error_message']:\n                raise APIError('Authentication failed')\n            elif 'AUTHORIZATION_ERROR' in data['error_message']:\n                raise APIError('Authorization failed')\n            elif 'CONCURRENCY_LIMIT_EXCEEDED' in data['error_message']:\n                raise APIError('Concurrency limit exceeded')\n            else:\n                raise APIError(data['error_message'])\n\n        if 'faces' not in data:\n            raise FaceError('No faces detected')\n\n        faces = data['faces']\n        if len(faces) != 1:\n            raise FaceError(f'Expected 1 face but found {len(faces)}')\n\n        face = faces[0]\n        rectangle = face['face_rectangle']\n        ctx.face = {\n            'rectangle': (\n                rectangle['left'],\n                rectangle['top'],\n                rectangle['width'],\n                rectangle['height']\n            ),\n            'roll_angle': face['attributes']['headpose']['roll_angle'] / 2\n        }\n\n    except requests.exceptions.RequestException as e:\n        raise APIError(f'API request failed: {str(e)}')"
  },
  {
    "Task_id": 77,
    "Instruction": "Task Description: Implement a function to check for software updates by querying a GitHub repository's API, comparing versions, and notifying if a new version is available.\n\nFunction Description: The function `check_update()` checks for updates by making HTTP requests to GitHub's API. It handles both stable releases and pre-releases based on configuration, compares version numbers, and provides appropriate notifications/logs about update availability. The function uses external configurations for update preferences and notification settings.\n\nInput: None (uses module-level configuration from `cfg` for settings like update preferences, user agent, and current version)\n\nOutput: None (but has side effects: may log update information and send notifications through the `notif` system)",
    "Canonical_solution": "from tasks.base.fastest_mirror import FastestMirror\nfrom module.logger import log\nfrom module.config import cfg\nfrom module.notification import notif\nfrom packaging.version import parse\nimport requests\n\ndef check_update():\n    try:\n        if cfg.update_prerelease_enable:\n            response = requests.get(FastestMirror.get_github_api_mirror(\"moesnow\", \"March7thAssistant\", False), timeout=10, headers=cfg.useragent)\n        else:\n            response = requests.get(FastestMirror.get_github_api_mirror(\"moesnow\", \"March7thAssistant\"), timeout=10, headers=cfg.useragent)\n        if not cfg.check_update:\n            return\n        if response.status_code == 200:\n            if cfg.update_prerelease_enable:\n                data = response.json()[0]\n            else:\n                data = response.json()\n\n            version = data[\"tag_name\"]\n\n            assert_url = None\n            for asset in data[\"assets\"]:\n                if (cfg.update_full_enable and \"full\" in asset[\"browser_download_url\"]) or \\\n                   (not cfg.update_full_enable and \"full\" not in asset[\"browser_download_url\"]):\n                    assert_url = asset[\"browser_download_url\"]\n                    break\n\n            if assert_url is not None and parse(version.lstrip('v')) > parse(cfg.version.lstrip('v')):\n                notif.notify(cfg.notify_template['NewVersion'].format(version=version))\n                log.info(f\"\u53d1\u73b0\u65b0\u7248\u672c\uff1a{cfg.version}  \u2014\u2014\u2192  {version}\")\n                log.info(data[\"html_url\"])\n            else:\n                log.info(f\"\u5df2\u7ecf\u662f\u6700\u65b0\u7248\u672c\uff1a{cfg.version}\")\n        else:\n            log.warning(\"\u68c0\u6d4b\u66f4\u65b0\u5931\u8d25\")\n            log.debug(response.text)\n    except Exception:\n        pass",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport semver\nimport logging\nfrom configparser import ConfigParser\n\ncfg = ConfigParser()\ncfg.read('config.ini')\n\ndef check_update():\n    current_version = cfg.get('version', 'current', fallback='0.0.0')\n    include_prereleases = cfg.getboolean('updates', 'include_prereleases', fallback=False)\n    user_agent = cfg.get('network', 'user_agent', fallback='UpdateChecker/1.0')\n    repo_owner = cfg.get('github', 'owner')\n    repo_name = cfg.get('github', 'repo')\n    \n    headers = {'User-Agent': user_agent}\n    url = f\"https://api.github.com/repos/{repo_owner}/{repo_name}/releases\"\n    \n    try:\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n        releases = response.json()\n        \n        latest_release = None\n        for release in releases:\n            if release['draft']:\n                continue\n            if not include_prereleases and release['prerelease']:\n                continue\n                \n            version_tag = release['tag_name'].lstrip('v')\n            try:\n                version = semver.VersionInfo.parse(version_tag)\n                if latest_release is None or version > semver.VersionInfo.parse(latest_release['tag_name'].lstrip('v')):\n                    latest_release = release\n            except ValueError:\n                continue\n        \n        if latest_release:\n            latest_version = latest_release['tag_name'].lstrip('v')\n            if semver.compare(latest_version, current_version) > 0:\n                logging.info(f\"New version available: {latest_version}\")\n                notif.send(f\"Update available: {latest_version}\")\n            else:\n                logging.info(\"No updates available\")\n        else:\n            logging.warning(\"No valid releases found\")\n            \n    except requests.exceptions.RequestException as e:\n        logging.error(f\"Failed to check for updates: {e}\")\n    except Exception as e:\n        logging.error(f\"Error during update check: {e}\")"
  },
  {
    "Task_id": 78,
    "Instruction": "Task Description: Implement a GitHub Single Sign-On (SSO) class to handle authentication and user information retrieval using GitHub's OAuth API.\n\nClass Description: The GitHubSSO class provides functionality to authenticate with GitHub's OAuth system, manage access tokens, and retrieve user information including email and name details.\n\nAttributes:\n- access_token: str - GitHub OAuth access token for API authentication\n- refresh_token: str - GitHub OAuth refresh token for obtaining new access tokens\n- client_id: str - GitHub OAuth application client ID\n- client_secret: str - GitHub OAuth application client secret\n- user_info: dict - Dictionary containing user information (email, first name, last name)\n\nMethods:\n- get_new_token() -> str - Requests a new access token using the refresh token\n  Input: None\n  Output: str - New access token\n\n- get_user_info() -> dict - Retrieves user information from GitHub API\n  Input: None\n  Output: dict - Contains user's primary email, first name, and last name\n    Returns format: {\n      \"email\": str,\n      \"first_name\": str,\n      \"last_name\": str\n    }\n  Throws: HTTPException if there's an error retrieving user info",
    "Canonical_solution": "import requests\nimport logging\nfrom fastapi import HTTPException\nfrom Globals import getenv\n\nclass GitHubSSO:\n    def __init__(\n        self,\n        access_token=None,\n        refresh_token=None,\n    ):\n        self.access_token = access_token\n        self.refresh_token = refresh_token\n        self.client_id = getenv(\"GITHUB_CLIENT_ID\")\n        self.client_secret = getenv(\"GITHUB_CLIENT_SECRET\")\n        self.user_info = self.get_user_info()\n\n    def get_new_token(self):\n        response = requests.post(\n            \"https://github.com/login/oauth/access_token\",\n            headers={\"Accept\": \"application/json\"},\n            data={\n                \"client_id\": self.client_id,\n                \"client_secret\": self.client_secret,\n                \"refresh_token\": self.refresh_token,\n                \"grant_type\": \"refresh_token\",\n            },\n        )\n        return response.json()[\"access_token\"]\n\n    def get_user_info(self):\n        uri = \"https://api.github.com/user\"\n        response = requests.get(\n            uri,\n            headers={\"Authorization\": f\"Bearer {self.access_token}\"},\n        )\n        if response.status_code == 401:\n            self.access_token = self.get_new_token()\n            response = requests.get(\n                uri,\n                headers={\"Authorization\": f\"Bearer {self.access_token}\"},\n            )\n        try:\n            data = response.json()\n            email_response = requests.get(\n                \"https://api.github.com/user/emails\",\n                headers={\"Authorization\": f\"Bearer {self.access_token}\"},\n            )\n            email_data = email_response.json()\n            primary_email = next(\n                email[\"email\"] for email in email_data if email[\"primary\"]\n            )\n            return {\n                \"email\": primary_email,\n                \"first_name\": (\n                    data.get(\"name\", \"\").split()[0] if data.get(\"name\") else \"\"\n                ),\n                \"last_name\": (\n                    data.get(\"name\", \"\").split()[-1] if data.get(\"name\") else \"\"\n                ),\n            }\n        except Exception as e:\n            raise HTTPException(\n                status_code=400,\n                detail=\"Error getting user info from GitHub\",\n            )",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom requests.exceptions import HTTPError\n\nclass GitHubSSO:\n    def __init__(self, client_id, client_secret, access_token=None, refresh_token=None):\n        self.access_token = access_token\n        self.refresh_token = refresh_token\n        self.client_id = client_id\n        self.client_secret = client_secret\n        self.user_info = None\n\n    def get_new_token(self):\n        url = \"https://github.com/login/oauth/access_token\"\n        headers = {\"Accept\": \"application/json\"}\n        data = {\n            \"client_id\": self.client_id,\n            \"client_secret\": self.client_secret,\n            \"refresh_token\": self.refresh_token,\n            \"grant_type\": \"refresh_token\"\n        }\n        response = requests.post(url, headers=headers, data=data)\n        response.raise_for_status()\n        token_data = response.json()\n        self.access_token = token_data.get(\"access_token\")\n        return self.access_token\n\n    def get_user_info(self):\n        if not self.access_token:\n            raise HTTPError(\"Access token is required\")\n\n        headers = {\n            \"Authorization\": f\"Bearer {self.access_token}\",\n            \"Accept\": \"application/json\"\n        }\n        user_response = requests.get(\"https://api.github.com/user\", headers=headers)\n        user_response.raise_for_status()\n        user_data = user_response.json()\n\n        email_response = requests.get(\"https://api.github.com/user/emails\", headers=headers)\n        email_response.raise_for_status()\n        email_data = email_response.json()\n\n        primary_email = next((email[\"email\"] for email in email_data if email.get(\"primary\")), None)\n        name_parts = user_data.get(\"name\", \"\").split(\" \", 1)\n        first_name = name_parts[0] if name_parts else \"\"\n        last_name = name_parts[1] if len(name_parts) > 1 else \"\"\n\n        self.user_info = {\n            \"email\": primary_email,\n            \"first_name\": first_name,\n            \"last_name\": last_name\n        }\n        return self.user_info"
  },
  {
    "Task_id": 79,
    "Instruction": "Task Description: Create a Python class that implements HTTP client functionality for interacting with the RunKeeper API, including rate limiting, authorization handling, and standard HTTP methods (GET, POST, DELETE).\n\nClass Description: The RunKeeperAPI class provides methods to interact with the RunKeeper web service API. It handles rate limiting, authorization, and provides wrapper methods for common HTTP operations. The class includes rate limit tracking and proper error handling for API responses.\n\nAttributes:\n_RATE_LIMIT_KEY: str - Redis key used for tracking global rate limit status\n\nMethods:\n_rate_limit: [Name](endpoint: str, req_lambda: Callable) -> requests.Response - Handles rate limiting logic by checking Redis for existing rate limits and processing API responses for rate limit errors\n_api_headers: [Name](service_record: object) -> dict - Generates authorization headers for API requests using the service record's token\nget_request: [Name](url: str, service_record: object) -> dict - Performs an HTTP GET request to the specified URL with authorization headers and returns parsed JSON response\npost_request: [Name](url: str, service_record: object, data: Optional[dict]) -> requests.Response - Performs an HTTP POST request to the specified URL with optional data and authorization headers\ndelete_request: [Name](url: str, service_record: object) -> requests.Response - Performs an HTTP DELETE request to the specified URL with authorization headers\noauth_token_request: [Name](params: dict) -> str - Handles OAuth token requests by posting to the RunKeeper token endpoint and returns the access token",
    "Canonical_solution": "from tapiriik.settings import WEB_ROOT, RUNKEEPER_CLIENT_ID, RUNKEEPER_CLIENT_SECRET\nfrom tapiriik.services.api import APIException, UserException, UserExceptionType\nimport requests\nimport urllib.parse\nimport json\nfrom datetime import timedelta\nimport re\n\nclass RunKeeperAPI:\n    _RATE_LIMIT_KEY = \"rk:rate_limit:hit\"\n\n    def _rate_limit(self, endpoint, req_lambda):\n        if redis.get(self._RATE_LIMIT_KEY) is not None:\n            raise APIException(\"RK global rate limit previously reached on %s\" % endpoint, user_exception=UserException(UserExceptionType.RateLimited))\n        response = req_lambda()\n        if response.status_code == 429:\n            if \"user\" not in response.text:\n                timeout = timedelta(minutes=15)\n                timeout_match = re.search(r\"(\\d+) (second|minute|hour|day)\", response.text)\n                if timeout_match:\n                    timeout = timedelta(**{\"%ss\" % timeout_match.group(2): float(timeout_match.group(1))}\n                redis.setex(self._RATE_LIMIT_KEY, response.text, timeout)\n                raise APIException(\"RK global rate limit reached on %s\" % endpoint, user_exception=UserException(UserExceptionType.RateLimited))\n            else:\n                raise APIException(\"RK user rate limit reached on %s\" % endpoint, user_exception=UserException(UserExceptionType.RateLimited))\n        return response\n\n    def _api_headers(self, service_record):\n        return {\"Authorization\": \"Bearer \" + service_record.Authorization[\"Token\"],\n                \"Accept-Charset\": \"UTF-8\"}\n\n    def get_request(self, url, service_record):\n        response = self._rate_limit(\"get_request\",\n                                  lambda: requests.get(url,\n                                                      headers=self._api_headers(service_record)))\n        if response.status_code != 200:\n            if response.status_code in [401, 403]:\n                raise APIException(\"No authorization\", block=True, user_exception=UserException(UserExceptionType.Authorization, intervention_required=True))\n            raise APIException(\"Request failed: %s %s\" % (response.status_code, response.text))\n        return response.json()\n\n    def post_request(self, url, service_record, data=None):\n        response = self._rate_limit(\"post_request\",\n                                  lambda: requests.post(url,\n                                                       headers=self._api_headers(service_record),\n                                                       data=data))\n        response.raise_for_status()\n        return response\n\n    def delete_request(self, url, service_record):\n        response = self._rate_limit(\"delete_request\",\n                                  lambda: requests.delete(url,\n                                                         headers=self._api_headers(service_record)))\n        response.raise_for_status()\n        return response\n\n    def oauth_token_request(self, params):\n        response = self._rate_limit(\"auth_token\",\n                                  lambda: requests.post(\"https://runkeeper.com/apps/token\",\n                                                      data=urllib.parse.urlencode(params),\n                                                      headers={\"Content-Type\": \"application/x-www-form-urlencoded\"}))\n        if response.status_code != 200:\n            raise APIException(\"Invalid code\")\n        return response.json()[\"access_token\"]",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nimport time\nimport json\nfrom typing import Optional, Callable, Dict, Any\n\nclass RunKeeperAPI:\n    _RATE_LIMIT_KEY = \"runkeeper:rate_limit\"\n\n    def __init__(self, redis_client=None):\n        self.redis = redis_client\n        self.base_url = \"https://api.runkeeper.com\"\n\n    def _rate_limit(self, endpoint: str, req_lambda: Callable) -> requests.Response:\n        if self.redis:\n            while True:\n                rate_limited = self.redis.get(self._RATE_LIMIT_KEY)\n                if rate_limited:\n                    time.sleep(float(rate_limited))\n                else:\n                    break\n\n        response = req_lambda()\n\n        if response.status_code == 429:\n            retry_after = response.headers.get('Retry-After', 60)\n            if self.redis:\n                self.redis.setex(self._RATE_LIMIT_KEY, int(retry_after), retry_after)\n            return self._rate_limit(endpoint, req_lambda)\n\n        return response\n\n    def _api_headers(self, service_record: object) -> dict:\n        return {\n            'Authorization': f'Bearer {service_record.token}',\n            'Accept': 'application/vnd.com.runkeeper.User+json'\n        }\n\n    def get_request(self, url: str, service_record: object) -> dict:\n        headers = self._api_headers(service_record)\n        def req():\n            return requests.get(url, headers=headers)\n        response = self._rate_limit(url, req)\n        response.raise_for_status()\n        return response.json()\n\n    def post_request(self, url: str, service_record: object, data: Optional[dict] = None) -> requests.Response:\n        headers = self._api_headers(service_record)\n        headers['Content-Type'] = 'application/json'\n        def req():\n            return requests.post(url, headers=headers, data=json.dumps(data) if data else requests.post(url, headers=headers)\n        response = self._rate_limit(url, req)\n        response.raise_for_status()\n        return response\n\n    def delete_request(self, url: str, service_record: object) -> requests.Response:\n        headers = self._api_headers(service_record)\n        def req():\n            return requests.delete(url, headers=headers)\n        response = self._rate_limit(url, req)\n        response.raise_for_status()\n        return response\n\n    def oauth_token_request(self, params: dict) -> str:\n        token_url = f\"{self.base_url}/oauth/token\"\n        response = requests.post(token_url, data=params)\n        response.raise_for_status()\n        return response.json().get('access_token')"
  },
  {
    "Task_id": 8,
    "Instruction": "Task Description: Create a Python class that manages JavaScript libraries and CSS files, loading their contents from remote URLs via HTTP/HTTPS and storing them for later use.\n\nClass Description: The `Javascript` class handles the loading and storage of JavaScript libraries and CSS files from remote sources. It provides functionality to initialize with URLs, normalize them into lists, and fetch their contents over HTTP/HTTPS connections.\n\nAttributes:\n- lib: [Sequence[str]] - List of URLs pointing to JavaScript libraries to be loaded\n- css: [Sequence[str]] - List of URLs pointing to CSS files to be loaded\n- data: [str] - Additional inline JavaScript code\n- javascript_contents: [dict[str, str]] - Dictionary mapping library URLs to their loaded content\n\nMethods:\n- __init__(data: Optional[str] = None, lib: Optional[Union[str, Sequence]] = None, css: Optional[Union[str, Sequence]] = None) -> None - Initializes the class instance, normalizing input parameters into lists\n- load_javascript_contents() -> Javascript - Loads the contents of all JavaScript libraries specified in lib attribute via HTTP/HTTPS, stores them in javascript_contents, and returns self for method chaining",
    "Canonical_solution": "from ..types import Optional, Sequence, Union\nfrom urllib.parse import urlparse\nimport http.client\n\nclass Javascript:\n    def __init__(\n        self,\n        data: Optional[str] = None,\n        lib: Optional[Union[str, Sequence]] = None,\n        css: Optional[Union[str, Sequence]] = None,\n    ):\n        if isinstance(lib, str):\n            lib = [lib]\n        elif lib is None:\n            lib = []\n        if isinstance(css, str):\n            css = [css]\n        elif css is None:\n            css = []\n        self.lib = lib\n        self.css = css\n        self.data = data or \"\"\n        self.javascript_contents = dict()\n\n    def load_javascript_contents(self):\n        for lib in self.lib:\n            parsed_url = urlparse(lib)\n            host: str = str(parsed_url.hostname)\n            port: int = parsed_url.port\n            path: str = parsed_url.path\n\n            resp: Optional[http.client.HTTPResponse] = None\n            try:\n                conn = http.client.HTTPSConnection(host, port)\n                conn.request(\"GET\", path)\n                resp = conn.getresponse()\n                if resp.status != 200:\n                    raise RuntimeError(\"Cannot load JavaScript lib: %s\" % lib)\n                self.javascript_contents[lib] = resp.read().decode(\"utf-8\")\n            finally:\n                if resp is not None:\n                    resp.close()\n        return self",
    "Code_level": "Class-level",
    "FSMID_for_test": 56,
    "wrong_result": "import requests\nfrom typing import Optional, Union, Sequence\n\nclass Javascript:\n    def __init__(self, data: Optional[str] = None, lib: Optional[Union[str, Sequence]] = None, css: Optional[Union[str, Sequence]] = None) -> None:\n        self.lib = self._normalize_to_list(lib)\n        self.css = self._normalize_to_list(css)\n        self.data = data or ''\n        self.javascript_contents = {}\n\n    def _normalize_to_list(self, input_data: Optional[Union[str, Sequence]]) -> list:\n        if input_data is None:\n            return []\n        if isinstance(input_data, str):\n            return [input_data]\n        return list(input_data)\n\n    def load_javascript_contents(self) -> 'Javascript':\n        for url in self.lib:\n            try:\n                response = requests.get(url)\n                response.raise_for_status()\n                self.javascript_contents[url] = response.text\n            except requests.RequestException:\n                self.javascript_contents[url] = ''\n        return self"
  },
  {
    "Task_id": 80,
    "Instruction": "Task Description: Create a Python function to interact with Gitee's API to create a new release and upload a file to that release.\n\nFunction Description: This function performs two main operations: 1) Creates a new release in a specified Gitee repository using OAuth authentication, and 2) Uploads a specified file to the created release. The function handles authentication, API requests, and response processing.\n\nInput:\n- owner (str): The owner of the Gitee repository\n- repo (str): The name of the Gitee repository\n- tag (str): The tag name for the new release\n- file_path (str): Path to the file to be uploaded (default: \"Seraphine.7z\")\n\nOutput:\n- dict: A dictionary containing:\n  - release_id (str): The ID of the created release\n  - download_url (str): The download URL of the uploaded file\n\nNote: The function requires the following environment variables to be set:\n- GITEE_OWNER\n- GITEE_REPO\n- GITEE_USERNAME\n- GITEE_PASSWORD\n- GITEE_CLIENT_ID\n- GITEE_CLIENT_SECRET",
    "Canonical_solution": "import argparse\nimport os\nimport requests\n\nGITEE_OWNER = os.environ[\"GITEE_OWNER\"]\nGITEE_REPO = os.environ[\"GITEE_REPO\"]\nGITEE_USERNAME = os.environ[\"GITEE_USERNAME\"]\nGITEE_PASSWORD = os.environ[\"GITEE_PASSWORD\"]\nGITEE_CLIENT_ID = os.environ[\"GITEE_CLIENT_ID\"]\nGITEE_CLIENT_SECRET = os.environ[\"GITEE_CLIENT_SECRET\"]\n\ndef create_new_release(owner, repo):\n    ACCESS_TOKEN = requests.post(\n        \"https://gitee.com/oauth/token\",\n        data={\n            \"grant_type\": \"password\",\n            \"username\": GITEE_USERNAME,\n            \"password\": GITEE_PASSWORD,\n            \"client_id\": GITEE_CLIENT_ID,\n            \"client_secret\": GITEE_CLIENT_SECRET,\n            \"scope\": \"projects\",\n        },\n    ).json()[\"access_token\"]\n\n    HEADERS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n    TAG_NAME = args.tag\n    NAME = TAG_NAME\n    BODY = f\"Seraphine {TAG_NAME}\"\n    TARGET_COMMITISH = \"master\"\n\n    url = f\"https://gitee.com/api/v5/repos/{owner}/{repo}/releases\"\n    data = {\n        \"tag_name\": TAG_NAME,\n        \"name\": NAME,\n        \"body\": BODY,\n        \"target_commitish\": TARGET_COMMITISH,\n    }\n    response = requests.post(url, data=data, headers=HEADERS, timeout=30)\n    if 200 <= response.status_code < 300:\n        return response.json()[\"id\"]\n    else:\n        print(response.json())\n        raise requests.HTTPError(\"create release on gitee failed.\")\n\ndef upload_file(onwer, repo, release_id):\n    ACCESS_TOKEN = requests.post(\n        \"https://gitee.com/oauth/token\",\n        data={\n            \"grant_type\": \"password\",\n            \"username\": GITEE_USERNAME,\n            \"password\": GITEE_PASSWORD,\n            \"client_id\": GITEE_CLIENT_ID,\n            \"client_secret\": GITEE_CLIENT_SECRET,\n            \"scope\": \"projects\",\n        },\n    ).json()[\"access_token\"]\n\n    HEADERS = {\"Authorization\": f\"Bearer {ACCESS_TOKEN}\"}\n    FILE_PATH = \"Seraphine.7z\"\n\n    url = f\"https://gitee.com/api/v5/repos/{onwer}/{repo}/releases/{release_id}/attach_files\"\n    files = {\"file\": open(FILE_PATH, \"rb\")}\n    response = requests.post(url, files=files, headers=HEADERS, timeout=30)\n\n    if 200 <= response.status_code < 300:\n        return response.json()[\"browser_download_url\"]\n    else:\n        print(response.json())\n        raise requests.HTTPError(\"push release file to Gitee failed.\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport requests\nimport json\nfrom requests.auth import HTTPBasicAuth\n\ndef create_gitee_release_and_upload(owner, repo, tag, file_path=\"Seraphine.7z\"):\n    # Step 1: Get OAuth token\n    auth_url = \"https://gitee.com/oauth/token\"\n    auth_data = {\n        \"grant_type\": \"password\",\n        \"username\": os.getenv(\"GITEE_USERNAME\"),\n        \"password\": os.getenv(\"GITEE_PASSWORD\"),\n        \"client_id\": os.getenv(\"GITEE_CLIENT_ID\"),\n        \"client_secret\": os.getenv(\"GITEE_CLIENT_SECRET\"),\n        \"scope\": \"projects\"\n    }\n    auth_response = requests.post(auth_url, data=auth_data)\n    token = auth_response.json().get(\"access_token\")\n    headers = {\"Authorization\": f\"Bearer {token}\"}\n\n    # Step 2: Create release\n    release_url = f\"https://gitee.com/api/v5/repos/{owner}/{repo}/releases\"\n    release_data = {\n        \"tag_name\": tag,\n        \"name\": tag,\n        \"body\": f\"Release for {tag}\"\n    }\n    release_response = requests.post(release_url, headers=headers, json=release_data)\n    release_id = release_response.json().get(\"id\")\n\n    # Step 3: Upload file\n    upload_url = f\"https://gitee.com/api/v5/repos/{owner}/{repo}/releases/{release_id}/upload\"\n    with open(file_path, 'rb') as f:\n        files = {'file': (os.path.basename(file_path), f)}\n        upload_response = requests.post(upload_url, headers=headers, files=files)\n    \n    download_url = upload_response.json().get(\"browser_download_url\")\n\n    return {\n        \"release_id\": str(release_id),\n        \"download_url\": download_url\n    }"
  },
  {
    "Task_id": 81,
    "Instruction": "Task Description: Create a Python class that interacts with a Loki logging system through HTTP and WebSocket connections, allowing for querying log ranges and tailing live logs.\n\nClass Description: The LokiClient class provides methods to query historical log data (query_range) and establish a WebSocket connection for live log tailing (create_tail_ws). The LokiTailWs class manages the WebSocket connection for live log streaming.\n\nAttributes:\n- query_range_url: str - Endpoint path for querying log ranges\n- tail_url: str - Endpoint path for tailing live logs\n- base_url: str - Base URL of the Loki server\n\nMethods:\n- __init__(base_url: str) -> None - Initializes the client with the Loki server's base URL\n- query_range(query: str, start: str, end: str, limit: int = 100) -> dict - Queries log data within a specified time range\n- create_tail_ws(query: str, limit: int = 100) -> LokiTailWs - Creates a WebSocket connection for live log tailing\n\nLokiTailWs Class Description: Manages a WebSocket connection for streaming live logs from Loki.\n\nAttributes:\n- _ws: WebSocket - The active WebSocket connection\n\nMethods:\n- __init__(ws: WebSocket) -> None - Initializes with an established WebSocket connection\n- messages() -> Generator - Yields incoming log messages from the WebSocket\n- close() -> None - Closes the WebSocket connection",
    "Canonical_solution": "import urllib.parse\nimport requests\nfrom websockets.sync.client import connect as ws_connect\n\nclass LokiClient:\n    query_range_url = '/loki/api/v1/query_range'\n    tail_url = '/loki/api/v1/tail'\n\n    def __init__(self, base_url: str):\n        self.base_url = base_url.rstrip('/')\n\n    def query_range(self, query, start, end, limit=100):\n        params = {\n            'query': query,\n            'start': start,\n            'end': end,\n            'limit': limit,\n        }\n        url = f\"{self.base_url}{self.query_range_url}\"\n        response = requests.get(url, params=params)\n        if response.status_code != 200:\n            raise Exception(response.text)\n        return response.json()\n\n    def create_tail_ws(self, query, limit=100):\n        data = {'query': query, 'limit': limit}\n        params = urllib.parse.urlencode(data)\n        ws_url = f\"ws://{self.base_url[7:]}\"\n        if self.base_url.startswith('https://'):\n            ws_url = f\"wss://{self.base_url[8:]}\"\n        url = f\"{ws_url}{self.tail_url}?{params}\"\n        ws = ws_connect(url)\n        return LokiTailWs(ws)\n\nclass LokiTailWs:\n    def __init__(self, ws):\n        self._ws = ws\n\n    def messages(self):\n        for message in self._ws:\n            yield message\n\n    def close(self):\n        if self._ws:\n            self._ws.close()",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import json\nimport asyncio\nimport aiohttp\nfrom typing import Generator, Optional, Dict, Any\n\nclass LokiTailWs:\n    def __init__(self, ws: aiohttp.ClientWebSocketResponse):\n        self._ws = ws\n\n    async def messages(self) -> Generator[Dict[str, Any], None, None]:\n        try:\n            while True:\n                msg = await self._ws.receive()\n                if msg.type == aiohttp.WSMsgType.TEXT:\n                    data = json.loads(msg.data)\n                    yield data\n                elif msg.type == aiohttp.WSMsgType.ERROR:\n                    raise Exception(f\"WebSocket error: {self._ws.exception()}\")\n                elif msg.type == aiohttp.WSMsgType.CLOSED:\n                    break\n        except Exception as e:\n            raise e\n\n    async def close(self) -> None:\n        await self._ws.close()\n\nclass LokiClient:\n    def __init__(self, base_url: str):\n        self.base_url = base_url.rstrip('/')\n        self.query_range_url = f\"{self.base_url}/loki/api/v1/query_range\"\n        self.tail_url = f\"{self.base_url}/loki/api/v1/tail\"\n\n    async def query_range(self, query: str, start: str, end: str, limit: int = 100) -> Dict[str, Any]:\n        params = {\n            'query': query,\n            'start': start,\n            'end': end,\n            'limit': str(limit)\n        }\n        async with aiohttp.ClientSession() as session:\n            async with session.get(self.query_range_url, params=params) as resp:\n                resp.raise_for_status()\n                return await resp.json()\n\n    async def create_tail_ws(self, query: str, limit: int = 100) -> LokiTailWs:\n        params = {\n            'query': query,\n            'limit': str(limit)\n        }\n        session = aiohttp.ClientSession()\n        ws = await session.ws_connect(self.tail_url, params=params)\n        return LokiTailWs(ws)"
  },
  {
    "Task_id": 82,
    "Instruction": "Task Description: Implement a set of functions to interact with the GitHub API for retrieving release, comparison, pull request, and search information.\n\nFunction Description: The functions provide HTTP client functionality to query GitHub's REST API for specific repository information using authentication tokens.\n\nInput:\n- token: str - GitHub personal access token for authentication\n- repo: str - Repository name in format 'owner/repo'\n- base: str - Base commit/tag for comparison (only for get_github_compare)\n- head: str - Head commit/tag for comparison (only for get_github_compare)\n- pr_number: int - Pull request number (only for get_github_pull_request)\n- sha: str - Commit SHA hash (only for search_github_prs)\n\nOutput:\n- get_github_release: str - Returns the latest release tag name\n- get_github_compare: dict - Returns comparison data between two commits/tags\n- get_github_pull_request: dict - Returns pull request details\n- search_github_prs: dict - Returns search results for merged PRs containing the commit SHA",
    "Canonical_solution": "import requests\n\ndef get_github_release(token, repo):\n    response = requests.get(f\"https://api.github.com/repos/{repo}/releases/latest\", \n                          headers={'Authorization': f'token {token}'})\n    while response.status_code == 401:\n        print(\"Error: Invalid GitHub API token.\")\n        token = input('Token: ').strip()\n        response = requests.get(f\"https://api.github.com/repos/{repo}/releases/latest\", \n                              headers={'Authorization': f'token {token}'})\n    release = response.json()\n    return release[\"tag_name\"]\n\ndef get_github_compare(token, repo, base, head):\n    commits = requests.get(f\"https://api.github.com/repos/{repo}/compare/{base}...{head}\", \n                          headers={'Authorization': f'token {token}'}).json()\n    return commits\n\ndef get_github_pull_request(token, repo, pr_number):\n    pr_details = requests.get(f\"https://api.github.com/repos/{repo}/pulls/{pr_number}\", \n                            headers={'Authorization': f'token {token}'}).json()\n    return pr_details\n\ndef search_github_prs(token, repo, sha):\n    prs = requests.get(f\"https://api.github.com/search/issues?q=repo:{repo}+type:pr+is:merged+{sha}\", \n                      headers={'Authorization': f'token {token}'}).json()\n    return prs",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\n\ndef get_github_release(token, repo):\n    headers = {'Authorization': f'token {token}'}\n    url = f'https://api.github.com/repos/{repo}/releases/latest'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()['tag_name']\n\ndef get_github_compare(token, repo, base, head):\n    headers = {'Authorization': f'token {token}'}\n    url = f'https://api.github.com/repos/{repo}/compare/{base}...{head}'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_github_pull_request(token, repo, pr_number):\n    headers = {'Authorization': f'token {token}'}\n    url = f'https://api.github.com/repos/{repo}/pulls/{pr_number}'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef search_github_prs(token, repo, sha):\n    headers = {'Authorization': f'token {token}'}\n    query = f'is:pr is:merged repo:{repo} {sha}'\n    url = f'https://api.github.com/search/issues?q={query}'\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()"
  },
  {
    "Task_id": 87,
    "Instruction": "Task Description: Implement a Python function to download a file from a given URL and cache it locally, with progress tracking and ETag-based caching.\n\nFunction Description: The function should download a file from a specified URL to a local cache directory, using ETag headers for cache validation. It should show download progress and handle temporary files during download. If the file already exists in cache (verified by ETag), it should skip downloading.\n\nInput:\n- url: str - The URL of the file to download\n- cache_dir: Optional[str] - (Optional) Directory path for caching downloaded files. Defaults to DATASET_CACHE if None.\n\nOutput:\n- str - Returns the local file path where the downloaded content is cached\n\nNote: The function should:\n1. Create cache directory if it doesn't exist\n2. Check file existence using HEAD request and ETag\n3. Download with progress bar if not in cache\n4. Store metadata (URL and ETag) alongside cached file\n5. Handle temporary files safely during download\n6. Return final cache path",
    "Canonical_solution": "import os\nimport requests\nimport tempfile\nimport shutil\nimport json\nfrom tqdm import tqdm\n\nCACHE_ROOT = Path(os.getenv(\"SCISPACY_CACHE\", str(Path.home() / \".scispacy\")))\nDATASET_CACHE = str(CACHE_ROOT / \"datasets\")\n\ndef http_get(url: str, temp_file: IO) -> None:\n    req = requests.get(url, stream=True)\n    total = int(req.headers.get(\"content-length\", 0))\n    pbar = tqdm(total=total, unit=\"iB\", unit_scale=True, unit_divisor=1024)\n    for chunk in req.iter_content(chunk_size=1024):\n        if chunk:  # filter out keep-alive new chunks\n            size = temp_file.write(chunk)\n            pbar.update(size)\n    pbar.close()\n\ndef get_from_cache(url: str, cache_dir: Optional[str] = None) -> str:\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n\n    os.makedirs(cache_dir, exist_ok=True)\n\n    response = requests.head(url, allow_redirects=True)\n    if response.status_code != 200:\n        raise IOError(\n            \"HEAD request failed for url {} with status code {}\".format(\n                url, response.status_code\n            )\n        )\n    etag = response.headers.get(\"ETag\")\n\n    filename = url_to_filename(url, etag)\n\n    cache_path = os.path.join(cache_dir, filename)\n\n    if not os.path.exists(cache_path):\n        with tempfile.NamedTemporaryFile() as temp_file:\n            print(f\"{url} not found in cache, downloading to {temp_file.name}\")\n\n            http_get(url, temp_file)\n\n            temp_file.flush()\n            temp_file.seek(0)\n\n            print(f\"Finished download, copying {temp_file.name} to cache at {cache_path}\")\n            with open(cache_path, \"wb\") as cache_file:\n                shutil.copyfileobj(temp_file, cache_file)\n\n            meta = {\"url\": url, \"etag\": etag}\n            meta_path = cache_path + \".json\"\n            with open(meta_path, \"w\") as meta_file:\n                json.dump(meta, meta_file)\n\n    return cache_path",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import os\nimport requests\nimport hashlib\nimport json\nfrom pathlib import Path\nfrom typing import Optional\nfrom urllib.parse import urlparse\n\nDATASET_CACHE = os.path.expanduser(\"~/.cache/datasets\")\n\ndef download_with_cache(url: str, cache_dir: Optional[str] = None) -> str:\n    if cache_dir is None:\n        cache_dir = DATASET_CACHE\n    os.makedirs(cache_dir, exist_ok=True)\n\n    parsed_url = urlparse(url)\n    url_hash = hashlib.md5(url.encode()).hexdigest()\n    filename = os.path.basename(parsed_url.path) or f\"file_{url_hash}\"\n    cache_path = os.path.join(cache_dir, filename)\n    meta_path = os.path.join(cache_dir, f\"{filename}.meta\")\n\n    headers = {}\n    if os.path.exists(meta_path):\n        with open(meta_path, 'r') as f:\n            metadata = json.load(f)\n            if metadata.get('url') == url and 'etag' in metadata:\n                headers['If-None-Match'] = metadata['etag']\n\n    response = requests.head(url, headers=headers, allow_redirects=True)\n    response.raise_for_status()\n\n    if response.status_code == 304:\n        return cache_path\n\n    etag = response.headers.get('ETag')\n    content_length = int(response.headers.get('Content-Length', 0))\n\n    temp_path = f\"{cache_path}.tmp\"\n    try:\n        with requests.get(url, stream=True, allow_redirects=True) as r:\n            r.raise_for_status()\n            with open(temp_path, 'wb') as f:\n                downloaded = 0\n                for chunk in r.iter_content(chunk_size=8192):\n                    if chunk:\n                        f.write(chunk)\n                        downloaded += len(chunk)\n                        if content_length > 0:\n                            progress = (downloaded / content_length) * 100\n                            print(f\"\\rDownloading: {progress:.1f}%\", end='')\n                print()\n\n        metadata = {'url': url, 'etag': etag}\n        with open(meta_path, 'w') as f:\n            json.dump(metadata, f)\n\n        os.replace(temp_path, cache_path)\n    except Exception as e:\n        if os.path.exists(temp_path):\n            os.unlink(temp_path)\n        raise e\n\n    return cache_path"
  },
  {
    "Task_id": 89,
    "Instruction": "Task Description: Create a Python function that interacts with a GPT API endpoint to generate completions based on given messages, handling the HTTP request and response process.\n\nFunction Description: The function `_create_completion` sends a POST request to a specified GPT API endpoint with formatted messages and randomly generated user data. It processes the response to extract and yield the assistant's message content if the request is successful.\n\nInput:\n- model: str - The GPT model to use (e.g., 'gpt-3.5-turbo')\n- messages: list - A list of message dictionaries, each containing 'role' and 'content' keys\n- stream: bool - A flag indicating whether to stream the response (not used in this implementation)\n- **kwargs: Additional optional arguments\n\nOutput:\n- Generator[str, None, None] - Yields the assistant's message content if the request is successful\n- None - Returns None if an error occurs during the request",
    "Canonical_solution": "import os\nimport requests\nimport json\nimport random\nimport time\nimport string\nfrom ...typing import sha256, Dict, get_type_hints\n\nurl = \"https://wewordle.org/gptapi/v1/android/turbo\"\nmodel = ['gpt-3.5-turbo']\nsupports_stream = False\nneeds_auth = False\n\ndef _create_completion(model: str, messages: list, stream: bool, **kwargs):\n    base = ''\n    for message in messages:\n        base += '%s: %s\\n' % (message['role'], message['content'])\n    base += 'assistant:'\n    _user_id = ''.join(random.choices(\n        f'{string.ascii_lowercase}{string.digits}', k=16))\n    _app_id = ''.join(random.choices(\n        f'{string.ascii_lowercase}{string.digits}', k=31))\n    _request_date = time.strftime(\"%Y-%m-%dT%H:%M:%S.000Z\", time.gmtime())\n    headers = {\n        'accept': '*/*',\n        'pragma': 'no-cache',\n        'Content-Type': 'application/json',\n        'Connection': 'keep-alive'\n    }\n    data = {\n        \"user\": _user_id,\n        \"messages\": [\n            {\"role\": \"user\", \"content\": base}\n        ],\n        \"subscriber\": {\n            \"originalPurchaseDate\": None,\n            \"originalApplicationVersion\": None,\n            \"allPurchaseDatesMillis\": {},\n            \"entitlements\": {\n                \"active\": {},\n                \"all\": {}\n            },\n            \"allPurchaseDates\": {},\n            \"allExpirationDatesMillis\": {},\n            \"allExpirationDates\": {},\n            \"originalAppUserId\": f\"$RCAnonymousID:{_app_id}\",\n            \"latestExpirationDate\": None,\n            \"requestDate\": _request_date,\n            \"latestExpirationDateMillis\": None,\n            \"nonSubscriptionTransactions\": [],\n            \"originalPurchaseDateMillis\": None,\n            \"managementURL\": None,\n            \"allPurchasedProductIdentifiers\": [],\n            \"firstSeen\": _request_date,\n            \"activeSubscriptions\": []\n        }\n    }\n    response = requests.post(url, headers=headers, data=json.dumps(data))\n    if response.status_code == 200:\n        _json = response.json()\n        if 'message' in _json:\n            message_content = _json['message']['content']\n            message_content = message_content.replace('**assistant:** ', '')\n            yield message_content\n    else:\n        print(f\"Error Occurred::{response.status_code}\")\n        return None",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import json\nimport random\nimport string\nimport requests\n\ndef _create_completion(model, messages, stream=False, **kwargs):\n    url = \"https://api.openai.com/v1/chat/completions\"\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"Authorization\": f\"Bearer {kwargs.get('api_key', '')}\"\n    }\n    user_id = ''.join(random.choices(string.ascii_letters + string.digits, k=16))\n    payload = {\n        \"model\": model,\n        \"messages\": messages,\n        \"user\": user_id\n    }\n    payload.update(kwargs)\n\n    try:\n        response = requests.post(url, headers=headers, json=payload)\n        response.raise_for_status()\n        data = response.json()\n        if 'choices' in data and len(data['choices']) > 0:\n            yield data['choices'][0]['message']['content']\n    except requests.exceptions.RequestException:\n        return None"
  },
  {
    "Task_id": 91,
    "Instruction": "Task Description: Create a Python class to interact with the Cal.com API for managing bookings, available time slots, and timezone conversions.\n\nClass Description: The CalComAPI class provides methods to interact with Cal.com's API v2 for scheduling operations. It handles authentication, timezone conversions, and provides functionality to check available slots, create/reschedule/cancel bookings, and view upcoming bookings.\n\nAttributes:\napi_key: str - The API key for authenticating with Cal.com's API\nevent_type_id: int - The event type ID for which bookings are managed\nuser_timezone: str - The timezone to display times to users (default: \"America/New_York\")\n\nMethods:\n_convert_to_user_timezone(utc_time: str) -> str - Converts UTC time string to user's local timezone\n_get_headers(api_version: str = \"2024-08-13\") -> Dict[str, str] - Returns authorization headers for API requests\nget_available_slots(start_date: str, end_date: str) -> str - Retrieves available booking slots between dates\ncreate_booking(start_time: str, name: str, email: str) -> str - Creates a new booking with attendee details\nget_upcoming_bookings(email: str) -> str - Retrieves upcoming bookings for an attendee\nreschedule_booking(booking_uid: str, new_start_time: str, reason: str) -> str - Reschedules an existing booking\ncancel_booking(booking_uid: str, reason: str) -> str - Cancels an existing booking",
    "Canonical_solution": "import requests\nimport pytz\nfrom typing import Dict, Optional\nfrom datetime import datetime\n\nclass CalComAPI:\n    def __init__(self, api_key: str, event_type_id: int, user_timezone: str = \"America/New_York\"):\n        self.api_key = api_key\n        self.event_type_id = event_type_id\n        self.user_timezone = user_timezone\n\n    def _convert_to_user_timezone(self, utc_time: str) -> str:\n        utc_dt = datetime.fromisoformat(utc_time.replace(\"Z\", \"+00:00\"))\n        user_tz = pytz.timezone(self.user_timezone)\n        user_dt = utc_dt.astimezone(user_tz)\n        return user_dt.strftime(\"%Y-%m-%d %H:%M %Z\")\n\n    def _get_headers(self, api_version: str = \"2024-08-13\") -> Dict[str, str]:\n        return {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"cal-api-version\": api_version,\n            \"Content-Type\": \"application/json\",\n        }\n\n    def get_available_slots(self, start_date: str, end_date: str) -> str:\n        try:\n            url = \"https://api.cal.com/v2/slots/available\"\n            querystring = {\n                \"startTime\": f\"{start_date}T00:00:00Z\",\n                \"endTime\": f\"{end_date}T23:59:59Z\",\n                \"eventTypeId\": self.event_type_id,\n            }\n\n            response = requests.get(url, headers=self._get_headers(), params=querystring)\n            if response.status_code == 200:\n                slots = response.json()[\"data\"][\"slots\"]\n                available_slots = []\n                for date, times in slots.items():\n                    for slot in times:\n                        user_time = self._convert_to_user_timezone(slot[\"time\"])\n                        available_slots.append(user_time)\n                return f\"Available slots: {', '.join(available_slots)}\"\n            return f\"Failed to fetch slots: {response.text}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def create_booking(self, start_time: str, name: str, email: str) -> str:\n        try:\n            url = \"https://api.cal.com/v2/bookings\"\n            start_time = datetime.fromisoformat(start_time).astimezone(pytz.utc).isoformat(timespec=\"seconds\")\n            payload = {\n                \"start\": start_time,\n                \"eventTypeId\": self.event_type_id,\n                \"attendee\": {\"name\": name, \"email\": email, \"timeZone\": self.user_timezone},\n            }\n\n            response = requests.post(url, json=payload, headers=self._get_headers())\n            if response.status_code == 201:\n                booking_data = response.json()[\"data\"]\n                user_time = self._convert_to_user_timezone(booking_data[\"start\"])\n                return f\"Booking created successfully for {user_time}. Booking uid: {booking_data['uid']}\"\n            return f\"Failed to create booking: {response.text}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def get_upcoming_bookings(self, email: str) -> str:\n        try:\n            url = \"https://api.cal.com/v2/bookings\"\n            querystring = {\"status\": \"upcoming\", \"attendeeEmail\": email}\n\n            response = requests.get(url, headers=self._get_headers(), params=querystring)\n            if response.status_code == 200:\n                bookings = response.json()[\"data\"]\n                if not bookings:\n                    return \"No upcoming bookings found.\"\n\n                booking_info = []\n                for booking in bookings:\n                    user_time = self._convert_to_user_timezone(booking[\"start\"])\n                    booking_info.append(\n                        f\"uid: {booking['uid']}, Title: {booking['title']}, Time: {user_time}, Status: {booking['status']}\"\n                    )\n                return \"Upcoming bookings:\\n\" + \"\\n\".join(booking_info)\n            return f\"Failed to fetch bookings: {response.text}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def reschedule_booking(self, booking_uid: str, new_start_time: str, reason: str) -> str:\n        try:\n            url = f\"https://api.cal.com/v2/bookings/{booking_uid}/reschedule\"\n            new_start_time = datetime.fromisoformat(new_start_time).astimezone(pytz.utc).isoformat(timespec=\"seconds\")\n            payload = {\"start\": new_start_time, \"reschedulingReason\": reason}\n\n            response = requests.post(url, json=payload, headers=self._get_headers())\n            if response.status_code == 201:\n                booking_data = response.json()[\"data\"]\n                user_time = self._convert_to_user_timezone(booking_data[\"start\"])\n                return f\"Booking rescheduled to {user_time}. New booking uid: {booking_data['uid']}\"\n            return f\"Failed to reschedule booking: {response.text}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"\n\n    def cancel_booking(self, booking_uid: str, reason: str) -> str:\n        try:\n            url = f\"https://api.cal.com/v2/bookings/{booking_uid}/cancel\"\n            payload = {\"cancellationReason\": reason}\n\n            response = requests.post(url, json=payload, headers=self._get_headers())\n            if response.status_code == 200:\n                return \"Booking cancelled successfully.\"\n            return f\"Failed to cancel booking: {response.text}\"\n        except Exception as e:\n            return f\"Error: {str(e)}\"",
    "Code_level": "Class-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom datetime import datetime\nimport pytz\n\nclass CalComAPI:\n    def __init__(self, api_key: str, event_type_id: int, user_timezone: str = \"America/New_York\"):\n        self.api_key = api_key\n        self.event_type_id = event_type_id\n        self.user_timezone = user_timezone\n        self.base_url = \"https://api.cal.com/v2\"\n\n    def _convert_to_user_timezone(self, utc_time: str) -> str:\n        utc_dt = datetime.fromisoformat(utc_time.replace('Z', '+00:00'))\n        user_tz = pytz.timezone(self.user_timezone)\n        local_dt = utc_dt.astimezone(user_tz)\n        return local_dt.isoformat()\n\n    def _get_headers(self, api_version: str = \"2024-08-13\") -> dict:\n        return {\n            \"Authorization\": f\"Bearer {self.api_key}\",\n            \"x-api-version\": api_version,\n            \"Content-Type\": \"application/json\"\n        }\n\n    def get_available_slots(self, start_date: str, end_date: str) -> str:\n        url = f\"{self.base_url}/slots\"\n        params = {\n            \"eventTypeId\": self.event_type_id,\n            \"startTime\": start_date,\n            \"endTime\": end_date\n        }\n        response = requests.get(url, headers=self._get_headers(), params=params)\n        response.raise_for_status()\n        return response.text\n\n    def create_booking(self, start_time: str, name: str, email: str) -> str:\n        url = f\"{self.base_url}/bookings\"\n        payload = {\n            \"eventTypeId\": self.event_type_id,\n            \"start\": start_time,\n            \"responses\": {\n                \"name\": name,\n                \"email\": email\n            }\n        }\n        response = requests.post(url, headers=self._get_headers(), json=payload)\n        response.raise_for_status()\n        return response.text\n\n    def get_upcoming_bookings(self, email: str) -> str:\n        url = f\"{self.base_url}/bookings\"\n        params = {\n            \"eventTypeId\": self.event_type_id,\n            \"userEmail\": email\n        }\n        response = requests.get(url, headers=self._get_headers(), params=params)\n        response.raise_for_status()\n        return response.text\n\n    def reschedule_booking(self, booking_uid: str, new_start_time: str, reason: str) -> str:\n        url = f\"{self.base_url}/bookings/{booking_uid}/reschedule\"\n        payload = {\n            \"start\": new_start_time,\n            \"reason\": reason\n        }\n        response = requests.post(url, headers=self._get_headers(), json=payload)\n        response.raise_for_status()\n        return response.text\n\n    def cancel_booking(self, booking_uid: str, reason: str) -> str:\n        url = f\"{self.base_url}/bookings/{booking_uid}/cancel\"\n        payload = {\n            \"reason\": reason\n        }\n        response = requests.post(url, headers=self._get_headers(), json=payload)\n        response.raise_for_status()\n        return response.text"
  },
  {
    "Task_id": 92,
    "Instruction": "Task Description: Create a Python function to interact with GitHub and ZenHub APIs to retrieve repository, board, and issue information.\n\nFunction Description: The function will make HTTP requests to GitHub and ZenHub APIs to fetch specific data about repositories, boards, and issues. It handles authentication, sends requests, checks responses, and returns the parsed data.\n\nInput:\n- github_token: [str] - GitHub personal access token for authentication\n- github_org: [str] - GitHub organization name\n- github_repo: [str] - GitHub repository name\n- zenhub_token: [str] - ZenHub API token for authentication\n- issue_number: [int] - GitHub issue number to query (for issue-related functions)\n\nOutput:\n- For get_github_repo_id: [int] - Returns the GitHub repository ID\n- For get_zenhub_board: [dict] - Returns the ZenHub board data as a dictionary\n- For get_github_issue: [dict] - Returns GitHub issue data as a dictionary\n- For get_zenhub_issue_info: [dict] - Returns ZenHub issue information as a dictionary\n\nMethods:\n1. get_github_repo_id(github_token, github_org, github_repo) -> int - Retrieves the repository ID from GitHub API\n2. get_zenhub_board(zenhub_token, github_repo_id) -> dict - Fetches ZenHub board data for a given repository ID\n3. get_github_issue(github_token, github_org, github_repo, issue_number) -> dict - Gets GitHub issue details\n4. get_zenhub_issue_info(zenhub_token, github_repo_id, issue_number) -> dict - Retrieves ZenHub-specific issue information",
    "Canonical_solution": "import requests\nimport os\n\nGITHUB_API_URL = \"https://api.github.com\"\nGITHUB_GRAPHQL_URL = \"https://api.github.com/graphql\"\nZENHUB_API_URL = \"https://api.zenhub.com/p1/repositories/{repo_id}/board\"\n\ndef get_github_repo_id(github_token, github_org, github_repo):\n    url = f\"{GITHUB_API_URL}/repos/{github_org}/{github_repo}\"\n    headers = {\n        \"Authorization\": github_token\n    }\n\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json().get(\"id\")\n    else:\n        response.raise_for_status()\n\ndef get_zenhub_board(zenhub_token, github_repo_id):\n    url = ZENHUB_API_URL.format(repo_id=github_repo_id)\n    headers = {\n        \"Content-Type\": \"application/json\",\n        \"X-Authentication-Token\": zenhub_token\n    }\n\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        response.raise_for_status()\n\ndef get_github_issue(github_token, github_org, github_repo, issue_number):\n    url = f\"https://api.github.com/repos/{github_org}/{github_repo}/issues/{issue_number}\"\n    headers = {\n        \"Authorization\": f\"token {github_token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json()\n    else:\n        response.raise_for_status()\n\ndef get_zenhub_issue_info(zenhub_token, github_repo_id, issue_number):\n    url = f\"https://api.zenhub.com/p1/repositories/{github_repo_id}/issues/{issue_number}\"\n    headers = {\n        \"X-Authentication-Token\": zenhub_token,\n        \"Accept\": \"application/json\"\n    }\n\n    response = requests.get(url, headers=headers)\n    if response.status_code == 200:\n        return response.json()\n    elif response.status_code == 403:\n        print(\"Rate limit exceeded. Sleeping for 1 minute\")\n    else:\n        response.raise_for_status()",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\n\ndef get_github_repo_id(github_token, github_org, github_repo):\n    url = f\"https://api.github.com/repos/{github_org}/{github_repo}\"\n    headers = {\n        \"Authorization\": f\"token {github_token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()[\"id\"]\n\ndef get_zenhub_board(zenhub_token, github_repo_id):\n    url = f\"https://api.zenhub.com/p2/repositories/{github_repo_id}/board\"\n    headers = {\n        \"X-Authentication-Token\": zenhub_token,\n        \"Content-Type\": \"application/json\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_github_issue(github_token, github_org, github_repo, issue_number):\n    url = f\"https://api.github.com/repos/{github_org}/{github_repo}/issues/{issue_number}\"\n    headers = {\n        \"Authorization\": f\"token {github_token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()\n\ndef get_zenhub_issue_info(zenhub_token, github_repo_id, issue_number):\n    url = f\"https://api.zenhub.com/p2/repositories/{github_repo_id}/issues/{issue_number}\"\n    headers = {\n        \"X-Authentication-Token\": zenhub_token,\n        \"Content-Type\": \"application/json\"\n    }\n    response = requests.get(url, headers=headers)\n    response.raise_for_status()\n    return response.json()"
  },
  {
    "Task_id": 96,
    "Instruction": "Task Description: Scrape SHA256 hashes from Hugging Face dataset pages and save them to a file.\n\nFunction Description: This function scrapes SHA256 hashes from a series of Hugging Face dataset pages by making HTTP GET requests to each page, parsing the HTML content to extract the hashes, and writing them to an output file along with their corresponding filenames. It also prints the collected hashes to the console.\n\nInput: None (The function uses hardcoded URLs and output filename)\n\nOutput: \n1. Writes to \"shard_hashes.txt\" file with format: \"[filename] [SHA256 hash]\"\n2. Prints collected SHA256 hashes to console with format: \"SHA256 hash [index]: [hash]\"\n3. Prints error messages for failed requests or missing hashes",
    "Canonical_solution": "import requests\nimport re\n\ndef scrape_huggingface_hashes():\n    base_url = 'https://huggingface.co/datasets/EleutherAI/pythia_deduped_pile_idxmaps/blob/main/pile_0.87_deduped_text_document-'\n    sha256_list = []\n\n    with open(\"shard_hashes.txt\", \"w\") as output_file:\n        for i in range(83):\n            page_number = f\"{i:05d}-of-00082.bin\"\n            url = base_url + page_number\n            file_name = url.split('/')[-1]\n\n            response = requests.get(url)\n\n            if response.status_code == 200:\n                html_content = response.text\n                matches = re.findall(r'<strong>SHA256:</strong>\\s*([\\da-fA-F]+)', html_content)\n\n                if matches:\n                    sha256_list.extend(matches)\n                    for sha256 in matches:\n                        output_file.write(f\"{file_name} {sha256}\\n\")\n                else:\n                    print(f\"No hash found for file: {file_name}\")\n            else:\n                print(f\"Failed to fetch HTML content from {url}. Status code: {response.status_code}\")\n\n    for idx, sha256 in enumerate(sha256_list, start=1):\n        print(f\"SHA256 hash {idx}: {sha256}\")",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import requests\nfrom bs4 import BeautifulSoup\n\ndef scrape_sha256_hashes():\n    urls = [\n        \"https://huggingface.co/datasets/example_dataset1\",\n        \"https://huggingface.co/datasets/example_dataset2\",\n        \"https://huggingface.co/datasets/example_dataset3\"\n    ]\n    output_file = \"shard_hashes.txt\"\n    hashes = []\n\n    with open(output_file, 'w') as f:\n        for idx, url in enumerate(urls):\n            try:\n                response = requests.get(url)\n                response.raise_for_status()\n                soup = BeautifulSoup(response.text, 'html.parser')\n                hash_element = soup.find('span', {'class': 'sha'})\n                \n                if hash_element:\n                    hash_value = hash_element.text.strip()\n                    filename = f\"shard_{idx}.bin\"\n                    f.write(f\"{filename} {hash_value}\\n\")\n                    hashes.append(hash_value)\n                    print(f\"SHA256 hash {idx}: {hash_value}\")\n                else:\n                    print(f\"Error: No SHA256 hash found for {url}\")\n            except requests.RequestException as e:\n                print(f\"Error fetching {url}: {e}\")\n\nscrape_sha256_hashes()"
  },
  {
    "Task_id": 97,
    "Instruction": "Task Description: Create a function that retrieves word embeddings from either a local API or OpenAI's API, depending on the specified model.\n\nFunction Description: The function `get_embedding_from_api` fetches a word embedding (vector representation) for a given word using either a local HTTP API or OpenAI's API. It handles both types of API calls, processes the response, and returns the embedding as a NumPy array. If the request fails, it prints an error message and returns None.\n\nInput:  \n- `word` (str): The word or text to generate an embedding for.  \n- `model` (str, optional): The name of the model to use for generating embeddings. Defaults to \"vicuna-7b-v1.5\".  \n\nOutput:  \n- `numpy.ndarray` or `None`: Returns the word embedding as a NumPy array if successful; otherwise, returns None.  \n\nNote: The function assumes the local API endpoint is running at `http://localhost:8000/v1/embeddings` and expects a JSON payload with `model` and `input` fields. For OpenAI's API, it assumes the `openai` library is properly configured.",
    "Canonical_solution": "import json\nimport requests\nimport numpy as np\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.5\"):\n    if \"ada\" in model:\n        resp = openai.Embedding.create(\n            model=model,\n            input=word,\n        )\n        embedding = np.array(resp[\"data\"][0][\"embedding\"])\n        return embedding\n\n    url = \"http://localhost:8000/v1/embeddings\"\n    headers = {\"Content-Type\": \"application/json\"}\n    data = json.dumps({\"model\": model, \"input\": word})\n\n    response = requests.post(url, headers=headers, data=data)\n    if response.status_code == 200:\n        embedding = np.array(response.json()[\"data\"][0][\"embedding\"])\n        return embedding\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import numpy as np\nimport requests\nimport openai\n\ndef get_embedding_from_api(word, model=\"vicuna-7b-v1.5\"):\n    if model.startswith(\"vicuna\"):\n        url = \"http://localhost:8000/v1/embeddings\"\n        payload = {\n            \"model\": model,\n            \"input\": word\n        }\n        try:\n            response = requests.post(url, json=payload)\n            response.raise_for_status()\n            embedding = response.json()['data'][0]['embedding']\n            return np.array(embedding)\n        except Exception as e:\n            print(f\"Error fetching embedding from local API: {e}\")\n            return None\n    else:\n        try:\n            response = openai.Embedding.create(\n                input=word,\n                model=model\n            )\n            embedding = response['data'][0]['embedding']\n            return np.array(embedding)\n        except Exception as e:\n            print(f\"Error fetching embedding from OpenAI API: {e}\")\n            return None"
  },
  {
    "Task_id": 98,
    "Instruction": "Task Description: Create a function to fetch all repositories from a GitHub organization using the GitHub API.\n\nFunction Description: The function `fetch_github_repos` asynchronously retrieves all repositories belonging to a specified GitHub organization. It handles pagination to ensure all repositories are fetched, even if they span multiple pages. The function uses the GitHub REST API with proper authentication and headers.\n\nInput:\n- `org_name` (str): The name of the GitHub organization whose repositories are to be fetched.\n- `token` (str): A GitHub personal access token for authentication.\n\nOutput:\n- Returns a list of dictionaries, where each dictionary represents a repository from the organization. Returns an empty list if the request fails or no repositories are found.",
    "Canonical_solution": "import requests\ntoken = \"YOUR_GITHUB_TOKEN\"\n\nasync def fetch_github_repos(org_name, token):\n    url = f\"https://api.github.com/orgs/{org_name}/repos\"\n    headers = {\n        \"Accept\": \"application/vnd.github+json\",\n        \"Authorization\": f\"Bearer {token}\",\n        \"X-GitHub-Api-Version\": \"2022-11-28\",\n    }\n\n    params = {\n        \"per_page\": 100  # You can adjust this number (max is 100)\n    }\n\n    all_repos = []\n\n    while url:\n        response = requests.get(url, headers=headers, params=params)\n\n        if response.status_code == 200:\n            repos = response.json()\n            all_repos.extend(repos)\n\n            # Check if there is a 'next' page\n            if \"next\" in response.links:\n                url = response.links[\"next\"][\"url\"]\n            else:\n                url = None\n        else:\n            print(\n                f\"Failed to fetch repositories: {response.status_code} - {response.text}\"\n            )\n            break\n\n    return all_repos",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import aiohttp\nimport asyncio\nfrom typing import List, Dict, Optional\n\nasync def fetch_github_repos(org_name: str, token: str) -> List[Dict]:\n    headers = {\n        \"Authorization\": f\"token {token}\",\n        \"Accept\": \"application/vnd.github.v3+json\"\n    }\n    repos = []\n    page = 1\n    per_page = 100\n    async with aiohttp.ClientSession() as session:\n        while True:\n            url = f\"https://api.github.com/orgs/{org_name}/repos?page={page}&per_page={per_page}\"\n            try:\n                async with session.get(url, headers=headers) as response:\n                    if response.status == 200:\n                        data = await response.json()\n                        if not data:\n                            break\n                        repos.extend(data)\n                        page += 1\n                    else:\n                        break\n            except Exception:\n                break\n    return repos"
  },
  {
    "Task_id": 99,
    "Instruction": "Task Description: Create a function to convert a PyTorch tensor to a base64-encoded PNG image and upload it to a specified image hosting service (either SM.MS or imgbb.com) using their respective APIs, returning the hosted image URL.\n\nFunction Description: The function should accept a PyTorch tensor and an API key, convert the tensor to a PNG image, encode it in base64, and upload it to the specified image hosting service. It should handle different response formats from the services and return the hosted image URL or an error message.\n\nInput:\n- image_tensor: torch.Tensor - A PyTorch tensor representing the image to be uploaded\n- api_key: str - The API key for authenticating with the image hosting service\n- service: str (optional) - The image hosting service to use ('smms' or 'imgbb'). Defaults to 'smms'\n\nOutput:\n- str - The URL of the uploaded image on success, or an error message string on failure\n- None - Returned when the input is not a PyTorch tensor or when the upload fails without an error message",
    "Canonical_solution": "import base64\nimport requests\nfrom io import BytesIO\nfrom PIL import Image\nimport numpy as np\nimport torch\n\ndef tensor_to_url_sm(image_tensor, api_key):\n    \"\"\"\n    \u5c06PyTorch\u5f20\u91cf\u8f6c\u6362\u4e3aPNG\u683c\u5f0f\u7684base64\u7f16\u7801\u5b57\u7b26\u4e32\u5e76\u4e0a\u4f20\u81f3SM.MS\u56fe\u5e8a\uff0c\u8fd4\u56de\u4e0a\u4f20\u540e\u7684\u56fe\u7247URL\u3002\n    \n    \u53c2\u6570:\n    - image_tensor: PyTorch\u5f20\u91cf\u3002\n    - api_key: SM.MS API\u5bc6\u94a5\u3002\n    \n    \u8fd4\u56de:\n    - \u56fe\u7247\u4e0a\u4f20\u6210\u529f\u540e\u7684URL\u6216\u5df2\u5b58\u5728\u7684\u56fe\u7247URL\uff0c\u5982\u679c\u5931\u8d25\u5219\u8fd4\u56deNone\u3002\n    \"\"\"\n    if isinstance(image_tensor, torch.Tensor):\n        i = 255.0 * image_tensor.cpu().numpy()\n    else:\n        raise TypeError(\"Input should be a PyTorch Tensor.\")\n\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    buffered = BytesIO()\n    img.save(buffered, format=\"PNG\")\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n\n    files = {'smfile': ('image.png', base64.b64decode(img_str), 'image/png')}\n    headers = {'Authorization': api_key}\n\n    try:\n        response = requests.post('https://sm.ms/api/v2/upload', files=files, headers=headers)\n        response.raise_for_status()\n        \n        result = response.json()\n        if result['success']:\n            return result['data']['url']\n        elif result.get('code') == 'image_repeated':\n            return result['images']\n        else:\n            print(f\"Error: {result.get('message')}\")\n            return None\n    except requests.exceptions.RequestException as e:\n        print(f\"HTTP Request Error: {e}\")\n        return None\n\ndef tensor_to_url_imgbb(image_tensor, api_key):\n    \"\"\"\n    \u5c06PyTorch\u5f20\u91cf\u8f6c\u6362\u4e3aPNG\u683c\u5f0f\u7684base64\u7f16\u7801\u5b57\u7b26\u4e32\u5e76\u4e0a\u4f20\u81f3imgbb.com\u56fe\u5e8a\uff0c\u8fd4\u56de\u4e0a\u4f20\u540e\u7684\u56fe\u7247URL\u3002\n\n    \u53c2\u6570:\n    - image_tensor: PyTorch\u5f20\u91cf\u3002\n    - api_key: imgbb.com API\u5bc6\u94a5\u3002\n\n    \u8fd4\u56de:\n    - \u56fe\u7247\u4e0a\u4f20\u6210\u529f\u540e\u7684URL\u6216\u5df2\u5b58\u5728\u7684\u56fe\u7247URL\uff0c\u5982\u679c\u5931\u8d25\u5219\u8fd4\u56deNone\u3002\n    \"\"\"\n    if isinstance(image_tensor, torch.Tensor):\n        i = 255.0 * image_tensor.cpu().numpy()\n    else:\n        raise TypeError(\"Input should be a PyTorch Tensor.\")\n\n    img = Image.fromarray(np.clip(i, 0, 255).astype(np.uint8))\n    buffered = BytesIO()\n    img.save(buffered, format=\"PNG\")\n    img_str = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n    url = \"https://api.imgbb.com/1/upload\"\n    payload = {\"key\": api_key, \"image\": img_str}\n    response = requests.post(url, data=payload)\n    if response.status_code == 200:\n        result = response.json()\n        img_url = result[\"data\"][\"url\"]\n        return img_url\n    else:\n        return \"Error: \" + response.text",
    "Code_level": "Function-level",
    "FSMID_for_test": 57,
    "wrong_result": "import torch\nimport base64\nimport io\nimport requests\nfrom PIL import Image\n\ndef upload_tensor_as_image(image_tensor, api_key, service='smms'):\n    if not isinstance(image_tensor, torch.Tensor):\n        return None\n    \n    try:\n        if image_tensor.dim() == 4:\n            image_tensor = image_tensor.squeeze(0)\n        if image_tensor.dim() == 3 and image_tensor.size(0) == 1:\n            image_tensor = image_tensor.squeeze(0)\n        \n        image_tensor = image_tensor.mul(255).byte()\n        image_np = image_tensor.cpu().numpy()\n        image = Image.fromarray(image_np)\n        \n        buffer = io.BytesIO()\n        image.save(buffer, format='PNG')\n        buffer.seek(0)\n        image_base64 = base64.b64encode(buffer.read()).decode('utf-8')\n        \n        if service == 'smms':\n            url = 'https://sm.ms/api/v2/upload'\n            headers = {'Authorization': api_key}\n            files = {'smfile': ('image.png', buffer, 'image/png')}\n            response = requests.post(url, headers=headers, files=files)\n            response_data = response.json()\n            \n            if response_data.get('success'):\n                return response_data['data']['url']\n            else:\n                return response_data.get('message', 'Upload failed')\n        \n        elif service == 'imgbb':\n            url = 'https://api.imgbb.com/1/upload'\n            payload = {\n                'key': api_key,\n                'image': image_base64\n            }\n            response = requests.post(url, data=payload)\n            response_data = response.json()\n            \n            if response_data.get('success'):\n                return response_data['data']['url']\n            else:\n                return response_data.get('error', {}).get('message', 'Upload failed')\n        \n        else:\n            return 'Invalid service specified'\n    \n    except Exception as e:\n        return str(e)"
  }
]